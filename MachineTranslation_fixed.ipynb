{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gau9xEXMGY8s"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OF-5Ml1jz1HG",
    "outputId": "05659fa6-97ef-4eb2-8b3e-877da63da8e2",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import unicodedata\n",
    "import re, math, random, os\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "rnn_encoder, rnn_encoder, transformer_encoder, transformer_decoder = None, None, None, None\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if __name__=='__main__':\n",
    "    print('Using device:', DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KiXPRWasyIj1"
   },
   "source": [
    "# Helper Functions\n",
    "This cell contains helper functions for the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k_WR8vEGMQyS"
   },
   "outputs": [],
   "source": [
    "# Converts the unicode file to ascii\n",
    "def unicode_to_ascii(s):\n",
    "    \"\"\"Normalizes latin chars with accent to their canonical decomposition\"\"\"\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    '''\n",
    "    Preprocess the sentence to add the start, end tokens and make them lower-case\n",
    "    '''\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "    w = re.sub(r'([?.!,\u00bf])', r' \\1 ', w)\n",
    "    w = re.sub(r'[\" \"]+', ' ', w)\n",
    "\n",
    "    w = re.sub(r'[^a-zA-Z?.!,\u00bf]+', ' ', w)\n",
    "\n",
    "    w = w.rstrip().strip()\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w\n",
    "\n",
    "\n",
    "def pad_sequences(x, max_len):\n",
    "    padded = np.zeros((max_len), dtype=np.int64)\n",
    "    if len(x) > max_len:\n",
    "        padded[:] = x[:max_len]\n",
    "    else:\n",
    "        padded[:len(x)] = x\n",
    "    return padded\n",
    "\n",
    "\n",
    "def preprocess_data_to_tensor(dataframe, src_vocab, trg_vocab):\n",
    "    # Vectorize the input and target languages\n",
    "    src_tensor = [[src_vocab.word2idx[s if s in src_vocab.vocab else '<unk>'] for s in es.split(' ')] for es in dataframe['es'].values.tolist()]\n",
    "    trg_tensor = [[trg_vocab.word2idx[s if s in trg_vocab.vocab else '<unk>'] for s in eng.split(' ')] for eng in dataframe['eng'].values.tolist()]\n",
    "\n",
    "    # Calculate the max_length of input and output tensor for padding\n",
    "    max_length_src, max_length_trg = max(len(t) for t in src_tensor), max(len(t) for t in trg_tensor)\n",
    "    print('max_length_src: {}, max_length_trg: {}'.format(max_length_src, max_length_trg))\n",
    "\n",
    "    # Pad all the sentences in the dataset with the max_length\n",
    "    src_tensor = [pad_sequences(x, max_length_src) for x in src_tensor]\n",
    "    trg_tensor = [pad_sequences(x, max_length_trg) for x in trg_tensor]\n",
    "\n",
    "    return src_tensor, trg_tensor, max_length_src, max_length_trg\n",
    "\n",
    "\n",
    "def train_test_split(src_tensor, trg_tensor):\n",
    "    '''\n",
    "    Create training and test sets.\n",
    "    '''\n",
    "    total_num_examples = len(src_tensor) - int(0.2*len(src_tensor))\n",
    "    src_tensor_train, src_tensor_test = src_tensor[:int(0.75*total_num_examples)], src_tensor[int(0.75*total_num_examples):total_num_examples]\n",
    "    trg_tensor_train, trg_tensor_test = trg_tensor[:int(0.75*total_num_examples)], trg_tensor[int(0.75*total_num_examples):total_num_examples]\n",
    "\n",
    "    return src_tensor_train, src_tensor_test, trg_tensor_train, trg_tensor_test"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sanity Check Function\n",
    "\n",
    "The code below will be used to perform a sanity check on both the RNN and transformer later in the homework"
   ],
   "metadata": {
    "id": "8ssom6vKO7Ra"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "count_parameters = lambda model: sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def sanityCheckModel(all_test_params, NN, expected_outputs, init_or_forward):\n",
    "    print('--- TEST: ' + ('Number of Model Parameters (tests __init__(...))' if init_or_forward=='init' else 'Output shape of forward(...)') + ' ---')\n",
    "    if init_or_forward == \"forward\":\n",
    "        # Creating random texts and lables batches\n",
    "        texts_batch = torch.randint(low=0, high=len(all_test_params[0]['src_vocab']), size=(10,16))\n",
    "        labels_batch = torch.randint(low=0, high=len(all_test_params[0]['src_vocab']), size=(10,12))\n",
    "\n",
    "    for tp_idx, (test_params, expected_output) in enumerate(zip(all_test_params, expected_outputs)):\n",
    "        if init_or_forward == \"forward\":\n",
    "            batch_size = test_params['batch_size']\n",
    "            texts = texts_batch[:batch_size]\n",
    "            # if NN.__name__ == \"RnnEncoder\":\n",
    "            texts = texts.transpose(0,1)\n",
    "\n",
    "        # Construct the student model\n",
    "        tps = {k:v for k, v in test_params.items() if k != 'batch_size'}\n",
    "        stu_nn = NN(**tps)\n",
    "\n",
    "        input_rep = str({k:v for k,v in tps.items()})\n",
    "\n",
    "        if init_or_forward == \"forward\":\n",
    "            with torch.no_grad():\n",
    "                if NN.__name__ == \"TransformerEncoder\":\n",
    "                    stu_out = stu_nn(texts)\n",
    "                else:\n",
    "                    stu_out, _ = stu_nn(texts)\n",
    "                    expected_output = torch.rand(expected_output).size()\n",
    "            ref_out_shape = expected_output\n",
    "\n",
    "            has_passed = torch.is_tensor(stu_out)\n",
    "            if not has_passed: msg = 'Output must be a torch.Tensor; received ' + str(type(stu_out))\n",
    "            else:\n",
    "                has_passed = stu_out.shape == ref_out_shape\n",
    "                msg = 'Your Output Shape: ' + str(stu_out.shape)\n",
    "\n",
    "\n",
    "            status = 'PASSED' if has_passed else 'FAILED'\n",
    "            message = '\\t' + status + \"\\t Init Input: \" + input_rep + '\\tForward Input Shape: ' + str(texts.shape) + '\\tExpected Output Shape: ' + str(ref_out_shape) + '\\t' + msg\n",
    "            print(message)\n",
    "        else:\n",
    "            stu_num_params = count_parameters(stu_nn)\n",
    "            ref_num_params = expected_output\n",
    "            comparison_result = (stu_num_params == ref_num_params)\n",
    "\n",
    "            status = 'PASSED' if comparison_result else 'FAILED'\n",
    "            message = '\\t' + status + \"\\tInput: \" + input_rep + ('\\tExpected Num. Params: ' + str(ref_num_params) + '\\tYour Num. Params: '+ str(stu_num_params))\n",
    "            print(message)\n",
    "\n",
    "        del stu_nn"
   ],
   "metadata": {
    "id": "YkelDym0Ow3b"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation Functions\n",
    "\n",
    "These functions will be used to evaluate both the RNN and Transformer Models."
   ],
   "metadata": {
    "id": "fxHs2mWsx2lI"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def get_reference_candidate(target, pred, trg_vocab):\n",
    "    def _to_token(sentence):\n",
    "        lis = []\n",
    "        for s in sentence[1:]:\n",
    "            x = trg_vocab.idx2word[s]\n",
    "            if x == \"<end>\": break\n",
    "            lis.append(x)\n",
    "        return lis\n",
    "    reference = _to_token(target)\n",
    "    candidate = _to_token(pred)\n",
    "    return reference, candidate\n",
    "\n",
    "def compute_bleu_scores(target_output, final_output, trg_vocab):\n",
    "    assert len(target_output) == len(final_output)\n",
    "    bleu_1 = 0.0\n",
    "    bleu_2 = 0.0\n",
    "    bleu_3 = 0.0\n",
    "    bleu_4 = 0.0\n",
    "\n",
    "    smoother = SmoothingFunction()\n",
    "    save_reference = []\n",
    "    save_candidate = []\n",
    "    for i in range(len(target_output)):\n",
    "        reference, candidate = get_reference_candidate(target_output[i], final_output[i], trg_vocab)\n",
    "\n",
    "        bleu_1 += sentence_bleu(reference, candidate, weights=(1,), smoothing_function=smoother.method1)\n",
    "        bleu_2 += sentence_bleu(reference, candidate, weights=(1/2, 1/2), smoothing_function=smoother.method1)\n",
    "        bleu_3 += sentence_bleu(reference, candidate, weights=(1/3, 1/3, 1/3), smoothing_function=smoother.method1)\n",
    "        bleu_4 += sentence_bleu(reference, candidate, weights=(1/4, 1/4, 1/4, 1/4), smoothing_function=smoother.method1)\n",
    "\n",
    "        save_reference.append(reference)\n",
    "        save_candidate.append(candidate)\n",
    "\n",
    "    bleu_1 = bleu_1/len(target_output)\n",
    "    bleu_2 = bleu_2/len(target_output)\n",
    "    bleu_3 = bleu_3/len(target_output)\n",
    "    bleu_4 = bleu_4/len(target_output)\n",
    "\n",
    "    scores = {\"bleu_1\": bleu_1, \"bleu_2\": bleu_2, \"bleu_3\": bleu_3, \"bleu_4\": bleu_4}\n",
    "    print('BLEU 1-gram: %f' % (bleu_1))\n",
    "    print('BLEU 2-gram: %f' % (bleu_2))\n",
    "    print('BLEU 3-gram: %f' % (bleu_3))\n",
    "    print('BLEU 4-gram: %f' % (bleu_4))\n",
    "\n",
    "    return save_candidate, scores"
   ],
   "metadata": {
    "id": "Jn7loRHZx-ZE"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yvR1U28i6Itb"
   },
   "source": [
    "# Step 1: Download & Prepare the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-XH8nu0ojQpt"
   },
   "source": [
    "## Download and Visualize the Data\n",
    "\n",
    "Here we will download the translation data. We will learn a model to translate Spanish to English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KtyBFlMKIg7g"
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    os.system(\"wget http://www.manythings.org/anki/spa-eng.zip\")\n",
    "    os.system(\"unzip -o spa-eng.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q3PqS2IJx0pt"
   },
   "source": [
    "Now we view the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4WNjizED6eFI",
    "outputId": "d5bcc0cc-b5c2-416e-ce67-956f1f80cde4",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                          eng                             es\n",
      "0           I bumped my knee.          Me golpe\u00e9 la rodilla.\n",
      "1           I'll be at Tom's.         Estar\u00e9 en casa de Tom.\n",
      "2       We love our children.       Amamos a nuestros hijos.\n",
      "3        He took out one egg.              \u00c9l sac\u00f3 un huevo.\n",
      "4       He achieved his goal.            \u00c9l alcanz\u00f3 su meta.\n",
      "...                       ...                            ...\n",
      "49995    The door burst open.   La puerta se abri\u00f3 de golpe.\n",
      "49996  She never wears green.  Ella nunca se viste de verde.\n",
      "49997   Those books are mine.                Son mis libros.\n",
      "49998  I don't play baseball.           No juego al b\u00e9isbol.\n",
      "49999    Please stop the war.    Paren la guerra, por favor.\n",
      "\n",
      "[50000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    total_num_examples = 50000\n",
    "    dat = pd.read_csv(\"spa.txt\",\n",
    "                    sep=\"\\t\",\n",
    "                    header=None,\n",
    "                    usecols=[0,1],\n",
    "                    names=['eng', 'es'],\n",
    "                    nrows=total_num_examples,\n",
    "                    encoding=\"UTF-8\"\n",
    "    ).sample(frac=1).reset_index().drop(['index'], axis=1)\n",
    "\n",
    "    print(dat) # Visualize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-fp16WlI7D_"
   },
   "source": [
    "Next we preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mud7HbQUMUHB",
    "outputId": "999e5b52-d6af-4f9c-c018-f29efe6f7351",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                         eng  \\\n",
      "0           <start> i bumped my knee . <end>   \n",
      "1           <start> i ll be at tom s . <end>   \n",
      "2       <start> we love our children . <end>   \n",
      "3        <start> he took out one egg . <end>   \n",
      "4       <start> he achieved his goal . <end>   \n",
      "...                                      ...   \n",
      "49995    <start> the door burst open . <end>   \n",
      "49996  <start> she never wears green . <end>   \n",
      "49997   <start> those books are mine . <end>   \n",
      "49998  <start> i don t play baseball . <end>   \n",
      "49999    <start> please stop the war . <end>   \n",
      "\n",
      "                                                 es  \n",
      "0              <start> me golpee la rodilla . <end>  \n",
      "1             <start> estare en casa de tom . <end>  \n",
      "2           <start> amamos a nuestros hijos . <end>  \n",
      "3                  <start> el saco un huevo . <end>  \n",
      "4                <start> el alcanzo su meta . <end>  \n",
      "...                                             ...  \n",
      "49995   <start> la puerta se abrio de golpe . <end>  \n",
      "49996  <start> ella nunca se viste de verde . <end>  \n",
      "49997                <start> son mis libros . <end>  \n",
      "49998           <start> no juego al beisbol . <end>  \n",
      "49999   <start> paren la guerra , por favor . <end>  \n",
      "\n",
      "[50000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    data = dat.copy()\n",
    "    data['eng'] = dat.eng.apply(lambda w: preprocess_sentence(w))\n",
    "    data['es'] = dat.es.apply(lambda w: preprocess_sentence(w))\n",
    "    print(data) # Visualizing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IHJw_CyykmMp"
   },
   "source": [
    "## Vocabulary & Dataloader\n",
    "\n",
    "First we create a class for managing our vocabulary as we did in Homework 2. In this homework, we have a separate class for the vocabulary as we need 2 different vocabularies $-$ one for English and one for Spanish.\n",
    "\n",
    "Then we prepare the dataloader and make sure it returns the source sentence and target sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1h4Q21azMW-T"
   },
   "outputs": [],
   "source": [
    "class Vocab_Lang():\n",
    "    def __init__(self, vocab):\n",
    "        self.word2idx = {'<pad>': 0, '<unk>': 1}\n",
    "        self.idx2word = {0: '<pad>', 1: '<unk>'}\n",
    "        self.vocab = vocab\n",
    "\n",
    "        for index, word in enumerate(vocab):\n",
    "            self.word2idx[word] = index + 2 # +2 because of <pad> and <unk> token\n",
    "            self.idx2word[index + 2] = word\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "    def __repr__(self):\n",
    "        if len(self.vocab) <= 5:\n",
    "            return str(self.vocab)\n",
    "        else:\n",
    "            return f'Vocab_Lang object with {len(self.vocab)} words'\n",
    "\n",
    "class MyData(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.length = torch.LongTensor([np.sum(1 - np.equal(x, 0)) for x in X])\n",
    "        self.data = torch.LongTensor(X)\n",
    "        self.target = torch.LongTensor(y)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.target[index]\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BA4lG_4fqL9U"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BrgMtGxHoqFi"
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # HYPERPARAMETERS\n",
    "    BATCH_SIZE = 64\n",
    "    EMBEDDING_DIM = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WrXOzSbAWrkT"
   },
   "outputs": [],
   "source": [
    "def build_vocabulary(pd_dataframe):\n",
    "    sentences = [sen.split() for sen in pd_dataframe]\n",
    "    vocab = {}\n",
    "    for sen in sentences:\n",
    "        for word in sen:\n",
    "            if word not in vocab:\n",
    "                vocab[word] = 1\n",
    "    return list(vocab.keys())\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    src_vocab_list = build_vocabulary(data['es'])\n",
    "    trg_vocab_list = build_vocabulary(data['eng'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46wvdhg_17bG"
   },
   "source": [
    "We instantiate our training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k6UsDOA2c-G6",
    "outputId": "124ae2b5-f07f-4827-d852-2022e212377c",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "max_length_src: 16, max_length_trg: 12\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    src_vocab = Vocab_Lang(src_vocab_list)\n",
    "    trg_vocab = Vocab_Lang(trg_vocab_list)\n",
    "\n",
    "    src_tensor, trg_tensor, max_length_src, max_length_trg = preprocess_data_to_tensor(data, src_vocab, trg_vocab)\n",
    "    src_tensor_train, src_tensor_val, trg_tensor_train, trg_tensor_val = train_test_split(src_tensor, trg_tensor)\n",
    "\n",
    "    # Create train and val datasets\n",
    "    train_dataset = MyData(src_tensor_train, trg_tensor_train)\n",
    "    train_dataset = DataLoader(train_dataset, batch_size=BATCH_SIZE, drop_last=True, shuffle=True)\n",
    "\n",
    "    test_dataset = MyData(src_tensor_val, trg_tensor_val)\n",
    "    test_dataset = DataLoader(test_dataset, batch_size=BATCH_SIZE, drop_last=True, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zWO5ptloc-HL",
    "outputId": "fab7b4f8-c0a8-4645-f5dd-8b73b15d0df2",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Source: tensor([[   2,   64, 1407,    7,    8,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0],\n",
      "        [   2,   38,    3,   28,   90,  308,    7,    8,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0],\n",
      "        [   2, 2879,  207, 1127, 8790,    7,    8,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0],\n",
      "        [   2, 2748,  568,    7,    8,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0],\n",
      "        [   2,  435,   30,  237,    7,    8,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0]])\n",
      "Source Dimensions:  torch.Size([5, 16])\n",
      "Target: tensor([[   2,    3,  900,   36,    7,    8,    0,    0,    0,    0,    0,    0],\n",
      "        [   2,    3,  127,   46,  192,    5,  262,    7,    8,    0,    0,    0],\n",
      "        [   2,  358,   64,  887,   82,    7,    8,    0,    0,    0,    0,    0],\n",
      "        [   2,    3, 1154,  473,    7,    8,    0,    0,    0,    0,    0,    0],\n",
      "        [   2,    3,  364,  208,    7,    8,    0,    0,    0,    0,    0,    0]])\n",
      "Target Dimensions:  torch.Size([5, 12])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    idxes = random.choices(range(len(train_dataset.dataset)), k=5)\n",
    "    src, trg =  train_dataset.dataset[idxes]\n",
    "    print('Source:', src)\n",
    "    print('Source Dimensions: ', src.size())\n",
    "    print('Target:', trg)\n",
    "    print('Target Dimensions: ', trg.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "amI0Dl7p64TI"
   },
   "source": [
    "# Step 2: Train a Recurrent Neural Network (RNN)\n",
    "\n",
    "Writing a recurrent model for machine translation, and then trainining and evaluating its results.\n",
    "\n",
    "1. Attention paper: https://arxiv.org/pdf/1409.0473.pdf\n",
    "2. Explanation of LSTM's & GRU's: https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21\n",
    "3. Attention explanation: https://towardsdatascience.com/attention-in-neural-networks-e66920838742\n",
    "4. Another attention explanation: https://towardsdatascience.com/attention-and-its-different-forms-7fc3674d14dc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KSHdxD658EzI"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction, corpus_bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENeT1fj_2f8t"
   },
   "source": [
    "##  Encoder Model\n",
    "\n",
    "Building a recurrent encoder model, instead of using a fully connected layer as the output, returns a sequence of outputs of GRU as well as the final hidden state. These will be used in the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Sx4QQd3M4XK"
   },
   "outputs": [],
   "source": [
    "class RnnEncoder(nn.Module):\n",
    "    def __init__(self, src_vocab, embedding_dim, hidden_units):\n",
    "        super(RnnEncoder, self).__init__()\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src_vocab: Vocab_Lang, the source vocabulary\n",
    "            embedding_dim: the dimension of the embedding\n",
    "            hidden_units: The number of features in the GRU hidden state\n",
    "        \"\"\"\n",
    "        self.src_vocab = src_vocab # Do not change\n",
    "        vocab_size = len(src_vocab)\n",
    "\n",
    "\n",
    "        # Initialize embedding layer\n",
    "        # (see: https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)\n",
    "\n",
    "        # Initialize a single directional GRU with 1 layer and batch_first=False\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_units, num_layers=1, batch_first=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: source texts, [max_len, batch_size]\n",
    "\n",
    "        Returns:\n",
    "            output: [max_len, batch_size, hidden_units]\n",
    "            hidden_state: [1, batch_size, hidden_units]\n",
    "\n",
    "        Pseudo-code:\n",
    "        - Pass x through an embedding layer and pass the results through the recurrent net\n",
    "        - Return output and hidden states from the recurrent net\n",
    "        \"\"\"\n",
    "        #output, hidden_state = None, None\n",
    "\n",
    "        x_embedded = self.embedding(x)\n",
    "\n",
    "        # Pass through GRU\n",
    "        output, hidden_state = self.gru(x_embedded)\n",
    "\n",
    "        return output, hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OZFuambIkWaD",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "ce7ecfff-24a3-4017-dc32-bc5976eeb491"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--- TEST: Number of Model Parameters (tests __init__(...)) ---\n",
      "\tPASSED\tInput: {'src_vocab': ['a', 'aa', 'aaa'], 'embedding_dim': 2, 'hidden_units': 50}\tExpected Num. Params: 8110\tYour Num. Params: 8110\n",
      "\tPASSED\tInput: {'src_vocab': ['a', 'aa', 'aaa'], 'embedding_dim': 2, 'hidden_units': 100}\tExpected Num. Params: 31210\tYour Num. Params: 31210\n",
      "\tPASSED\tInput: {'src_vocab': ['a', 'aa', 'aaa'], 'embedding_dim': 2, 'hidden_units': 200}\tExpected Num. Params: 122410\tYour Num. Params: 122410\n",
      "\tPASSED\tInput: {'src_vocab': ['a', 'aa', 'aaa'], 'embedding_dim': 5, 'hidden_units': 50}\tExpected Num. Params: 8575\tYour Num. Params: 8575\n",
      "\tPASSED\tInput: {'src_vocab': ['a', 'aa', 'aaa'], 'embedding_dim': 5, 'hidden_units': 100}\tExpected Num. Params: 32125\tYour Num. Params: 32125\n",
      "\tPASSED\tInput: {'src_vocab': ['a', 'aa', 'aaa'], 'embedding_dim': 5, 'hidden_units': 200}\tExpected Num. Params: 124225\tYour Num. Params: 124225\n",
      "\tPASSED\tInput: {'src_vocab': ['a', 'aa', 'aaa'], 'embedding_dim': 8, 'hidden_units': 50}\tExpected Num. Params: 9040\tYour Num. Params: 9040\n",
      "\tPASSED\tInput: {'src_vocab': ['a', 'aa', 'aaa'], 'embedding_dim': 8, 'hidden_units': 100}\tExpected Num. Params: 33040\tYour Num. Params: 33040\n",
      "\tPASSED\tInput: {'src_vocab': ['a', 'aa', 'aaa'], 'embedding_dim': 8, 'hidden_units': 200}\tExpected Num. Params: 126040\tYour Num. Params: 126040\n",
      "\n",
      "--- TEST: Output shape of forward(...) ---\n",
      "\tPASSED\t Init Input: {'embedding_dim': 256, 'src_vocab': ['a', 'aa', 'aaa'], 'hidden_units': 50}\tForward Input Shape: torch.Size([16, 1])\tExpected Output Shape: torch.Size([16, 1, 50])\tYour Output Shape: torch.Size([16, 1, 50])\n",
      "\tPASSED\t Init Input: {'embedding_dim': 256, 'src_vocab': ['a', 'aa', 'aaa'], 'hidden_units': 50}\tForward Input Shape: torch.Size([16, 2])\tExpected Output Shape: torch.Size([16, 2, 50])\tYour Output Shape: torch.Size([16, 2, 50])\n",
      "\tPASSED\t Init Input: {'embedding_dim': 256, 'src_vocab': ['a', 'aa', 'aaa'], 'hidden_units': 100}\tForward Input Shape: torch.Size([16, 1])\tExpected Output Shape: torch.Size([16, 1, 100])\tYour Output Shape: torch.Size([16, 1, 100])\n",
      "\tPASSED\t Init Input: {'embedding_dim': 256, 'src_vocab': ['a', 'aa', 'aaa'], 'hidden_units': 100}\tForward Input Shape: torch.Size([16, 2])\tExpected Output Shape: torch.Size([16, 2, 100])\tYour Output Shape: torch.Size([16, 2, 100])\n",
      "\tPASSED\t Init Input: {'embedding_dim': 256, 'src_vocab': ['a', 'aa', 'aaa'], 'hidden_units': 200}\tForward Input Shape: torch.Size([16, 1])\tExpected Output Shape: torch.Size([16, 1, 200])\tYour Output Shape: torch.Size([16, 1, 200])\n",
      "\tPASSED\t Init Input: {'embedding_dim': 256, 'src_vocab': ['a', 'aa', 'aaa'], 'hidden_units': 200}\tForward Input Shape: torch.Size([16, 2])\tExpected Output Shape: torch.Size([16, 2, 200])\tYour Output Shape: torch.Size([16, 2, 200])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Set random seed\n",
    "    torch.manual_seed(42)\n",
    "    # Create test inputs\n",
    "    embedding_dim = [2, 5, 8]\n",
    "    hidden_units = [50, 100, 200]\n",
    "    sanity_vocab = Vocab_Lang(vocab=[\"a\", \"aa\", \"aaa\"])\n",
    "    params = []\n",
    "    inputs = []\n",
    "    for ed in embedding_dim:\n",
    "        for hu in hidden_units:\n",
    "            inp = {}\n",
    "            inp['src_vocab'] = sanity_vocab\n",
    "            inp['embedding_dim'] = ed\n",
    "            inp['hidden_units'] = hu\n",
    "            inputs.append(inp)\n",
    "    # Test init\n",
    "    expected_outputs = [8110, 31210, 122410, 8575, 32125, 124225, 9040, 33040, 126040]\n",
    "\n",
    "    sanityCheckModel(inputs, RnnEncoder, expected_outputs, \"init\")\n",
    "    print()\n",
    "\n",
    "    # Test forward\n",
    "    inputs = []\n",
    "    batch_sizes = [1, 2]\n",
    "    for hu in hidden_units:\n",
    "        for b in batch_sizes:\n",
    "            inp = {}\n",
    "            inp['embedding_dim'] = EMBEDDING_DIM\n",
    "            inp['src_vocab'] = sanity_vocab\n",
    "            inp[\"batch_size\"] = b\n",
    "            inp['hidden_units'] = hu\n",
    "            inputs.append(inp)\n",
    "    expected_outputs = [torch.Size([16, 1, 50]), torch.Size([16, 2, 50]), torch.Size([16, 1, 100]), torch.Size([16, 2, 100]), torch.Size([16, 1, 200]), torch.Size([16, 2, 200])]\n",
    "\n",
    "    sanityCheckModel(inputs, RnnEncoder, expected_outputs, \"forward\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vKwsEWpK2mcT"
   },
   "source": [
    "We will implement a Decoder model that uses an attention mechanism, as provided in https://arxiv.org/pdf/1409.0473.pdf.\n",
    "\n",
    "  1. <b>Attention scores:</b> Compute real-valued scores for the decoder hidden state $\\mathbf{h}_t$ and each encoder hidden state $\\mathbf{\\bar h}_s$: $$\\mathrm{score}(\\mathbf{h}_t, \\mathbf{\\bar h}_s)=\n",
    "      \\mathbf{v}_a^T \\tanh(\\mathbf{W}_1 \\mathbf{h}_t +\\mathbf{W}_2 \\mathbf{\\bar h}_s)\n",
    "$$\n",
    "  A higher score indicates a stronger \"affinity\" between the decoder state and a specific encoder state.\n",
    "\n",
    " 2. <b>Attention weights:</b> Normalizing the attention scores to obtain a valid probability distribution: $$\\alpha_{ts} = \\frac{\\exp \\big (\\mathrm{score}(\\mathbf{h}_t, \\mathbf{\\bar h}_s) \\big)}{\\sum_{s'=1}^S \\exp \\big (\\mathrm{score}(\\mathbf{h}_t, \\mathbf{\\bar h}_{s'}) \\big)}$$ (softmax function)\n",
    "\n",
    " 3. <b>Context vector:</b>  $\\mathbf{c}_t$  is a weighted average of the encoder hidden states, where the weights are given by the attention weights  just computed: $$\\mathbf{c}_t=\\sum_{s=1}^S \\alpha_{ts} \\mathbf{\\bar h}_s$$\n",
    "\n",
    "Returns this context vector, along with the attention weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cw84M2LPM-PC"
   },
   "outputs": [],
   "source": [
    "class RnnDecoder(nn.Module):\n",
    "    def __init__(self, trg_vocab, embedding_dim, hidden_units):\n",
    "        super(RnnDecoder, self).__init__()\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            trg_vocab: Vocab_Lang, the target vocabulary\n",
    "            embedding_dim: The dimension of the embedding\n",
    "            hidden_units: The number of features in the GRU hidden state\n",
    "        \"\"\"\n",
    "        self.trg_vocab = trg_vocab # Do not change\n",
    "        vocab_size = len(trg_vocab)\n",
    "\n",
    "\n",
    "        # Initialize embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # Initialize layers to compute attention score\n",
    "        self.W1 = nn.Linear(hidden_units, hidden_units)\n",
    "        self.W2 = nn.Linear(hidden_units, hidden_units)\n",
    "        self.Va = nn.Linear(hidden_units, 1)\n",
    "\n",
    "        # Initialize a single directional GRU with 1 layer and batch_first=True\n",
    "        # NOTE: Input to your RNN will be the concatenation of your embedding vector and the context vector\n",
    "        self.gru = nn.GRU(embedding_dim + hidden_units, hidden_units, num_layers=1, batch_first=True)\n",
    "\n",
    "\n",
    "        # Initialize fully connected layer\n",
    "        self.fc = nn.Linear(hidden_units, vocab_size)\n",
    "\n",
    "    def compute_attention(self, dec_hs, enc_output):\n",
    "        '''\n",
    "        This function computes the context vector and attention weights.\n",
    "\n",
    "        Args:\n",
    "            dec_hs: Decoder hidden state; [1, batch_size, hidden_units]\n",
    "            enc_output: Encoder outputs; [max_len_src, batch_size, hidden_units]\n",
    "\n",
    "        Returns:\n",
    "            context_vector: Context vector, according to formula; [batch_size, hidden_units]\n",
    "            attention_weights: The attention weights you have calculated; [batch_size, max_len_src, 1]\n",
    "\n",
    "        Pseudo-code:\n",
    "            (1) Compute the attention scores for dec_hs & enc_output\n",
    "                    - Hint: You may need to permute the dimensions of the tensors in order to pass them through linear layers\n",
    "                    - Output size: [batch_size, max_len_src, 1]\n",
    "            (2) Compute attention_weights by taking a softmax over your scores to normalize the distribution (Make sure that after softmax the normalized scores add up to 1)\n",
    "                    - Output size: [batch_size, max_len_src, 1]\n",
    "            (3) Compute context_vector from attention_weights & enc_output\n",
    "                    - Hint: You may find it helpful to use torch.sum & element-wise multiplication (* operator)\n",
    "            (4) Return context_vector & attention_weights\n",
    "        '''\n",
    "        context_vector, attention_weights = None, None\n",
    "\n",
    "\n",
    "        dec_hs = dec_hs.permute(1, 0, 2)\n",
    "        enc_output = enc_output.permute(1, 0, 2)\n",
    "\n",
    "        score = self.Va(torch.tanh(self.W1(enc_output) + self.W2(dec_hs)))\n",
    "\n",
    "        attention_weights = F.softmax(score, dim=1)\n",
    "\n",
    "        context_vector = torch.sum(attention_weights * enc_output, dim=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "    def forward(self, x, dec_hs, enc_output):\n",
    "        '''\n",
    "        This function runs the decoder for a **single** time step.\n",
    "\n",
    "        Args:\n",
    "            x: Input token; [batch_size, 1]\n",
    "            dec_hs: Decoder hidden state; [1, batch_size, hidden_units]\n",
    "            enc_output: Encoder outputs; [max_len_src, batch_size, hidden_units]\n",
    "\n",
    "        Returns:\n",
    "            fc_out: (Unnormalized) output distribution [batch_size, vocab_size]\n",
    "            dec_hs: Decoder hidden state; [1, batch_size, hidden_units]\n",
    "            attention_weights: The attention weights you have learned; [batch_size, max_len_src, 1]\n",
    "\n",
    "        Pseudo-code:\n",
    "            (1) Compute the context vector & attention weights by calling self.compute_attention(...) on the appropriate input\n",
    "            (2) Obtain embedding vectors for your input x\n",
    "                    - Output size: [batch_size, 1, embedding_dim]\n",
    "            (3) Concatenate the context vector & the embedding vectors along the appropriate dimension\n",
    "            (4) Feed this result through your RNN (along with the current hidden state) to get output and new hidden state\n",
    "                    - Output sizes: [batch_size, 1, hidden_units] & [1, batch_size, hidden_units]\n",
    "            (5) Feed the output of your RNN through linear layer to get (unnormalized) output distribution (don't call softmax!)\n",
    "            (6) Return this output, the new decoder hidden state, & the attention weights\n",
    "        '''\n",
    "        fc_out, attention_weights = None, None\n",
    "\n",
    "        context_vector, attention_weights = self.compute_attention(dec_hs, enc_output)\n",
    "        x = self.embedding(x.type(torch.long))\n",
    "        context_vector = context_vector.unsqueeze(1)\n",
    "        rnn_input = torch.cat((x, context_vector), dim=-1)\n",
    "        output, dec_hs = self.gru(rnn_input, dec_hs)\n",
    "        fc_out = self.fc(output.squeeze(1))\n",
    "\n",
    "        return fc_out, dec_hs, attention_weights\n",
    "\n",
    "    def decode_step(self, inputs, enc_output, dec_hs):\n",
    "        '''\n",
    "        Call one step of the decoder.\n",
    "\n",
    "        Args:\n",
    "            inputs: Input tokens; [batch_size, sequence length]\n",
    "            enc_output: Encoder outputs; [max_len_src, batch_size, hidden_units]\n",
    "            dec_hs: Decoder hidden state; [1, batch_size, hidden_units]\n",
    "\n",
    "        Returns:\n",
    "            fc_out: (Unnormalized) output distribution [batch_size, vocab_size]\n",
    "            dec_hs: Decoder hidden state; [1, batch_size, hidden_units]\n",
    "        '''\n",
    "        assert inputs.shape[0] == enc_output.shape[1] == dec_hs.shape[1], 'batch_size must be the same across tensors'\n",
    "        fc_out, dec_hs, attention_weights = self(inputs[:,-1].unsqueeze(1), dec_hs, enc_output)\n",
    "        return fc_out, dec_hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tc4r19oiq4YE"
   },
   "outputs": [],
   "source": [
    "def sanityCheckDecoderModelForward(inputs, NN, expected_outputs):\n",
    "    print('--- TEST: Output shape of forward(...) ---\\n')\n",
    "    expected_fc_outs = expected_outputs[0]\n",
    "    expected_dec_hs = expected_outputs[1]\n",
    "    expected_attention_weights = expected_outputs[2]\n",
    "    msg = ''\n",
    "    for i, inp in enumerate(inputs):\n",
    "        input_rep = '{'\n",
    "        for k,v in inp.items():\n",
    "            if torch.is_tensor(v):\n",
    "                input_rep += str(k) + ': ' + 'Tensor with shape ' + str(v.size()) + ', '\n",
    "            else:\n",
    "                input_rep += str(k) + ': ' + str(v) + ', '\n",
    "        input_rep += '}'\n",
    "        dec = RnnDecoder(trg_vocab=inp['trg_vocab'],embedding_dim=inp['embedding_dim'],hidden_units=inp['hidden_units'])\n",
    "        dec_hs = torch.rand(1, inp[\"batch_size\"], inp['hidden_units'])\n",
    "        x = torch.randint(low=0,high=len(inp[\"trg_vocab\"]),size=(inp[\"batch_size\"], 1))\n",
    "        with torch.no_grad():\n",
    "            dec_out = dec(x=x, dec_hs=dec_hs,enc_output=inp['encoder_outputs'])\n",
    "            if not isinstance(dec_out, tuple):\n",
    "                msg = '\\tFAILED\\tYour RnnDecoder.forward() output must be a tuple; received ' + str(type(dec_out))\n",
    "                print(msg)\n",
    "                continue\n",
    "            elif len(dec_out)!=3:\n",
    "                msg = '\\tFAILED\\tYour RnnDecoder.forward() output must be a tuple of size 3; received tuple of size ' + str(len(dec_out))\n",
    "                print(msg)\n",
    "                continue\n",
    "            stu_fc_out, stu_dec_hs, stu_attention_weights = dec_out\n",
    "        del dec\n",
    "        has_passed = True\n",
    "        msg = \"\"\n",
    "        if not torch.is_tensor(stu_fc_out):\n",
    "            has_passed = False\n",
    "            msg += '\\tFAILED\\tOutput must be a torch.Tensor; received ' + str(type(stu_fc_out)) + \" \"\n",
    "        if not torch.is_tensor(stu_dec_hs):\n",
    "            has_passed = False\n",
    "            msg += '\\tFAILED\\tDecoder Hidden State must be a torch.Tensor; received ' + str(type(stu_dec_hs)) + \" \"\n",
    "        if not torch.is_tensor(stu_attention_weights):\n",
    "            has_passed = False\n",
    "            msg += '\\tFAILED\\tAttention Weights must be a torch.Tensor; received ' + str(type(stu_attention_weights)) + \" \"\n",
    "\n",
    "        status = 'PASSED' if has_passed else 'FAILED'\n",
    "        if not has_passed:\n",
    "            message = '\\t' + status + \"\\t Init Input: \" + input_rep + '\\tForward Input Shape (x): ' + str(os.XATTR_REPLACE.shape) + '\\tExpected Output Shape: ' + str(expected_fc_outs[i]) + '\\t' + msg\n",
    "            print(message)\n",
    "            continue\n",
    "\n",
    "        has_passed = stu_fc_out.size() == expected_fc_outs[i]\n",
    "        msg = 'Your Output Shape: ' + str(stu_fc_out.size())\n",
    "        status = 'PASSED' if has_passed else 'FAILED'\n",
    "        message = '\\t' + status + \"\\t Init Input: \" + input_rep + '\\tForward Input Shape (x): ' + str(x.shape) + '\\tExpected Output Shape: ' + str(expected_fc_outs[i]) + '\\t' + msg\n",
    "        print(message)\n",
    "\n",
    "        has_passed = stu_dec_hs.size() == expected_dec_hs[i]\n",
    "        msg = 'Your Hidden State Shape: ' + str(stu_dec_hs.size())\n",
    "        status = 'PASSED' if has_passed else 'FAILED'\n",
    "        message = '\\t' + status + \"\\t Init Input: \" + input_rep + '\\tForward Input Shape (x): ' + str(x.shape) + '\\tExpected Hidden State Shape: ' + str(expected_dec_hs[i]) + '\\t' + msg\n",
    "        print(message)\n",
    "\n",
    "        has_passed = stu_attention_weights.size() == expected_attention_weights[i]\n",
    "        msg = 'Your Attention Weights Shape: ' + str(stu_attention_weights.size())\n",
    "        status = 'PASSED' if has_passed else 'FAILED'\n",
    "        message = '\\t' + status + \"\\t Init Input: \" + input_rep + '\\tForward Input Shape (x): ' + str(x.shape) + '\\tExpected Attention Weights Shape: ' + str(expected_attention_weights[i]) + '\\t' + msg\n",
    "        print(message)\n",
    "\n",
    "        stu_sum = stu_attention_weights.sum(dim=1).squeeze()\n",
    "        if torch.allclose(stu_sum, torch.ones_like(stu_sum), atol=1e-5):\n",
    "            print('\\tPASSED\\t The sum of your attention_weights along dim 1 is 1.')\n",
    "        else:\n",
    "            print('\\tFAILED\\t The sum of your attention_weights along dim 1 is not 1.')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oUplFvOw1O_f",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "ed872582-ee89-4afe-d006-81f86c26dc31"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--- TEST: Number of Model Parameters (tests __init__(...)) ---\n",
      "\tPASSED\tInput: {'trg_vocab': ['a', 'aa', 'aaa'], 'embedding_dim': 2, 'hidden_units': 50}\tExpected Num. Params: 21016\tYour Num. Params: 21016\n",
      "\tPASSED\tInput: {'trg_vocab': ['a', 'aa', 'aaa'], 'embedding_dim': 2, 'hidden_units': 100}\tExpected Num. Params: 82016\tYour Num. Params: 82016\n",
      "\tPASSED\tInput: {'trg_vocab': ['a', 'aa', 'aaa'], 'embedding_dim': 2, 'hidden_units': 200}\tExpected Num. Params: 324016\tYour Num. Params: 324016\n",
      "\tPASSED\tInput: {'trg_vocab': ['a', 'aa', 'aaa'], 'embedding_dim': 5, 'hidden_units': 50}\tExpected Num. Params: 21481\tYour Num. Params: 21481\n",
      "\tPASSED\tInput: {'trg_vocab': ['a', 'aa', 'aaa'], 'embedding_dim': 5, 'hidden_units': 100}\tExpected Num. Params: 82931\tYour Num. Params: 82931\n",
      "\tPASSED\tInput: {'trg_vocab': ['a', 'aa', 'aaa'], 'embedding_dim': 5, 'hidden_units': 200}\tExpected Num. Params: 325831\tYour Num. Params: 325831\n",
      "\tPASSED\tInput: {'trg_vocab': ['a', 'aa', 'aaa'], 'embedding_dim': 8, 'hidden_units': 50}\tExpected Num. Params: 21946\tYour Num. Params: 21946\n",
      "\tPASSED\tInput: {'trg_vocab': ['a', 'aa', 'aaa'], 'embedding_dim': 8, 'hidden_units': 100}\tExpected Num. Params: 83846\tYour Num. Params: 83846\n",
      "\tPASSED\tInput: {'trg_vocab': ['a', 'aa', 'aaa'], 'embedding_dim': 8, 'hidden_units': 200}\tExpected Num. Params: 327646\tYour Num. Params: 327646\n",
      "\n",
      "--- TEST: Output shape of forward(...) ---\n",
      "\n",
      "\tPASSED\t Init Input: {embedding_dim: 50, trg_vocab: ['a', 'aa', 'aaa'], batch_size: 1, hidden_units: 50, encoder_outputs: Tensor with shape torch.Size([16, 1, 50]), }\tForward Input Shape (x): torch.Size([1, 1])\tExpected Output Shape: torch.Size([1, 5])\tYour Output Shape: torch.Size([1, 5])\n",
      "\tPASSED\t Init Input: {embedding_dim: 50, trg_vocab: ['a', 'aa', 'aaa'], batch_size: 1, hidden_units: 50, encoder_outputs: Tensor with shape torch.Size([16, 1, 50]), }\tForward Input Shape (x): torch.Size([1, 1])\tExpected Hidden State Shape: torch.Size([1, 1, 50])\tYour Hidden State Shape: torch.Size([1, 1, 50])\n",
      "\tPASSED\t Init Input: {embedding_dim: 50, trg_vocab: ['a', 'aa', 'aaa'], batch_size: 1, hidden_units: 50, encoder_outputs: Tensor with shape torch.Size([16, 1, 50]), }\tForward Input Shape (x): torch.Size([1, 1])\tExpected Attention Weights Shape: torch.Size([1, 16, 1])\tYour Attention Weights Shape: torch.Size([1, 16, 1])\n",
      "\tPASSED\t The sum of your attention_weights along dim 1 is 1.\n",
      "\n",
      "\tPASSED\t Init Input: {embedding_dim: 80, trg_vocab: ['a', 'aa', 'aaa'], batch_size: 2, hidden_units: 50, encoder_outputs: Tensor with shape torch.Size([16, 2, 50]), }\tForward Input Shape (x): torch.Size([2, 1])\tExpected Output Shape: torch.Size([2, 5])\tYour Output Shape: torch.Size([2, 5])\n",
      "\tPASSED\t Init Input: {embedding_dim: 80, trg_vocab: ['a', 'aa', 'aaa'], batch_size: 2, hidden_units: 50, encoder_outputs: Tensor with shape torch.Size([16, 2, 50]), }\tForward Input Shape (x): torch.Size([2, 1])\tExpected Hidden State Shape: torch.Size([1, 2, 50])\tYour Hidden State Shape: torch.Size([1, 2, 50])\n",
      "\tPASSED\t Init Input: {embedding_dim: 80, trg_vocab: ['a', 'aa', 'aaa'], batch_size: 2, hidden_units: 50, encoder_outputs: Tensor with shape torch.Size([16, 2, 50]), }\tForward Input Shape (x): torch.Size([2, 1])\tExpected Attention Weights Shape: torch.Size([2, 16, 1])\tYour Attention Weights Shape: torch.Size([2, 16, 1])\n",
      "\tPASSED\t The sum of your attention_weights along dim 1 is 1.\n",
      "\n",
      "\tPASSED\t Init Input: {embedding_dim: 100, trg_vocab: ['a', 'aa', 'aaa'], batch_size: 4, hidden_units: 50, encoder_outputs: Tensor with shape torch.Size([16, 4, 50]), }\tForward Input Shape (x): torch.Size([4, 1])\tExpected Output Shape: torch.Size([4, 5])\tYour Output Shape: torch.Size([4, 5])\n",
      "\tPASSED\t Init Input: {embedding_dim: 100, trg_vocab: ['a', 'aa', 'aaa'], batch_size: 4, hidden_units: 50, encoder_outputs: Tensor with shape torch.Size([16, 4, 50]), }\tForward Input Shape (x): torch.Size([4, 1])\tExpected Hidden State Shape: torch.Size([1, 4, 50])\tYour Hidden State Shape: torch.Size([1, 4, 50])\n",
      "\tPASSED\t Init Input: {embedding_dim: 100, trg_vocab: ['a', 'aa', 'aaa'], batch_size: 4, hidden_units: 50, encoder_outputs: Tensor with shape torch.Size([16, 4, 50]), }\tForward Input Shape (x): torch.Size([4, 1])\tExpected Attention Weights Shape: torch.Size([4, 16, 1])\tYour Attention Weights Shape: torch.Size([4, 16, 1])\n",
      "\tPASSED\t The sum of your attention_weights along dim 1 is 1.\n",
      "\n",
      "\tPASSED\t Init Input: {embedding_dim: 120, trg_vocab: ['a', 'aa', 'aaa'], batch_size: 1, hidden_units: 100, encoder_outputs: Tensor with shape torch.Size([16, 1, 100]), }\tForward Input Shape (x): torch.Size([1, 1])\tExpected Output Shape: torch.Size([1, 5])\tYour Output Shape: torch.Size([1, 5])\n",
      "\tPASSED\t Init Input: {embedding_dim: 120, trg_vocab: ['a', 'aa', 'aaa'], batch_size: 1, hidden_units: 100, encoder_outputs: Tensor with shape torch.Size([16, 1, 100]), }\tForward Input Shape (x): torch.Size([1, 1])\tExpected Hidden State Shape: torch.Size([1, 1, 100])\tYour Hidden State Shape: torch.Size([1, 1, 100])\n",
      "\tPASSED\t Init Input: {embedding_dim: 120, trg_vocab: ['a', 'aa', 'aaa'], batch_size: 1, hidden_units: 100, encoder_outputs: Tensor with shape torch.Size([16, 1, 100]), }\tForward Input Shape (x): torch.Size([1, 1])\tExpected Attention Weights Shape: torch.Size([1, 16, 1])\tYour Attention Weights Shape: torch.Size([1, 16, 1])\n",
      "\tPASSED\t The sum of your attention_weights along dim 1 is 1.\n",
      "\n",
      "\tPASSED\t Init Input: {embedding_dim: 150, trg_vocab: ['a', 'aa', 'aaa'], batch_size: 2, hidden_units: 100, encoder_outputs: Tensor with shape torch.Size([16, 2, 100]), }\tForward Input Shape (x): torch.Size([2, 1])\tExpected Output Shape: torch.Size([2, 5])\tYour Output Shape: torch.Size([2, 5])\n",
      "\tPASSED\t Init Input: {embedding_dim: 150, trg_vocab: ['a', 'aa', 'aaa'], batch_size: 2, hidden_units: 100, encoder_outputs: Tensor with shape torch.Size([16, 2, 100]), }\tForward Input Shape (x): torch.Size([2, 1])\tExpected Hidden State Shape: torch.Size([1, 2, 100])\tYour Hidden State Shape: torch.Size([1, 2, 100])\n",
      "\tPASSED\t Init Input: {embedding_dim: 150, trg_vocab: ['a', 'aa', 'aaa'], batch_size: 2, hidden_units: 100, encoder_outputs: Tensor with shape torch.Size([16, 2, 100]), }\tForward Input Shape (x): torch.Size([2, 1])\tExpected Attention Weights Shape: torch.Size([2, 16, 1])\tYour Attention Weights Shape: torch.Size([2, 16, 1])\n",
      "\tPASSED\t The sum of your attention_weights along dim 1 is 1.\n",
      "\n",
      "\tPASSED\t Init Input: {embedding_dim: 200, trg_vocab: ['a', 'aa', 'aaa'], batch_size: 4, hidden_units: 100, encoder_outputs: Tensor with shape torch.Size([16, 4, 100]), }\tForward Input Shape (x): torch.Size([4, 1])\tExpected Output Shape: torch.Size([4, 5])\tYour Output Shape: torch.Size([4, 5])\n",
      "\tPASSED\t Init Input: {embedding_dim: 200, trg_vocab: ['a', 'aa', 'aaa'], batch_size: 4, hidden_units: 100, encoder_outputs: Tensor with shape torch.Size([16, 4, 100]), }\tForward Input Shape (x): torch.Size([4, 1])\tExpected Hidden State Shape: torch.Size([1, 4, 100])\tYour Hidden State Shape: torch.Size([1, 4, 100])\n",
      "\tPASSED\t Init Input: {embedding_dim: 200, trg_vocab: ['a', 'aa', 'aaa'], batch_size: 4, hidden_units: 100, encoder_outputs: Tensor with shape torch.Size([16, 4, 100]), }\tForward Input Shape (x): torch.Size([4, 1])\tExpected Attention Weights Shape: torch.Size([4, 16, 1])\tYour Attention Weights Shape: torch.Size([4, 16, 1])\n",
      "\tPASSED\t The sum of your attention_weights along dim 1 is 1.\n",
      "\n",
      "\tPASSED\t Init Input: {embedding_dim: 300, trg_vocab: ['a', 'aa', 'aaa'], batch_size: 1, hidden_units: 200, encoder_outputs: Tensor with shape torch.Size([16, 1, 200]), }\tForward Input Shape (x): torch.Size([1, 1])\tExpected Output Shape: torch.Size([1, 5])\tYour Output Shape: torch.Size([1, 5])\n",
      "\tPASSED\t Init Input: {embedding_dim: 300, trg_vocab: ['a', 'aa', 'aaa'], batch_size: 1, hidden_units: 200, encoder_outputs: Tensor with shape torch.Size([16, 1, 200]), }\tForward Input Shape (x): torch.Size([1, 1])\tExpected Hidden State Shape: torch.Size([1, 1, 200])\tYour Hidden State Shape: torch.Size([1, 1, 200])\n",
      "\tPASSED\t Init Input: {embedding_dim: 300, trg_vocab: ['a', 'aa', 'aaa'], batch_size: 1, hidden_units: 200, encoder_outputs: Tensor with shape torch.Size([16, 1, 200]), }\tForward Input Shape (x): torch.Size([1, 1])\tExpected Attention Weights Shape: torch.Size([1, 16, 1])\tYour Attention Weights Shape: torch.Size([1, 16, 1])\n",
      "\tPASSED\t The sum of your attention_weights along dim 1 is 1.\n",
      "\n",
      "\tPASSED\t Init Input: {embedding_dim: 400, trg_vocab: ['a', 'aa', 'aaa'], batch_size: 2, hidden_units: 200, encoder_outputs: Tensor with shape torch.Size([16, 2, 200]), }\tForward Input Shape (x): torch.Size([2, 1])\tExpected Output Shape: torch.Size([2, 5])\tYour Output Shape: torch.Size([2, 5])\n",
      "\tPASSED\t Init Input: {embedding_dim: 400, trg_vocab: ['a', 'aa', 'aaa'], batch_size: 2, hidden_units: 200, encoder_outputs: Tensor with shape torch.Size([16, 2, 200]), }\tForward Input Shape (x): torch.Size([2, 1])\tExpected Hidden State Shape: torch.Size([1, 2, 200])\tYour Hidden State Shape: torch.Size([1, 2, 200])\n",
      "\tPASSED\t Init Input: {embedding_dim: 400, trg_vocab: ['a', 'aa', 'aaa'], batch_size: 2, hidden_units: 200, encoder_outputs: Tensor with shape torch.Size([16, 2, 200]), }\tForward Input Shape (x): torch.Size([2, 1])\tExpected Attention Weights Shape: torch.Size([2, 16, 1])\tYour Attention Weights Shape: torch.Size([2, 16, 1])\n",
      "\tPASSED\t The sum of your attention_weights along dim 1 is 1.\n",
      "\n",
      "\tPASSED\t Init Input: {embedding_dim: 500, trg_vocab: ['a', 'aa', 'aaa'], batch_size: 4, hidden_units: 200, encoder_outputs: Tensor with shape torch.Size([16, 4, 200]), }\tForward Input Shape (x): torch.Size([4, 1])\tExpected Output Shape: torch.Size([4, 5])\tYour Output Shape: torch.Size([4, 5])\n",
      "\tPASSED\t Init Input: {embedding_dim: 500, trg_vocab: ['a', 'aa', 'aaa'], batch_size: 4, hidden_units: 200, encoder_outputs: Tensor with shape torch.Size([16, 4, 200]), }\tForward Input Shape (x): torch.Size([4, 1])\tExpected Hidden State Shape: torch.Size([1, 4, 200])\tYour Hidden State Shape: torch.Size([1, 4, 200])\n",
      "\tPASSED\t Init Input: {embedding_dim: 500, trg_vocab: ['a', 'aa', 'aaa'], batch_size: 4, hidden_units: 200, encoder_outputs: Tensor with shape torch.Size([16, 4, 200]), }\tForward Input Shape (x): torch.Size([4, 1])\tExpected Attention Weights Shape: torch.Size([4, 16, 1])\tYour Attention Weights Shape: torch.Size([4, 16, 1])\n",
      "\tPASSED\t The sum of your attention_weights along dim 1 is 1.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Set random seed\n",
    "    torch.manual_seed(42)\n",
    "    # Create test inputs\n",
    "    embedding_dim = [2, 5, 8]\n",
    "    hidden_units = [50, 100, 200]\n",
    "    sanity_vocab = Vocab_Lang(vocab=[\"a\", \"aa\", \"aaa\"])\n",
    "    params = []\n",
    "    inputs = []\n",
    "    for ed in embedding_dim:\n",
    "        for hu in hidden_units:\n",
    "            inp = {}\n",
    "            inp['trg_vocab'] = sanity_vocab\n",
    "            inp['embedding_dim'] = ed\n",
    "            inp['hidden_units'] = hu\n",
    "            inputs.append(inp)\n",
    "    # Test init\n",
    "    expected_outputs = [21016, 82016, 324016, 21481, 82931, 325831, 21946, 83846, 327646]\n",
    "    sanityCheckModel(inputs, RnnDecoder, expected_outputs, \"init\")\n",
    "    print()\n",
    "\n",
    "    # Test forward\n",
    "    inputs = []\n",
    "    hidden_units = [50, 100, 200]\n",
    "    batch_sizes = [1, 2, 4]\n",
    "    embedding_dims = iter([50,80,100,120,150,200,300,400,500])\n",
    "    encoder_outputs = iter([torch.rand([16, 1, 50]), torch.rand([16, 2, 50]), torch.rand([16, 4, 50]), torch.rand([16, 1, 100]), torch.rand([16, 2, 100]), torch.rand([16, 4, 100]), torch.rand([16, 1, 200]), torch.rand([16, 2, 200]),torch.rand([16, 4, 200])])\n",
    "    expected_fc_outs = [torch.Size([1, 5]),torch.Size([2, 5]),torch.Size([4, 5]),torch.Size([1, 5]),torch.Size([2, 5]),torch.Size([4, 5]),torch.Size([1, 5]),torch.Size([2, 5]),torch.Size([4, 5])]\n",
    "    expected_dec_hs = [torch.Size([1, 1, 50]), torch.Size([1, 2, 50]), torch.Size([1, 4, 50]), torch.Size([1, 1, 100]), torch.Size([1, 2, 100]), torch.Size([1, 4, 100]), torch.Size([1, 1, 200]), torch.Size([1, 2, 200]), torch.Size([1, 4, 200])]\n",
    "    expected_attention_weights = [torch.Size([1, 16, 1]), torch.Size([2, 16, 1]), torch.Size([4, 16, 1]), torch.Size([1, 16, 1]), torch.Size([2, 16, 1]), torch.Size([4, 16, 1]), torch.Size([1, 16, 1]), torch.Size([2, 16, 1]), torch.Size([4, 16, 1])]\n",
    "    expected_outputs = (expected_fc_outs, expected_dec_hs, expected_attention_weights)\n",
    "\n",
    "    for hu in hidden_units:\n",
    "        for b in batch_sizes:\n",
    "            inp = {}\n",
    "            edim = next(embedding_dims)\n",
    "            inp['embedding_dim'] = edim\n",
    "            inp['trg_vocab'] = sanity_vocab\n",
    "            inp[\"batch_size\"] = b\n",
    "            inp['hidden_units'] = hu\n",
    "            inp['encoder_outputs'] = next(encoder_outputs)\n",
    "            inputs.append(inp)\n",
    "\n",
    "    sanityCheckDecoderModelForward(inputs, RnnDecoder, expected_outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQKYT5w3n82V"
   },
   "source": [
    "## Train RNN Model\n",
    "\n",
    "We will train the encoder and decoder using cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IyPi_PfkFtSr"
   },
   "outputs": [],
   "source": [
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = real.ge(1).float() # Only consider non-zero inputs in the loss\n",
    "\n",
    "    loss_ = F.cross_entropy(pred, real) * mask\n",
    "    return torch.mean(loss_)\n",
    "\n",
    "def train_rnn_model(encoder, decoder, dataset, optimizer, trg_vocab, device, n_epochs):\n",
    "    batch_size = dataset.batch_size\n",
    "    for epoch in range(n_epochs):\n",
    "        start = time.time()\n",
    "        n_batch = 0\n",
    "        total_loss = 0\n",
    "\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "\n",
    "        for src, trg in tqdm(dataset):\n",
    "            n_batch += 1\n",
    "            loss = 0\n",
    "\n",
    "            enc_output, enc_hidden = encoder(src.transpose(0,1).to(device))\n",
    "            dec_hidden = enc_hidden\n",
    "\n",
    "            # use teacher forcing - feeding the target as the next input (via dec_input)\n",
    "            dec_input = torch.tensor([[trg_vocab.word2idx['<start>']]] * batch_size)\n",
    "\n",
    "            # run code below for every timestep in the ys batch\n",
    "            for t in range(1, trg.size(1)):\n",
    "                predictions, dec_hidden, _ = decoder(dec_input.to(device), dec_hidden.to(device), enc_output.to(device))\n",
    "                assert len(predictions.shape) == 2 and predictions.shape[0] == dec_input.shape[0] and predictions.shape[1] == len(trg_vocab.word2idx), \"First output of decoder must have shape [batch_size, vocab_size], you returned shape \" + str(predictions.shape)\n",
    "                loss += loss_function(trg[:, t].to(device), predictions.to(device))\n",
    "                dec_input = trg[:, t].unsqueeze(1)\n",
    "\n",
    "            batch_loss = (loss / int(trg.size(1)))\n",
    "            total_loss += batch_loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            batch_loss.backward()\n",
    "\n",
    "            ### update model parameters\n",
    "            optimizer.step()\n",
    "\n",
    "        print('Epoch:{:2d}/{}\\t Loss: {:.4f} \\t({:.2f}s)'.format(epoch + 1, n_epochs, total_loss / n_batch, time.time() - start))\n",
    "\n",
    "    print('Model trained!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IjbsUkcpNK9W",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "f1b04810-629e-4ebe-9fe7-8d111caba482"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Encoder and Decoder models initialized!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # HYPERPARAMETERS\n",
    "    LEARNING_RATE = 0.001\n",
    "    HIDDEN_UNITS=256\n",
    "    N_EPOCHS=10\n",
    "\n",
    "    rnn_encoder = RnnEncoder(src_vocab, EMBEDDING_DIM, HIDDEN_UNITS).to(DEVICE)\n",
    "    rnn_decoder = RnnDecoder(trg_vocab, EMBEDDING_DIM, HIDDEN_UNITS).to(DEVICE)\n",
    "\n",
    "    rnn_model_params = list(rnn_encoder.parameters()) + list(rnn_decoder.parameters())\n",
    "    optimizer = torch.optim.Adam(rnn_model_params, lr=LEARNING_RATE)\n",
    "\n",
    "    print('Encoder and Decoder models initialized!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OPosimvgdx_O",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 528,
     "referenced_widgets": [
      "24f0b827a6ef40ccb161d9445a392a3a",
      "e70ada419dd941e9888d8ae29aaf52c9",
      "9139d4ceb5bf47c7ad2c05c543d682ef",
      "bd3820bad34149ccbe253f7ce5d7ac35",
      "0edb90ce007747f0a9b67ae7a42557c2",
      "46a15d92c6644ec2881b5c0b97a4c717",
      "148fdb9b355a47cdb4607d44725d6f1d",
      "4592a45b5a224a88b4803ab45776347a",
      "c052acd988f94f9d8f084bbe8128ae61",
      "7447f1f234e249dd9c0adb7ccf216f9f",
      "46d8a6e2d1834ac3864abaef7f0d9fc8",
      "c357e56ef3b846bf89f5fff0623ba9a0",
      "8921a2e98067465e9e90c8813186639c",
      "c2a23428afb242cc8304bc51c811f838",
      "d4a3807f5bfd4c17980c58389aaf853c",
      "701c9fa3191e45ba80b385a2b17dcb7a",
      "d7afaa3ed7184a77b14bcc9e8bfe905d",
      "8b9b4cf6c0b94c0aadaca7ec98737185",
      "3f6e3810a2814fae9d6c7c7d9dda60f9",
      "566a5170f6714203ac1100bdd078f5d2",
      "d19a4ffefec34339a3c3124816169b77",
      "a3865727e4604a6aae1a2d5ed4a900dd",
      "0646bae5e3f44b8ea9a2fd19b9db1eb6",
      "b54f59c1a7304d7988f9ee695a21858c",
      "2eb3f5d535d0406093afd396b98dfba6",
      "8b61317b58c54e63a02e87b962e993ef",
      "ebdc037aab54460ca9e8242c74c41029",
      "1752d277a75e4af485079b8940bea2ce",
      "1b57c16c2d524788bba0bdef966da647",
      "91a3cfc13d2a4938b0c7a0134b2ce95b",
      "3223219235b4407c9d9fb75b8dfe46aa",
      "04b943acb0c940f78ead928c6a3fa078",
      "55f3eb9f92d34bd491657e2fa7173aeb",
      "4f9b8e9cce38431bb247e6adae268205",
      "03414d657fef4a2b96e26d33c0f68765",
      "cd3660d7f2cf4f9485cb0ca7bc022ce3",
      "71ae0a2ae7b94163abd4df8b38e7f55a",
      "e123e964475247b2ad4d8e05071ed2db",
      "67078f639a4d4d24b2b4d1804bfc8f0e",
      "ffd451f7b5e44b898c73168f1a989acb",
      "39c56efcab4b49e7a0e2a556ca885373",
      "fcb30a1b877845648d654a289e0e9984",
      "60a1f58309bf4158a1cc3e851568b323",
      "17fa763d214049319318c499edec6d11",
      "662b08704e4e49cf98e5e39b785de878",
      "c92179d48bd84c0282e011f0710d0763",
      "2774bda01b1349019f050d9d86fcf2ef",
      "45def3e4c6bd40ae821e4c9499688e62",
      "008c1e62541e41b38ca8f623deb8532c",
      "861b79a03ecf4097b88e7e62dc5a7925",
      "b132c9291d88405c9d66c60ae9c2138a",
      "887886fc55b0477ca0fefb89c97f7561",
      "ce0fd94b7cf845bd86d5257edf82163a",
      "68d377ba73c647a59de5e17359e80100",
      "edf7fb668f02465e94ab5f9b1258a120",
      "9b55a8a50d84419aae966433a2d8dd64",
      "3f683f2d6df049ae8e0cf3e4d1a33173",
      "0de7f13d8b744751b0e3eac14955a9fc",
      "1f2dc15dfe294fc68343d83a3cad26d6",
      "b73e28f764e44575a4f1fe3e65904d28",
      "cc301458766b4698ad6f60bc4aefa54c",
      "51fa7b8847fe4de6bf7ae8f175b508d4",
      "f6e88c758ee74e398079d60ced04874e",
      "19d42a3e33824cb38382b3561cef048d",
      "9c34a0b99f104dedaa5cfba25aa06379",
      "a02a324101dc49c4be31e37af13e8437",
      "0d65c89f83fa420987bc80b3dae3e3e5",
      "eb94920651fd426ea9ae3a53688e9767",
      "374307ee8bd94b3b8743cb3a30b5d72a",
      "640e4a35891a4d9e84f843e034f14f21",
      "ce87f5ab941943a3912869cf14052c1f",
      "dd081548dcee4e9798893f32c978913c",
      "30707dc0e5ad4a23996c29258aa7de3a",
      "9de52448c89d43a681cf2cc585ed9e54",
      "b4df1d7d10e1453997e41389992a717c",
      "1da4df85ebfa4d12950a9ed9f5b1f9f4",
      "0072c156ac754b87b081a4f75e8e615d",
      "7c751e2a29974139890fc03578f88bbc",
      "4090aa6e5a3b499da7adb7966ca9afeb",
      "778ca26ee0904e9595ae1a7e9a4c4a74",
      "8f38fece65bd41fd9e6d1b21d117b614",
      "1e3975bb2ff644759c3c220b0aabefca",
      "580194f0088b4aa589c412fa203f74a5",
      "857c22dff78d4ce08fb1a4974291a752",
      "915cb6b622544c59ad5cc3270fcc0649",
      "fb972b6c38e24fbdaaf7952f0b6a1687",
      "c960ee977a6d46a6a3d70e8d49bfdc70",
      "b13be62ceb7b4c029c99fd618f2df6c8",
      "bfdb03f32dcd406c956518657c26ed12",
      "c8f384c743004298a8def2c89ef4bf41",
      "8333c1f6578741528dd94a0e14b57338",
      "c21cea87dcf744849f202379b2a3f7fc",
      "50a9d8dafba0409d80ec0e7bdd8331dc",
      "0d6a4b92d1df491d8420b6b58e52b5ac",
      "58719d45e14b46b9b9571d351ddbb80a",
      "11c6d322ce974e2498a3f032e5c05c01",
      "b8643d17989944169ea3bb899b05432b",
      "5a6070b0301a446ba54188fe46360052",
      "b0645de7787a450e9fee2c533ec911c2",
      "d718445d0081420b9f75d0c767d14b16",
      "3950b51fc7024ce595ffaf7303cfce7b",
      "1b0246d11b9640489ffcfe8d6768975b",
      "37639abd6fb646c39efe221c92bf9a88",
      "d68d37b03a6042caa2f97ebe58f9c75c",
      "9eb9941d1c964328b8a9986785431858",
      "b7b0f1bded014628926bc36c3dd1bf49",
      "1789310728e84b3c9156df5276993657",
      "2f0578db568c4fd38ba81918f9782b26",
      "e836fa89512a49a990cab097034ca011",
      "5777a0861f4b47dca2341d18864a8096"
     ]
    },
    "outputId": "a1097b3a-0405-4db9-838d-ad52b15a529d"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/468 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "24f0b827a6ef40ccb161d9445a392a3a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 1/10\t Loss: 1.7197 \t(16.17s)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/468 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c357e56ef3b846bf89f5fff0623ba9a0"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 2/10\t Loss: 1.0464 \t(17.23s)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/468 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0646bae5e3f44b8ea9a2fd19b9db1eb6"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 3/10\t Loss: 0.7378 \t(16.26s)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/468 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4f9b8e9cce38431bb247e6adae268205"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 4/10\t Loss: 0.5225 \t(16.17s)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/468 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "662b08704e4e49cf98e5e39b785de878"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 5/10\t Loss: 0.3691 \t(16.62s)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/468 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9b55a8a50d84419aae966433a2d8dd64"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 6/10\t Loss: 0.2598 \t(17.83s)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/468 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0d65c89f83fa420987bc80b3dae3e3e5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 7/10\t Loss: 0.1844 \t(17.00s)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/468 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7c751e2a29974139890fc03578f88bbc"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 8/10\t Loss: 0.1363 \t(17.73s)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/468 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bfdb03f32dcd406c956518657c26ed12"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 9/10\t Loss: 0.1038 \t(17.43s)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/468 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d718445d0081420b9f75d0c767d14b16"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch:10/10\t Loss: 0.0832 \t(17.85s)\n",
      "Model trained!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    train_rnn_model(rnn_encoder, rnn_decoder, train_dataset, optimizer, trg_vocab, DEVICE, N_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step 3: Inference Functions\n",
    "\n",
    "Now that we have trained the model, we can use it on test data. But first, I must *decode* from the model: given an input (Spanish sentence), what is the most likely output (English sentence)?\n",
    "\n",
    "Recall that sequence-to-sequence models like RNN's factorize the probability distribution of output sequence $\\mathbf{y}$ given input sequence $\\mathbf{x}$ as $$P(\\mathbf{y} \\mid \\mathbf{x})=\\prod _{t=1}^{|\\mathbf{y}|}P(y_t\\mid y_0 \\cdots y_{t-1}, \\mathbf{x})$$\n",
    "\n",
    "where $y_0$ is the start-of-sentence token.\n",
    "\n",
    "First, y exploring how to *sample* from this distribution. Then, implementing a decoding method, which aims to find $\\mathrm{argmax}_\\mathbf{y} P(\\mathbf{y} \\mid \\mathbf{x})$. Because there are infinitely many sequences $\\mathbf{y}$, it is not possible to directly optimize this quantity; thus we resort to *beam search decoding*, a heuristic method."
   ],
   "metadata": {
    "id": "vOKT1peUxUYR"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "szVF1Yn-yoZP"
   },
   "source": [
    "## Temperature Sampling\n",
    "\n",
    "Temperature sampling is a method to sample from $P(\\mathbf{y}\\mid \\mathbf{x})$. At each timestep, the softmax function $\\sigma(\\mathbf{z})$ transforms the output logits $\\mathbf{z}$ of the final layer of the transformer to a probability distribution. To apply temperature $T$, we use a modified softmax function to produce the distribution: $$P(z_i)=\\frac{\\exp (z_i\\,/\\,T)}{\\sum _j \\exp (z_j\\,/\\,T)}$$ What is the effect of applying temperature?\n",
    "* If $T=1$, then this is equivalent to the softmax function $-$ hence, the unmodified probability distributon learned by the transformer.\n",
    "* As $T \\rightarrow \\infty$, this approaches the uniform distribution.\n",
    "* As $T \\rightarrow 0$, the distribution becomes increasingly \"peaked\": that is, the probability of the most likely token increases to $1$.\n",
    "\n",
    "Temperature sampling is implemented in the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B76wtwJDyoZP"
   },
   "outputs": [],
   "source": [
    "def sample_model(encoder, decoder, src, max_decode_len, temperature):\n",
    "    \"\"\"\n",
    "    Perform temperature sampling of the target sentence for each source sentence in src based on the trained encoder and decoder.\n",
    "\n",
    "    Args:\n",
    "        encoder: Your RnnEncoder object\n",
    "        decoder: Your RnnDecoder object\n",
    "        src: [max_src_length, batch_size] the source sentences you wish to translate\n",
    "        max_decode_len: The maximum desired length (int) of your target translated sentences\n",
    "        temperature: The temperature in temperature sampling\n",
    "\n",
    "    Returns:\n",
    "        sentences: A list of length batch_size, where sentences[i] is a list containing vocab indexes of the sampled\n",
    "            trg sentence for the ith item in the src batch.\n",
    "\n",
    "    Pseudo-code:\n",
    "    - Obtain encoder output and hidden state by encoding src sentences\n",
    "    - For 1 \u2264 t < max_decode_len:\n",
    "        - Obtain dec_input as the best words so far for previous time steps (you can get this from curr_output)\n",
    "        - Obtain your prediction logits and hidden state by feeding dec_input, enc_output, and dec_hidden to decoder\n",
    "        - Sample from the distribution over tokens (consider using torch.distributions.Categorical) with temperature\n",
    "        - Save result in curr_output at index t\n",
    "    - Truncate each sentence in the batch at EOS before returning them as a list of lists of vocab indexes\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize variables\n",
    "    trg_vocab = decoder.trg_vocab\n",
    "    batch_size = src.size(1)\n",
    "    curr_output = torch.zeros((batch_size, max_decode_len))\n",
    "    eos_idx = trg_vocab.word2idx['<end>']\n",
    "    device = src.device\n",
    "    is_rnn = 'Rnn' in encoder.__class__.__name__\n",
    "    sentences = []\n",
    "\n",
    "    # We start the decoding with the start token for each example\n",
    "    dec_input = torch.tensor([[trg_vocab.word2idx['<start>']]] * batch_size, device=device)\n",
    "    curr_output[:, 0] = dec_input.squeeze(1)\n",
    "\n",
    "    # Obtain encoder outputs\n",
    "    if is_rnn: enc_output, dec_hidden = encoder(src)\n",
    "    else: enc_output, dec_hidden = encoder(src), None\n",
    "    enc_output, dec_hidden = enc_output.to(device), dec_hidden.to(device) if dec_hidden is not None else None\n",
    "\n",
    "    #   At each time step, sample the next token and save it in curr_output\n",
    "    curr_output = curr_output.long()\n",
    "    enc_output = enc_output.to(device)\n",
    "    if dec_hidden is not None:\n",
    "        dec_hidden = dec_hidden.to(device)\n",
    "\n",
    "    for t in range(1, max_decode_len):\n",
    "        logits, dec_hidden = decoder.decode_step(curr_output[:, :t].to(device), enc_output, dec_hidden)\n",
    "        probs = F.softmax(logits/temperature, dim=-1)  # [batch_size, vocab_size]\n",
    "        next_token = torch.distributions.Categorical(probs).sample()\n",
    "\n",
    "        curr_output[:, t] = next_token\n",
    "\n",
    "\n",
    "    #   For each item in the batch, truncate the sentence to EOS; if EOS was not predicted, choose sentence up to max_decode_len\n",
    "    for i in range(batch_size):\n",
    "        sentence = curr_output[i].tolist()\n",
    "        if eos_idx in sentence:\n",
    "            eos_pos = sentence.index(eos_idx)\n",
    "            sentences.append(sentence[:eos_pos+1])\n",
    "        else:\n",
    "            sentence = sentence[:max_decode_len]\n",
    "            if sentence[-1] != eos_idx:\n",
    "                sentence.append(eos_idx)\n",
    "            sentences.append(sentence)\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VSbHm2Ua1PW6"
   },
   "source": [
    "Qualitatively compare some of the sentences generated by model with the some of the correct translations."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "if __name__ == '__main__':\n",
    "    rnn_encoder.eval()\n",
    "    rnn_decoder.eval()\n",
    "    idxes = random.choices(range(len(test_dataset.dataset)), k=5)\n",
    "    src, trg =  train_dataset.dataset[idxes]\n",
    "    results = {}\n",
    "    for temp in [0.01, 1.0, 2.0]:\n",
    "        results[temp] = sample_model(rnn_encoder, rnn_decoder, src.transpose(0,1).to(DEVICE), trg.size(1), temp)\n",
    "    for i in range(len(src)):\n",
    "        print(\"Source sentence:\\t\\t\", ' '.join([x for x in [src_vocab.idx2word[j.item()] for j in src[i]] if x != '<pad>']))\n",
    "        print(\"Target sentence:\\t\\t\", ' '.join([x for x in [trg_vocab.idx2word[j.item()] for j in trg[i]] if x != '<pad>']))\n",
    "        for temp in results:\n",
    "            print(\"Sampled sentence (T=\"+str(temp)+\"):\\t\", ' '.join([x for x in [trg_vocab.idx2word[j] for j in results[temp][i]] if x != '<pad>']))\n",
    "        print(\"----------------\")"
   ],
   "metadata": {
    "id": "YwQXVMEE1Chr",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "001f3c55-f004-47eb-9302-98abdc5f3cf9"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Source sentence:\t\t <start> eso pienso yo . <end>\n",
      "Target sentence:\t\t <start> i do think so . <end>\n",
      "Sampled sentence (T=0.01):\t <start> i think so . <end>\n",
      "Sampled sentence (T=1.0):\t <start> i do think so . <end>\n",
      "Sampled sentence (T=2.0):\t <start> i now help yet . <end>\n",
      "----------------\n",
      "Source sentence:\t\t <start> usted sabe algo . <end>\n",
      "Target sentence:\t\t <start> you know something . <end>\n",
      "Sampled sentence (T=0.01):\t <start> you know something . <end>\n",
      "Sampled sentence (T=1.0):\t <start> you know something . <end>\n",
      "Sampled sentence (T=2.0):\t <start> traveled check something . <end>\n",
      "----------------\n",
      "Source sentence:\t\t <start> puede que el cambie de idea . <end>\n",
      "Target sentence:\t\t <start> he might change his mind . <end>\n",
      "Sampled sentence (T=0.01):\t <start> he may return of her . <end>\n",
      "Sampled sentence (T=1.0):\t <start> he may keep idea . <end>\n",
      "Sampled sentence (T=2.0):\t <start> he sure like smile support . <end>\n",
      "----------------\n",
      "Source sentence:\t\t <start> estoy viviendo mi sueno . <end>\n",
      "Target sentence:\t\t <start> i m living my dream . <end>\n",
      "Sampled sentence (T=0.01):\t <start> i m living my dream . <end>\n",
      "Sampled sentence (T=1.0):\t <start> i m living my dream . <end>\n",
      "Sampled sentence (T=2.0):\t <start> i remains skirt my dream . <end>\n",
      "----------------\n",
      "Source sentence:\t\t <start> \u00bf cual es la proxima estacion ? <end>\n",
      "Target sentence:\t\t <start> what s the next station ? <end>\n",
      "Sampled sentence (T=0.01):\t <start> what is the next station ? <end>\n",
      "Sampled sentence (T=1.0):\t <start> what s the next station ? <end>\n",
      "Sampled sentence (T=2.0):\t <start> owed if this up christmas ? honorable ? <end>\n",
      "----------------\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2mqLidH62lJh"
   },
   "source": [
    "## Beam Search Decoding\n",
    "\n",
    "Here, instead of sampling sequences from $P(\\mathbf{y}\\mid \\mathbf{x})$, I will find the maximum *a-posteriori* estimate $\\mathbf{\\hat y} = \\mathrm{argmax}_{\\mathbf{y}} P(\\mathbf{y}\\mid \\mathbf{x})$. Because it is not possible to search over all sequences $\\mathbf{y}$, we use a heuristic method called beam search decoding.\n",
    "\n",
    "The idea is as follows: at each timestep, you keep track of the $K$ best hypotheses (in terms of probability) seen up to that time that have not terminated; all terminated hypotheses (those that have predicted the end-of-sentence token) that are encountered should be saved along with their log probability. Once you have proceeded for the maximum number of timesteps, the final hypothesis is the one among the terminated hypotheses that has highest probability.\n",
    "\n",
    "However, to avoid bias toward shorter hypotheses, we apply a length penalty (described on page 12 of [this paper](https://arxiv.org/pdf/1609.08144)) when selecting the final hypothesis. Specifically, out of the hypotheses that have terminated, we select the one that maximizes the score $$\\frac{\\log P(\\mathbf{y}\\mid \\mathbf{x})}{lp}$$ where the length penalty $lp$ is defined as $$lp = \\Big (\\frac{5+t}{5+1}\\Big )^\\alpha $$ The higher the hyperparameter $\\alpha$ is, the more short hypotheses are penalized.\n",
    "\n",
    "First implementing 3 helper functions"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def sort_predictions(predictions):\n",
    "    '''\n",
    "    Sort prediction probabilities in descending order, and keep track of which hypothesis and vocab index produced each.\n",
    "\n",
    "    Args:\n",
    "        predictions: A tensor of size [beam_size, trg_vocab_size] containing log probabilities.\n",
    "\n",
    "    Returns:\n",
    "        probs: A tensor of size beam_size*trg_vocab_size containing all log probabilities sorted in descending order. Each row correponds to a separate\n",
    "           hypothesis.\n",
    "        idxes: A tensor of size beam_size*trg_vocab_size, where idxes[i] indicates which index in the vocabulary probs[i] corresponds\n",
    "           to. Thus, 0 \u2264 idxes[i] < trg_vocab_size.\n",
    "        hypothesis_ids: A tensor of size beam_size*trg_vocab_size where hypothesis_ids[i] indicates which hypothesis probs[i]\n",
    "           corresonds to. Thus, 0 \u2264 hypothesis_ids[i] < beam_size.\n",
    "    '''\n",
    "    assert len(predictions.shape) == 2, 'predictions should be a 2d tensor'\n",
    "    beam_size, trg_vocab_size = predictions.shape[0], predictions.shape[1]\n",
    "    probs = torch.zeros(beam_size * trg_vocab_size, device=predictions.device)\n",
    "    idxes = torch.zeros(beam_size * trg_vocab_size, device=predictions.device)\n",
    "    hypothesis_ids = torch.zeros(beam_size * trg_vocab_size, device=predictions.device)\n",
    "\n",
    "    flat_probs = predictions.view(-1)\n",
    "    sorted_probs, sorted_indices = torch.sort(flat_probs, descending=True)\n",
    "\n",
    "    probs = sorted_probs\n",
    "    hypothesis_ids = sorted_indices // trg_vocab_size\n",
    "    idxes = sorted_indices % trg_vocab_size\n",
    "\n",
    "    return probs, idxes, hypothesis_ids"
   ],
   "metadata": {
    "id": "6GiQasDuV9Db"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The cell below will provide a sanity check for this function."
   ],
   "metadata": {
    "id": "5gsfQwW6bNGT"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "def sanityCheckSortPredictions():\n",
    "    predictions_list = [torch.tensor([[-9.3672, -10.1368],[-5.2810, -7.4104],[-6.2810, -8.4154]]),\n",
    "                        torch.tensor([[-5.2810, -7.4104],[-9.3672, -10.1368],[-6.2810, -8.4154]]),\n",
    "                        torch.tensor([[-6.2810, -8.4154],[-5.2810, -7.4104],[-9.3672, -10.1368]]),\n",
    "                        torch.tensor([[-10.4256, -4.9818],[-5.0244, -8.7471],[-7.2406, -6.3092]]),\n",
    "                        torch.tensor([[-7.5414, -7.9009],[-8.2827, -8.5210],[-9.2406, -9.3092]])]\n",
    "    expected_outputs = [(torch.tensor([ -5.2810,  -6.2810,  -7.4104,  -8.4154,  -9.3672, -10.1368]),\n",
    "                         torch.tensor([0, 0, 1, 1, 0, 1]), torch.tensor([1, 2, 1, 2, 0, 0])),\n",
    "                        (torch.tensor([ -5.2810,  -6.2810,  -7.4104,  -8.4154,  -9.3672, -10.1368]),\n",
    "                         torch.tensor([0, 0, 1, 1, 0, 1]), torch.tensor([0, 2, 0, 2, 1, 1])),\n",
    "                        (torch.tensor([ -5.2810,  -6.2810,  -7.4104,  -8.4154,  -9.3672, -10.1368]),\n",
    "                         torch.tensor([0, 0, 1, 1, 0, 1]), torch.tensor([1, 0, 1, 0, 2, 2])),\n",
    "                        (torch.tensor([ -4.9818,  -5.0244,  -6.3092,  -7.2406,  -8.7471, -10.4256]),\n",
    "                         torch.tensor([1, 0, 1, 0, 1, 0]), torch.tensor([0, 1, 2, 2, 1, 0])),\n",
    "                        (torch.tensor([-7.5414, -7.9009, -8.2827, -8.5210, -9.2406, -9.3092]),\n",
    "                         torch.tensor([0, 1, 0, 1, 0, 1]), torch.tensor([0, 0, 1, 1, 2, 2]))]\n",
    "\n",
    "    print(\"--- TEST: sort_predictions() outputs ---\")\n",
    "    print()\n",
    "    for i in range(len(predictions_list)):\n",
    "        probs, idxes, hypotheses_ids = sort_predictions(predictions_list[i])\n",
    "        if torch.equal(probs, expected_outputs[i][0]):\n",
    "            status = \"PASSED\"\n",
    "        else:\n",
    "            status = \"FAILED\"\n",
    "        message = '\\t' + status + \"\\tInput: Tensor of shape \" + str(predictions_list[i].shape) + ('\\tExpected probs: ' + str(expected_outputs[i][0]) + '\\tYour probs: '+ str(probs))\n",
    "        print(message)\n",
    "\n",
    "        if torch.equal(idxes, expected_outputs[i][1]):\n",
    "            status = \"PASSED\"\n",
    "        else:\n",
    "            status = \"FAILED\"\n",
    "        message = '\\t' + status + \"\\tInput: Tensor of shape \" + str(predictions_list[i].shape) + ('\\tExpected idxes: ' + str(expected_outputs[i][1]) + '\\tYour idxes: '+ str(idxes))\n",
    "        print(message)\n",
    "\n",
    "        if torch.equal(hypotheses_ids, expected_outputs[i][2]):\n",
    "            status = \"PASSED\"\n",
    "        else:\n",
    "            status = \"FAILED\"\n",
    "        message = '\\t' + status + \"\\tInput: Tensor of shape \" + str(predictions_list[i].shape) + ('\\tExpected hypotheses_ids: ' + str(expected_outputs[i][2]) + '\\tYour hypotheses_ids: '+ str(hypotheses_ids))\n",
    "        print(message)\n",
    "        print()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    sanityCheckSortPredictions()"
   ],
   "metadata": {
    "id": "YMykJqAiayyQ",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "73ffb8e8-4a40-4c30-b1ee-e5c562cf8708"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--- TEST: sort_predictions() outputs ---\n",
      "\n",
      "\tPASSED\tInput: Tensor of shape torch.Size([3, 2])\tExpected probs: tensor([ -5.2810,  -6.2810,  -7.4104,  -8.4154,  -9.3672, -10.1368])\tYour probs: tensor([ -5.2810,  -6.2810,  -7.4104,  -8.4154,  -9.3672, -10.1368])\n",
      "\tPASSED\tInput: Tensor of shape torch.Size([3, 2])\tExpected idxes: tensor([0, 0, 1, 1, 0, 1])\tYour idxes: tensor([0, 0, 1, 1, 0, 1])\n",
      "\tPASSED\tInput: Tensor of shape torch.Size([3, 2])\tExpected hypotheses_ids: tensor([1, 2, 1, 2, 0, 0])\tYour hypotheses_ids: tensor([1, 2, 1, 2, 0, 0])\n",
      "\n",
      "\tPASSED\tInput: Tensor of shape torch.Size([3, 2])\tExpected probs: tensor([ -5.2810,  -6.2810,  -7.4104,  -8.4154,  -9.3672, -10.1368])\tYour probs: tensor([ -5.2810,  -6.2810,  -7.4104,  -8.4154,  -9.3672, -10.1368])\n",
      "\tPASSED\tInput: Tensor of shape torch.Size([3, 2])\tExpected idxes: tensor([0, 0, 1, 1, 0, 1])\tYour idxes: tensor([0, 0, 1, 1, 0, 1])\n",
      "\tPASSED\tInput: Tensor of shape torch.Size([3, 2])\tExpected hypotheses_ids: tensor([0, 2, 0, 2, 1, 1])\tYour hypotheses_ids: tensor([0, 2, 0, 2, 1, 1])\n",
      "\n",
      "\tPASSED\tInput: Tensor of shape torch.Size([3, 2])\tExpected probs: tensor([ -5.2810,  -6.2810,  -7.4104,  -8.4154,  -9.3672, -10.1368])\tYour probs: tensor([ -5.2810,  -6.2810,  -7.4104,  -8.4154,  -9.3672, -10.1368])\n",
      "\tPASSED\tInput: Tensor of shape torch.Size([3, 2])\tExpected idxes: tensor([0, 0, 1, 1, 0, 1])\tYour idxes: tensor([0, 0, 1, 1, 0, 1])\n",
      "\tPASSED\tInput: Tensor of shape torch.Size([3, 2])\tExpected hypotheses_ids: tensor([1, 0, 1, 0, 2, 2])\tYour hypotheses_ids: tensor([1, 0, 1, 0, 2, 2])\n",
      "\n",
      "\tPASSED\tInput: Tensor of shape torch.Size([3, 2])\tExpected probs: tensor([ -4.9818,  -5.0244,  -6.3092,  -7.2406,  -8.7471, -10.4256])\tYour probs: tensor([ -4.9818,  -5.0244,  -6.3092,  -7.2406,  -8.7471, -10.4256])\n",
      "\tPASSED\tInput: Tensor of shape torch.Size([3, 2])\tExpected idxes: tensor([1, 0, 1, 0, 1, 0])\tYour idxes: tensor([1, 0, 1, 0, 1, 0])\n",
      "\tPASSED\tInput: Tensor of shape torch.Size([3, 2])\tExpected hypotheses_ids: tensor([0, 1, 2, 2, 1, 0])\tYour hypotheses_ids: tensor([0, 1, 2, 2, 1, 0])\n",
      "\n",
      "\tPASSED\tInput: Tensor of shape torch.Size([3, 2])\tExpected probs: tensor([-7.5414, -7.9009, -8.2827, -8.5210, -9.2406, -9.3092])\tYour probs: tensor([-7.5414, -7.9009, -8.2827, -8.5210, -9.2406, -9.3092])\n",
      "\tPASSED\tInput: Tensor of shape torch.Size([3, 2])\tExpected idxes: tensor([0, 1, 0, 1, 0, 1])\tYour idxes: tensor([0, 1, 0, 1, 0, 1])\n",
      "\tPASSED\tInput: Tensor of shape torch.Size([3, 2])\tExpected hypotheses_ids: tensor([0, 0, 1, 1, 2, 2])\tYour hypotheses_ids: tensor([0, 0, 1, 1, 2, 2])\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def find_eos_sentences(probs, idxes, hypothesis_ids, curr_output, eos_idx, beam_size, alpha):\n",
    "    '''\n",
    "    Find the sentences that have generated EOS, but only if they are within the top beam_size sentences ranked by log probability.\n",
    "    Return the sentences (specified by a list of indexes from the vocabulary) that have terminated (generated EOS), and their scores.\n",
    "\n",
    "    Args:\n",
    "        probs: A tensor of size num_hyps*trg_vocab_size containing log probabilities (where 1 \u2264 num_hyps \u2264 beam_size). You may\n",
    "            assume that this is sorted in descending order.\n",
    "        idxes: A tensor of size num_hyps*trg_vocab_size where idxes[i] indicates which vocab index probs[i] corresponds to\n",
    "        hypothesis_ids: A tensor of size num_hyps*trg_vocab_size where hypothesis_ids[i] indicates which hypothesis probs[i] corresponds to\n",
    "        curr_output: A tensor of size [num_hyps, t-1] containing vocab indexes chosen for each hypothesis up to time t-1.\n",
    "        eos_idx: The end-of-sentence index in the vocabulary.\n",
    "        beam_size: The beam size.\n",
    "        alpha: The alpha used in calculating length penalty.\n",
    "    Returns:\n",
    "        eos_sents: A list of lists, where eos_sents[i] is a list containing the vocab indices of the ith sentence.\n",
    "        eos_probs: A list where eos_probs[i] is the score of eos_sents[i]. The score is the log proabability of the sentence, divided by\n",
    "            the length penalty, which is described above.\n",
    "    '''\n",
    "\n",
    "    assert probs.shape == idxes.shape == hypothesis_ids.shape, 'probs, idxes, and hypothesis_ids must all be the same shape'\n",
    "    assert alpha >= 0 and eos_idx >= 0 and beam_size >= 1\n",
    "\n",
    "    eos_probs, eos_sents = [], []\n",
    "\n",
    "\n",
    "    #    1. Find the indexes of idxes that (1) correspond to predicting EOS and (2) are within the top beam_size results ranked by log probability.\n",
    "    #         Hint: Make a binary mask that is the same size as probs, where an element is True if it meets these conditions.\n",
    "    mask = (idxes == eos_idx)\n",
    "    top_beam_mask = torch.zeros_like(mask)\n",
    "    top_beam_mask[:beam_size] = True\n",
    "    combined_mask = mask & top_beam_mask\n",
    "    top_eos_indices = torch.nonzero(combined_mask).squeeze(-1)\n",
    "\n",
    "    #    2. Calculate the length penalty (do not count BOS or EOS in the length!), and divide the log probabilities by it. These are the scores.\n",
    "    for i in top_eos_indices:\n",
    "        hyp_id = hypothesis_ids[i].item()\n",
    "\n",
    "        # Count non-zero tokens in the hypothesis (excluding padding)\n",
    "        tokens = curr_output[hyp_id].tolist()\n",
    "        # Remove padding (zeros) from the end\n",
    "        while tokens and tokens[-1] == 0:\n",
    "            tokens.pop()\n",
    "\n",
    "        # Calculate actual length (don't count BOS at the beginning)\n",
    "        # BOS is usually token 2 in many NLP tasks\n",
    "        length = len(tokens)\n",
    "        if tokens and tokens[0] == 2:  # If BOS is at the beginning\n",
    "            length -= 1\n",
    "\n",
    "        # Length penalty formula: ((5 + len)/6)^alpha where len is the length excluding BOS and EOS\n",
    "        length_penalty = ((5 + length)/6) ** alpha\n",
    "\n",
    "        # Calculate score\n",
    "        score = probs[i].item() / length_penalty\n",
    "\n",
    "        # Create the full sentence by appending EOS to the current hypothesis\n",
    "        final_sent = curr_output[hyp_id].tolist()\n",
    "        # Remove padding zeros from the end\n",
    "        while final_sent and final_sent[-1] == 0:\n",
    "            final_sent.pop()\n",
    "        # Add EOS token\n",
    "        final_sent.append(eos_idx)\n",
    "\n",
    "        eos_sents.append(final_sent)\n",
    "        eos_probs.append(score)\n",
    "\n",
    "    return eos_sents, eos_probs\n",
    "\n"
   ],
   "metadata": {
    "id": "VBDy8gUxXJth"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def find_non_eos_sentences(probs, idxes, hypothesis_ids, curr_output, eos_idx, beam_size, t):\n",
    "    '''\n",
    "    Finds the top beam_size continuations based on their cumulative log probability.\n",
    "\n",
    "    Args:\n",
    "        probs: A tensor of size beam_size*trg_vocab_size containing log probabilities. You may assume that this is\n",
    "           sorted in descending order.\n",
    "        idxes: A tensor of size beam_size*trg_vocab_size where idxes[i] indicates which vocab index probs[i]\n",
    "           corresponds to\n",
    "        hypothesis_ids: A tensor of size beam_size*trg_vocab_size where hypothesis_ids[i] indicates which hypothesis\n",
    "           probs[i] corresponds to\n",
    "        curr_output: A tensor of size [beam_size, max_decode_len] containing vocab indexes chosen for each hypothesis.\n",
    "        eos_idx: The end-of-sentence index in the vocabulary.\n",
    "        beam_size: The beam size.\n",
    "        t: The timestep currently being processed.\n",
    "    Returns:\n",
    "        new_output: A tensor of size [beam_size, max_decode_len], where the first t columns are filled with the\n",
    "           vocab indexes found so far for each hypothesis chosen.\n",
    "        new_probs: A tensor of size beam_size where new_probs[i] contains the cumulative log probability of the\n",
    "           hypothesis new_output[i]\n",
    "        next_hyps: A tensor of size beam_size where next_hyps[i] contains the hypothesis index from the previous\n",
    "           timestep that the selected hypothesis new_output[i] continues. For example, if new_output[i] continues\n",
    "           hypothesis 4 from curr_output (i.e. it continues curr_output[4]), then next_hyps[i] = 4.\n",
    "    '''\n",
    "\n",
    "    assert beam_size == curr_output.shape[0], 'curr_output must have beam_size rows'\n",
    "    assert probs.shape == idxes.shape == hypothesis_ids.shape\n",
    "    assert eos_idx >= 0 and beam_size >= 1 and t >= 1\n",
    "\n",
    "    new_output = torch.zeros_like(curr_output, device=curr_output.device)\n",
    "    new_probs, next_hyps = torch.zeros(beam_size, device=curr_output.device), torch.zeros(beam_size, device=curr_output.device)\n",
    "\n",
    "    #    1. Find the vocab indexes (hint: in idxes) and probabilities (hint: in probs) of the top beam_size continuations\n",
    "    #       that are not EOS, based on their log probabilities.\n",
    "    mask = (idxes != eos_idx)\n",
    "    top_indices = torch.nonzero(mask).squeeze(-1)[:beam_size]\n",
    "\n",
    "    selected_probs = probs[top_indices]\n",
    "    selected_vocab_idxs = idxes[top_indices]\n",
    "    selected_hyp_ids = hypothesis_ids[top_indices]\n",
    "\n",
    "    #    2. Find the hypothesis indexes that each of these continues (hint: in hypothesis_ids).\n",
    "    new_probs[:] = selected_probs\n",
    "    next_hyps[:] = selected_hyp_ids\n",
    "\n",
    "    #    3. Set the first t columns of new_output to the existing hypotheses that you have chosen to continue\n",
    "    for i in range(beam_size):\n",
    "        hyp_id = selected_hyp_ids[i].long()\n",
    "        # Copy all tokens from the selected hypothesis (up to and including position t-1)\n",
    "        new_output[i, :t] = curr_output[hyp_id, :t]\n",
    "        # Add the new token at position t\n",
    "        new_output[i, t] = selected_vocab_idxs[i]\n",
    "\n",
    "    return new_output, new_probs, next_hyps"
   ],
   "metadata": {
    "id": "8i5eIgoGYVBr"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Main beam search function"
   ],
   "metadata": {
    "id": "pxYuvgUGnfua"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wfvJmBrk2lJh"
   },
   "outputs": [],
   "source": [
    "def beam_decode_model(encoder, decoder, src, max_decode_len, beam_size,alpha):\n",
    "    \"\"\"\n",
    "    Perform beam search for the target sentence for the source sentence in src based on the trained encoder and decoder.\n",
    "\n",
    "    Args:\n",
    "        encoder: Your RnnEncoder object\n",
    "        decoder: Your RnnDecoder object\n",
    "        src: [max_src_length, 1] the source sentence you wish to translate (note: to simplify this function, we do not batch this)\n",
    "        max_decode_len: The maximum desired length (int) of your target translated sentences\n",
    "        beam_size: The beam size\n",
    "        alpha: The alpha in the length penalty formula\n",
    "\n",
    "    Returns:\n",
    "        sentence: A list containing vocab indexes of the best target sentence found.\n",
    "\n",
    "    Pseudo-code:\n",
    "    - Obtain encoder output and hidden state by encoding src sentences\n",
    "    - Expand start-of-sentence token by taking the beam_search continuations that have highest probability.\n",
    "    - For 2 \u2264 t < max_decode_len:\n",
    "        - Expand each of the previous beam_sarch hypotheses, and compute the cumulative log probability of each possible continuation\n",
    "        - Save the hypotheses that terminate and their log probabilities if they are in the top beam_size hypotheses by log probability\n",
    "        - Select the beam_size continuations that are not EOS to proceed with based on their log probabilities, and also track\n",
    "          which hypothesis they continue\n",
    "        - For the RNN only (not transformer), choose the hidden states correspoonding to the hypotheses that have been selected to continue\n",
    "    - Among the terminated sentences, select the one with highest score and return it\n",
    "\n",
    "    Hints:\n",
    "    - You should interact with the decoder using its decode_step function.\n",
    "    - This function will be used for the transformer model as well later in the homework. That model does not have a hidden state, so\n",
    "      dec_hidden is None.\n",
    "      Thus, when choosing which hidden state vectors to pass between timesteps, you only need to do this when the variable is_rnn is True.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize variables\n",
    "    trg_vocab = decoder.trg_vocab\n",
    "    eos_idx = trg_vocab.word2idx['<end>']\n",
    "    batch_size = src.size(1)\n",
    "    assert batch_size == 1 # For beam search, we will keep it simple and just do one item at a time.\n",
    "    device = src.device\n",
    "    is_rnn = 'Rnn' in encoder.__class__.__name__\n",
    "    sentence = []\n",
    "\n",
    "    # For beam search, we initialize our candidate hypotheses as start-of-sentence, each having probability 1 (log probability 0)\n",
    "    curr_output = torch.zeros((beam_size, max_decode_len),device=device) # For beam search, have to track outputs for each beam\n",
    "    curr_probs = torch.zeros(beam_size,device=device) # Each candidate hypothesis probability\n",
    "    completed_hypotheses = []\n",
    "    completed_scores = []\n",
    "\n",
    "    # We start the decoding with the start token for each example\n",
    "    dec_input = torch.tensor([[trg_vocab.word2idx['<start>']]] * batch_size, device=device)\n",
    "    curr_output[:, 0] = dec_input.squeeze(1)\n",
    "\n",
    "    # Obtain encoder outputs\n",
    "    if is_rnn: enc_output, dec_hidden = encoder(src)\n",
    "    else: enc_output, dec_hidden = encoder(src), None\n",
    "    enc_output, dec_hidden = enc_output.to(device), dec_hidden.to(device) if dec_hidden is not None else None\n",
    "\n",
    "    # Step 1: Decode start token\n",
    "    logits, dec_hidden = decoder.decode_step(dec_input, enc_output, dec_hidden)\n",
    "    log_probs = torch.log_softmax(logits, dim=-1)  # [1, vocab_size]\n",
    "\n",
    "    # Step 2: Expand predictions and sort\n",
    "    flat_probs, idxes, hyp_ids = sort_predictions(log_probs)  # each of shape [vocab_size]\n",
    "\n",
    "    # Step 3: Select top beam_size non-EOS to initialize beam\n",
    "    curr_output, curr_probs, next_hyps = find_non_eos_sentences(\n",
    "        flat_probs, idxes, hyp_ids, curr_output, eos_idx, beam_size, t=1\n",
    "    )\n",
    "    enc_output = enc_output.repeat(1, beam_size, 1)\n",
    "    if is_rnn:\n",
    "        dec_hidden = dec_hidden.repeat(1, beam_size, 1)\n",
    "\n",
    "    for t in range(2, max_decode_len):\n",
    "        dec_input = curr_output[:, :t]\n",
    "        logits, dec_hidden = decoder.decode_step(dec_input, enc_output, dec_hidden)\n",
    "        log_probs = torch.log_softmax(logits, dim=-1)  # [beam_size, vocab_size]\n",
    "\n",
    "        cumulative_log_probs = log_probs + curr_probs.unsqueeze(1)\n",
    "\n",
    "        flat_probs, idxes, hypothesis_ids = sort_predictions(cumulative_log_probs)\n",
    "\n",
    "        eos_sents, eos_scores = find_eos_sentences(flat_probs, idxes, hypothesis_ids, curr_output, eos_idx, beam_size, alpha)\n",
    "        completed_hypotheses.extend(eos_sents)\n",
    "        completed_scores.extend(eos_scores)\n",
    "\n",
    "        curr_output, curr_probs, next_hyps = find_non_eos_sentences(\n",
    "            flat_probs, idxes, hypothesis_ids, curr_output, eos_idx, beam_size, t\n",
    "        )\n",
    "\n",
    "        if is_rnn:\n",
    "            dec_hidden = dec_hidden[:, next_hyps.long(), :]\n",
    "\n",
    "    if len(completed_hypotheses) > 0:\n",
    "        best_idx = torch.tensor(completed_scores).argmax().item()\n",
    "        sentence = completed_hypotheses[best_idx]\n",
    "    else:\n",
    "        best_idx = curr_probs.argmax().item()\n",
    "        sentence = curr_output[best_idx][:max_decode_len].tolist()\n",
    "\n",
    "        if eos_idx in sentence:\n",
    "            sentence = sentence[:sentence.index(eos_idx) + 1]\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_jhu6JmL2lJh"
   },
   "source": [
    "Qualitatively compare some of the sentences generated by model with the some of the correct translations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F02IIwM-2lJh",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "56bd9293-ceff-44ce-ce6f-da1fdb341eb5"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Source sentence:\t\t\t\t <start> hizo llorar a mi madre . <end>\n",
      "Target sentence:\t\t\t\t <start> it made my mother cry . <end>\n",
      "Predicted sentence (greedy search):\t\t <start> he did my mother . <end>\n",
      "Predicted sentence (beam search, alpha=0.6):\t <start> he did my mother . <end>\n",
      "Predicted sentence (beam search, alpha=100):\t <start> do you hit my mother cry . m on . <end>\n",
      "----------------\n",
      "Source sentence:\t\t\t\t <start> perdi el tren de las . <end>\n",
      "Target sentence:\t\t\t\t <start> i missed the train . <end>\n",
      "Predicted sentence (greedy search):\t\t <start> i missed the train . <end>\n",
      "Predicted sentence (beam search, alpha=0.6):\t <start> i missed the train . <end>\n",
      "Predicted sentence (beam search, alpha=100):\t <start> i ve got on the train about . sorry . <end>\n",
      "----------------\n",
      "Source sentence:\t\t\t\t <start> no puedo protegerte aqui . <end>\n",
      "Target sentence:\t\t\t\t <start> i can t protect you here . <end>\n",
      "Predicted sentence (greedy search):\t\t <start> i can t protect you here . <end>\n",
      "Predicted sentence (beam search, alpha=0.6):\t <start> i can t protect you here . <end>\n",
      "Predicted sentence (beam search, alpha=100):\t <start> i can t protect you here . do it . <end>\n",
      "----------------\n",
      "Source sentence:\t\t\t\t <start> \u00bf que dijo ? <end>\n",
      "Target sentence:\t\t\t\t <start> what did he say ? <end>\n",
      "Predicted sentence (greedy search):\t\t <start> what did he say ? <end>\n",
      "Predicted sentence (beam search, alpha=0.6):\t <start> what did he say ? <end>\n",
      "Predicted sentence (beam search, alpha=100):\t <start> what did he say ? done it ? job ? <end>\n",
      "----------------\n",
      "Source sentence:\t\t\t\t <start> se quedo despierta toda la noche . <end>\n",
      "Target sentence:\t\t\t\t <start> she lay awake all night . <end>\n",
      "Predicted sentence (greedy search):\t\t <start> i stayed all night . <end>\n",
      "Predicted sentence (beam search, alpha=0.6):\t <start> she lay awake all night . <end>\n",
      "Predicted sentence (beam search, alpha=100):\t <start> she lay awake all night . i left out . <end>\n",
      "----------------\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    rnn_encoder.eval()\n",
    "    rnn_decoder.eval()\n",
    "    for h in range(5): # Do whole thing 5 times here, since doing batch size of 1 for beam search\n",
    "        idxes = random.choices(range(len(test_dataset.dataset)), k=1)\n",
    "        src, trg =  train_dataset.dataset[idxes]\n",
    "        beam_size = 5\n",
    "        alphas = [0.6, 100]\n",
    "        beam_result = [beam_decode_model(rnn_encoder, rnn_decoder, src.transpose(0,1).to(DEVICE), trg.size(1), beam_size,alpha=alpha) for alpha in alphas]\n",
    "        greedy_result = beam_decode_model(rnn_encoder, rnn_decoder, src.transpose(0,1).to(DEVICE), trg.size(1), 1,alpha=0)\n",
    "\n",
    "        print(\"Source sentence:\\t\\t\\t\\t\", ' '.join([x for x in [src_vocab.idx2word[j.item()] for j in src[0]] if x != '<pad>']))\n",
    "        print(\"Target sentence:\\t\\t\\t\\t\", ' '.join([x for x in [trg_vocab.idx2word[j.item()] for j in trg[0]] if x != '<pad>']))\n",
    "        print(\"Predicted sentence (greedy search):\\t\\t\", ' '.join([x for x in [trg_vocab.idx2word[j] for j in greedy_result] if x != '<pad>']))\n",
    "        for i in range(len(alphas)):\n",
    "            print(\"Predicted sentence (beam search, alpha=\"+str(alphas[i])+\"):\\t\", ' '.join([x for x in [trg_vocab.idx2word[j] for j in beam_result[i]] if x != '<pad>']))\n",
    "        print(\"----------------\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vpr-psDzf5BV"
   },
   "source": [
    "## Evaluate RNN Model\n",
    "\n",
    "1.   https://en.wikipedia.org/wiki/BLEU\n",
    "2.   https://www.aclweb.org/anthology/P02-1040.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bpLm8d1iq9bH",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "1930c6d2-2ec6-4d86-c4e9-bee783d67e38"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "BLEU 1-gram: 0.296868\n",
      "BLEU 2-gram: 0.082743\n",
      "BLEU 3-gram: 0.060334\n",
      "BLEU 4-gram: 0.057294\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(encoder, decoder, test_dataset, target_tensor_val, device):\n",
    "    trg_vocab = decoder.trg_vocab\n",
    "    batch_size = test_dataset.batch_size\n",
    "    pad_idx = trg_vocab.word2idx['<pad>']\n",
    "\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    final_output, target_output = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, (src, trg) in enumerate(test_dataset):\n",
    "            sentences = sample_model(encoder,\n",
    "                                     decoder,\n",
    "                                     src.transpose(0,1).to(DEVICE),\n",
    "                                     trg.size(1),\n",
    "                                     temperature=0.0000001) # Low temperature ==> greedy decoding\n",
    "\n",
    "            final_output += sentences\n",
    "            trg_sents = [trg[i,:][trg[i,:] != pad_idx].tolist() for i in range(trg.shape[0])]\n",
    "            target_output += trg_sents\n",
    "\n",
    "    # Compute BLEU scores\n",
    "    return compute_bleu_scores(target_output, final_output, trg_vocab)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    rnn_save_candidate, rnn_scores = evaluate_model(rnn_encoder, rnn_decoder, test_dataset, trg_tensor_val, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gZgsgEI8t0Jm"
   },
   "source": [
    "# Step 4: Train a Transformer\n",
    "\n",
    "Writing a transformer model for machine translation, training and evaluating its results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HT3k1qK5jc_i"
   },
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PusbB7PNL0v3"
   },
   "source": [
    "## Positional Embeddings\n",
    "\n",
    "Similar to the RNN, we start with the Encoder model. A key component of the encoder is the Positional Embedding. As we know, word embeddings encode words in such a way that words with similar meaning have similar vectors. Because there are no recurrences in a Transformer, we need a way to tell the transformer the relative position of words in a sentence: so will add a positional embedding to the word embeddings. Now, two words with a similar embedding will both be close in meaning and occur near each other in the sentence.\n",
    "\n",
    "Creating a positional embedding matrix of size $(max\\_len, embed\\_dim)$ using the following formulae:\n",
    "<br>\n",
    "$\\begin{align*} pe[pos,2i] &= \\sin \\Big (\\frac{pos}{10000^{2i/embed\\_dim}}\\Big )\\\\pe[pos,2i+1] &= \\cos \\Big (\\frac{pos}{10000^{2i/embed\\_dim}}\\Big ) \\end{align*}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FI1qLGOOdtYR"
   },
   "outputs": [],
   "source": [
    "def create_positional_embedding(max_len, embed_dim):\n",
    "    '''\n",
    "    Args:\n",
    "        max_len: The maximum length supported for positional embeddings\n",
    "        embed_dim: The size of your embeddings\n",
    "    Returns:\n",
    "        pe: [max_len, 1, embed_dim] computed as in the formulae above\n",
    "    '''\n",
    "    pe = None\n",
    "\n",
    "    position = torch.arange(0, max_len).unsqueeze(1).float()  # [max_len, 1]\n",
    "    div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * -(math.log(10000.0) / embed_dim))  # [embed_dim/2]\n",
    "\n",
    "    pe = torch.zeros(max_len, embed_dim)  # \u2705 Fix: initialize first\n",
    "\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)   # sin for even indices\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)   # cos for odd indices\n",
    "\n",
    "    pe = pe.unsqueeze(1)\n",
    "\n",
    "    return pe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q9juHL8hikXv"
   },
   "source": [
    "## Encoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0BVTSc8HtrjF"
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, src_vocab, embedding_dim, num_heads,\n",
    "        num_layers, dim_feedforward, max_len_src, device, dropout=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.device = device\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src_vocab: Vocab_Lang, the source vocabulary\n",
    "            embedding_dim: the dimension of the embedding (also the number of expected features for the input of the Transformer)\n",
    "            num_heads: The number of attention heads\n",
    "            num_layers: the number of Transformer Encoder layers\n",
    "            dim_feedforward: the dimension of the feedforward network models in the Transformer\n",
    "            max_len_src: maximum length of the source sentences\n",
    "            device: the working device (you may need to map your postional embedding to this device)\n",
    "            dropout: the dropout to be applied. Default=0.1.\n",
    "        \"\"\"\n",
    "        self.src_vocab = src_vocab # Do not change\n",
    "        src_vocab_size = len(src_vocab)\n",
    "\n",
    "        # Create positional embedding matrix\n",
    "        self.position_embedding = create_positional_embedding(max_len_src, embedding_dim).to(device)\n",
    "        self.register_buffer('positional_embedding', self.position_embedding) # this informs the model that position_embedding is not a learnable parameter\n",
    "\n",
    "\n",
    "        # Initialize embedding layer\n",
    "        self.embedding = nn.Embedding(src_vocab_size, embedding_dim)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Initialize a nn.TransformerEncoder model (you'll need to use embedding_dim,\n",
    "        #    num_layers, num_heads, & dim_feedforward here)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embedding_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=False\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: [max_len, batch_size]\n",
    "        Returns:\n",
    "            Boolean matrix of size [batch_size, max_len] indicating which indices are padding\n",
    "        \"\"\"\n",
    "        assert len(src.shape) == 2, 'src must have exactly 2 dimensions'\n",
    "        src_mask = src.transpose(0, 1) == 0 # padding idx\n",
    "        return src_mask.to(self.device) # [batch_size, max_src_len]\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [max_len, batch_size]\n",
    "        Returns:\n",
    "            output: [max_len, batch_size, embed_dim]\n",
    "        Pseudo-code (note: x refers to the original input to this function throughout the pseudo-code):\n",
    "        - Pass x through the word embedding\n",
    "        - Add positional embedding to the word embedding, then apply dropout\n",
    "        - Call make_src_mask(x) to compute a mask: this tells us which indexes in x\n",
    "          are padding, which we want to ignore for the self-attention\n",
    "        - Call the encoder, with src_key_padding_mask = src_mask\n",
    "        \"\"\"\n",
    "        output = None\n",
    "\n",
    "\n",
    "\n",
    "        word_embed = self.embedding(x)\n",
    "        pos_embed = self.position_embedding.to(x.device)[:x.size(0), :, :]\n",
    "        x_embed = self.dropout(word_embed + pos_embed)\n",
    "\n",
    "        src_mask = self.make_src_mask(x)\n",
    "        src_mask = src_mask.to(x.device)\n",
    "\n",
    "        output = self.encoder(x_embed, src_key_padding_mask=src_mask)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xet_lf1rq0uR",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "1b9a910a-88d3-487b-d824-d25a120c4d93"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--- TEST: Number of Model Parameters (tests __init__(...)) ---\n",
      "\tPASSED\tInput: {'src_vocab': ['a', 'aa', 'aaa'], 'embedding_dim': 4, 'num_heads': 1, 'dim_feedforward': 50, 'num_layers': 1, 'max_len_src': 16, 'device': device(type='cuda')}\tExpected Num. Params: 570\tYour Num. Params: 570\n",
      "\tPASSED\tInput: {'src_vocab': ['a', 'aa', 'aaa'], 'embedding_dim': 8, 'num_heads': 1, 'dim_feedforward': 50, 'num_layers': 1, 'max_len_src': 16, 'device': device(type='cuda')}\tExpected Num. Params: 1218\tYour Num. Params: 1218\n",
      "\tPASSED\tInput: {'src_vocab': ['a', 'aa', 'aaa'], 'embedding_dim': 12, 'num_heads': 1, 'dim_feedforward': 50, 'num_layers': 1, 'max_len_src': 16, 'device': device(type='cuda')}\tExpected Num. Params: 1994\tYour Num. Params: 1994\n",
      "\tPASSED\tInput: {'src_vocab': ['a', 'aa', 'aaa'], 'embedding_dim': 4, 'num_heads': 1, 'dim_feedforward': 100, 'num_layers': 2, 'max_len_src': 16, 'device': device(type='cuda')}\tExpected Num. Params: 2020\tYour Num. Params: 2020\n",
      "\tPASSED\tInput: {'src_vocab': ['a', 'aa', 'aaa'], 'embedding_dim': 8, 'num_heads': 2, 'dim_feedforward': 100, 'num_layers': 2, 'max_len_src': 16, 'device': device(type='cuda')}\tExpected Num. Params: 4096\tYour Num. Params: 4096\n",
      "\tPASSED\tInput: {'src_vocab': ['a', 'aa', 'aaa'], 'embedding_dim': 12, 'num_heads': 2, 'dim_feedforward': 100, 'num_layers': 2, 'max_len_src': 16, 'device': device(type='cuda')}\tExpected Num. Params: 6428\tYour Num. Params: 6428\n",
      "\tPASSED\tInput: {'src_vocab': ['a', 'aa', 'aaa'], 'embedding_dim': 4, 'num_heads': 2, 'dim_feedforward': 150, 'num_layers': 3, 'max_len_src': 16, 'device': device(type='cuda')}\tExpected Num. Params: 4370\tYour Num. Params: 4370\n",
      "\tPASSED\tInput: {'src_vocab': ['a', 'aa', 'aaa'], 'embedding_dim': 8, 'num_heads': 4, 'dim_feedforward': 150, 'num_layers': 3, 'max_len_src': 16, 'device': device(type='cuda')}\tExpected Num. Params: 8674\tYour Num. Params: 8674\n",
      "\tPASSED\tInput: {'src_vocab': ['a', 'aa', 'aaa'], 'embedding_dim': 12, 'num_heads': 4, 'dim_feedforward': 150, 'num_layers': 3, 'max_len_src': 16, 'device': device(type='cuda')}\tExpected Num. Params: 13362\tYour Num. Params: 13362\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    # Set random seed\n",
    "    torch.manual_seed(42)\n",
    "    # Create test inputs\n",
    "    dimf = [50, 100, 150]\n",
    "    embedding_dim = [4, 8, 12]\n",
    "    max_len = 16\n",
    "    num_layers = iter([1,1,1,2,2,2,3,3,3])\n",
    "    nheads = iter([1, 1, 1, 1, 2, 2, 2, 4, 4])\n",
    "    sanity_vocab = Vocab_Lang(vocab=[\"a\", \"aa\", \"aaa\"])\n",
    "    params = []\n",
    "    inputs = []\n",
    "    for df in dimf:\n",
    "        for ed in embedding_dim:\n",
    "            inp = {}\n",
    "            inp['src_vocab'] = sanity_vocab\n",
    "            inp['embedding_dim'] = ed\n",
    "            inp['num_heads'] = next(nheads)\n",
    "            inp['dim_feedforward'] = df\n",
    "            inp['num_layers'] = next(num_layers)\n",
    "            inp['max_len_src'] = max_len\n",
    "            inp['device'] = DEVICE\n",
    "            inputs.append(inp)\n",
    "    # Test init\n",
    "    expected_outputs = [570, 1218, 1994, 2020, 4096, 6428, 4370, 8674, 13362]\n",
    "\n",
    "    sanityCheckModel(inputs, TransformerEncoder, expected_outputs, \"init\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XQUWrgE-1Eo5",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b3d2bc74-42e3-4913-839d-9027fbe594d4"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--- TEST: Output shape of forward(...) ---\n",
      "\tPASSED\t Init Input: {'src_vocab': ['a', 'aa', 'aaa'], 'embedding_dim': 32, 'num_heads': 1, 'dim_feedforward': 100, 'num_layers': 1, 'max_len_src': 16, 'device': device(type='cuda')}\tForward Input Shape: torch.Size([16, 1])\tExpected Output Shape: torch.Size([16, 1, 32])\tYour Output Shape: torch.Size([16, 1, 32])\n",
      "\tPASSED\t Init Input: {'src_vocab': ['a', 'aa', 'aaa'], 'embedding_dim': 32, 'num_heads': 1, 'dim_feedforward': 100, 'num_layers': 1, 'max_len_src': 16, 'device': device(type='cuda')}\tForward Input Shape: torch.Size([16, 2])\tExpected Output Shape: torch.Size([16, 2, 32])\tYour Output Shape: torch.Size([16, 2, 32])\n",
      "\tPASSED\t Init Input: {'src_vocab': ['a', 'aa', 'aaa'], 'embedding_dim': 64, 'num_heads': 2, 'dim_feedforward': 100, 'num_layers': 2, 'max_len_src': 16, 'device': device(type='cuda')}\tForward Input Shape: torch.Size([16, 1])\tExpected Output Shape: torch.Size([16, 1, 64])\tYour Output Shape: torch.Size([16, 1, 64])\n",
      "\tPASSED\t Init Input: {'src_vocab': ['a', 'aa', 'aaa'], 'embedding_dim': 64, 'num_heads': 2, 'dim_feedforward': 100, 'num_layers': 2, 'max_len_src': 16, 'device': device(type='cuda')}\tForward Input Shape: torch.Size([16, 2])\tExpected Output Shape: torch.Size([16, 2, 64])\tYour Output Shape: torch.Size([16, 2, 64])\n",
      "\tPASSED\t Init Input: {'src_vocab': ['a', 'aa', 'aaa'], 'embedding_dim': 128, 'num_heads': 4, 'dim_feedforward': 100, 'num_layers': 3, 'max_len_src': 16, 'device': device(type='cuda')}\tForward Input Shape: torch.Size([16, 1])\tExpected Output Shape: torch.Size([16, 1, 128])\tYour Output Shape: torch.Size([16, 1, 128])\n",
      "\tPASSED\t Init Input: {'src_vocab': ['a', 'aa', 'aaa'], 'embedding_dim': 128, 'num_heads': 4, 'dim_feedforward': 100, 'num_layers': 3, 'max_len_src': 16, 'device': device(type='cuda')}\tForward Input Shape: torch.Size([16, 2])\tExpected Output Shape: torch.Size([16, 2, 128])\tYour Output Shape: torch.Size([16, 2, 128])\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    # Set random seed\n",
    "    torch.manual_seed(42)\n",
    "    # Test forward\n",
    "    inputs = []\n",
    "    embedding_dims = [32,64,128]\n",
    "    batch_sizes = [1, 2]\n",
    "    dimf = 100\n",
    "    nheads = iter([1,1,2,2,4,4])\n",
    "    num_layers = iter([1,1,2,2,3,3])\n",
    "    max_len = 16\n",
    "    sanity_vocab = Vocab_Lang(vocab=[\"a\", \"aa\", \"aaa\"])\n",
    "    for ed in embedding_dims:\n",
    "        for b in batch_sizes:\n",
    "            inp = {}\n",
    "            inp['src_vocab'] = sanity_vocab\n",
    "            inp['embedding_dim'] = ed\n",
    "            inp['num_heads'] = next(nheads)\n",
    "            inp['dim_feedforward'] = dimf\n",
    "            inp['num_layers'] = next(num_layers)\n",
    "            inp['max_len_src'] = max_len\n",
    "            inp['device'] = DEVICE\n",
    "            inp[\"batch_size\"] = b\n",
    "            inputs.append(inp)\n",
    "    expected_outputs = [torch.Size([16, 1, 32]), torch.Size([16, 2, 32]), torch.Size([16, 1, 64]), torch.Size([16, 2, 64]), torch.Size([16, 1, 128]), torch.Size([16, 2, 128])]\n",
    "\n",
    "    sanityCheckModel(inputs, TransformerEncoder, expected_outputs, \"forward\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CVCo7w4MMPKt"
   },
   "source": [
    "##  Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DJtTIHJYyNmU"
   },
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, trg_vocab, embedding_dim, num_heads,\n",
    "        num_layers, dim_feedforward, max_len_trg, device, dropout=0.1):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.device = device\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            trg_vocab: Vocab_Lang, the target vocabulary\n",
    "            embedding_dim: the dimension of the embedding (also the number of expected features for the input of the Transformer)\n",
    "            num_heads: The number of attention heads\n",
    "            num_layers: the number of Transformer Decoder layers\n",
    "            dim_feedforward: the dimension of the feedforward network models in the Transformer\n",
    "            max_len_trg: maximum length of the target sentences\n",
    "            device: the working device (you may need to map your postional embedding to this device)\n",
    "            dropout: the dropout to be applied. Default=0.1.\n",
    "        \"\"\"\n",
    "        self.trg_vocab = trg_vocab # Do not change\n",
    "        trg_vocab_size = len(trg_vocab)\n",
    "\n",
    "        # Create positional embedding matrix\n",
    "        self.position_embedding = create_positional_embedding(max_len_trg, embedding_dim).to(device)\n",
    "        self.register_buffer('positional_embedding', self.position_embedding) # this informs the model that positional_embedding is not a learnable parameter\n",
    "\n",
    "\n",
    "        # Initialize embedding layer\n",
    "        self.embedding = nn.Embedding(trg_vocab_size, embedding_dim)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Initialize a nn.TransformerDecoder model (you'll need to use embedding_dim,\n",
    "        # num_layers, num_heads, & dim_feedforward here)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=embedding_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=False\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Final fully connected layer\n",
    "        self.fc = nn.Linear(embedding_dim, trg_vocab_size)\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        \"\"\"Generate a square mask for the sequence. The masked positions are filled with float('-inf').\n",
    "            Unmasked positions are filled with float(0.0).\n",
    "        \"\"\"\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0)).to(self.device)\n",
    "        return mask\n",
    "\n",
    "    def forward(self, dec_in, enc_out):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dec_in: [sequence length, batch_size]\n",
    "            enc_out: [max_len_src, batch_size, embed_dim]\n",
    "        Returns:\n",
    "            output: [sequence length, batch_size, trg_vocab_size]\n",
    "        Pseudo-code:\n",
    "            - Compute input word and positional embeddings in similar manner to encoder\n",
    "            - Call generate_square_subsequent_mask() to compute a mask: this time,\n",
    "              the mask is to prevent the decoder from attending to tokens in the \"future\".\n",
    "              In other words, at time step i, the decoder should only attend to tokens\n",
    "              1 to i-1.\n",
    "            - Call the decoder, with tgt_mask = trg_mask\n",
    "            - Run the output through the fully-connected layer and return it\n",
    "        \"\"\"\n",
    "        output = None\n",
    "\n",
    "\n",
    "        word_embed = self.embedding(dec_in)\n",
    "        pos_embed = self.position_embedding.to(dec_in.device)[:dec_in.size(0), :, :]\n",
    "        dec_embed = self.dropout(word_embed + pos_embed)\n",
    "\n",
    "        trg_mask = self.generate_square_subsequent_mask(dec_in.size(0))\n",
    "\n",
    "        decoded = self.decoder(dec_embed, enc_out, tgt_mask=trg_mask)\n",
    "        output = self.fc(decoded)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def decode_step(self, inputs, enc_output, dec_hs):\n",
    "        '''\n",
    "        Call one step of the decoder.\n",
    "\n",
    "        Args:\n",
    "            inputs: Input tokens; [batch_size, sequence length]\n",
    "            enc_output: Encoder outputs; [max_len_src, batch_size, embed_dim]\n",
    "            dec_hs: None\n",
    "\n",
    "        Returns:\n",
    "            fc_out: (Unnormalized) output distribution [batch_size, vocab_size]\n",
    "            dec_hs: None\n",
    "        '''\n",
    "        assert dec_hs is None, 'For the transformer model, make sure you pass dec_hs = None!'\n",
    "        inputs = inputs.type(torch.long)\n",
    "        preds = self(inputs.transpose(0,1), enc_output)[-1]\n",
    "        return preds, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1W093yUg25an"
   },
   "outputs": [],
   "source": [
    "def sanityCheckTransformerDecoderModelForward(inputs, NN, expected_outputs):\n",
    "    print('--- TEST: Output shape of forward(...) ---\\n')\n",
    "    msg = ''\n",
    "    for i, inp in enumerate(inputs):\n",
    "        input_rep = '{'\n",
    "        for k,v in inp.items():\n",
    "            if torch.is_tensor(v):\n",
    "                input_rep += str(k) + ': ' + 'Tensor with shape ' + str(v.size()) + ', '\n",
    "            else:\n",
    "                input_rep += str(k) + ': ' + str(v) + ', '\n",
    "        input_rep += '}'\n",
    "        dec = NN(trg_vocab=inp['trg_vocab'],embedding_dim=inp['embedding_dim'],num_heads=inp['num_heads'],num_layers=inp['num_layers'],dim_feedforward=inp['dim_feedforward'],max_len_trg=inp['max_len_trg'],device=inp['device'])\n",
    "        dec_in = torch.randint(low=0,high=len(inputs[0]['trg_vocab']),size=(inp['max_len_trg'], inp['batch_size']))\n",
    "        enc_out = torch.rand(inp['max_len_trg'], inp['batch_size'], inp['embedding_dim'])\n",
    "        inp['encoder_outputs'] = enc_out\n",
    "        with torch.no_grad():\n",
    "            stu_out = dec(enc_out=enc_out, dec_in=dec_in)\n",
    "        del dec\n",
    "        has_passed = True\n",
    "        if not torch.is_tensor(stu_out):\n",
    "            has_passed = False\n",
    "            msg = 'Output must be a torch.Tensor; received ' + str(type(stu_out))\n",
    "        status = 'PASSED' if has_passed else 'FAILED'\n",
    "        if not has_passed:\n",
    "            message = '\\t' + status + \"\\t Init Input: \" + input_rep + '\\tForward Input Shape (dec_in): ' + str(dec_in.shape) + '\\tExpected Output Shape: ' + str(expected_outputs[i]) + '\\t' + msg\n",
    "            print(message)\n",
    "            continue\n",
    "\n",
    "        has_passed = stu_out.size() == expected_outputs[i]\n",
    "        msg = 'Your Output Shape: ' + str(stu_out.size())\n",
    "        status = 'PASSED' if has_passed else 'FAILED'\n",
    "        message = '\\t' + status + \"\\t Init Input: \" + input_rep + '\\tForward Input Shape (dec_in): ' + str(dec_in.shape) + '\\tExpected Output Shape: ' + str(expected_outputs[i]) + '\\t' + msg\n",
    "        print(message)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2dIb90hi3fC0",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "97a2599a-a528-4669-c0d7-eeaea0d53898"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--- TEST: Number of Model Parameters (tests __init__(...)) ---\n",
      "\tPASSED\tInput: {'trg_vocab': ['a', 'aa', 'aaa'], 'embedding_dim': 8, 'num_heads': 1, 'num_layers': 1, 'dim_feedforward': 50, 'max_len_trg': 64, 'device': device(type='cuda')}\tExpected Num. Params: 1567\tYour Num. Params: 1567\n",
      "\tPASSED\tInput: {'trg_vocab': ['a', 'aa', 'aaa'], 'embedding_dim': 8, 'num_heads': 1, 'num_layers': 2, 'dim_feedforward': 50, 'max_len_trg': 64, 'device': device(type='cuda')}\tExpected Num. Params: 3049\tYour Num. Params: 3049\n",
      "\tPASSED\tInput: {'trg_vocab': ['a', 'aa', 'aaa'], 'embedding_dim': 8, 'num_heads': 2, 'num_layers': 1, 'dim_feedforward': 50, 'max_len_trg': 64, 'device': device(type='cuda')}\tExpected Num. Params: 1567\tYour Num. Params: 1567\n",
      "\tPASSED\tInput: {'trg_vocab': ['a', 'aa', 'aaa'], 'embedding_dim': 8, 'num_heads': 2, 'num_layers': 2, 'dim_feedforward': 50, 'max_len_trg': 64, 'device': device(type='cuda')}\tExpected Num. Params: 3049\tYour Num. Params: 3049\n",
      "\tPASSED\tInput: {'trg_vocab': ['a', 'aa', 'aaa'], 'embedding_dim': 8, 'num_heads': 1, 'num_layers': 1, 'dim_feedforward': 100, 'max_len_trg': 64, 'device': device(type='cuda')}\tExpected Num. Params: 2417\tYour Num. Params: 2417\n",
      "\tPASSED\tInput: {'trg_vocab': ['a', 'aa', 'aaa'], 'embedding_dim': 8, 'num_heads': 1, 'num_layers': 2, 'dim_feedforward': 100, 'max_len_trg': 64, 'device': device(type='cuda')}\tExpected Num. Params: 4749\tYour Num. Params: 4749\n",
      "\tPASSED\tInput: {'trg_vocab': ['a', 'aa', 'aaa'], 'embedding_dim': 8, 'num_heads': 2, 'num_layers': 1, 'dim_feedforward': 100, 'max_len_trg': 64, 'device': device(type='cuda')}\tExpected Num. Params: 2417\tYour Num. Params: 2417\n",
      "\tPASSED\tInput: {'trg_vocab': ['a', 'aa', 'aaa'], 'embedding_dim': 8, 'num_heads': 2, 'num_layers': 2, 'dim_feedforward': 100, 'max_len_trg': 64, 'device': device(type='cuda')}\tExpected Num. Params: 4749\tYour Num. Params: 4749\n",
      "\n",
      "--- TEST: Output shape of forward(...) ---\n",
      "\n",
      "\tPASSED\t Init Input: {embedding_dim: 100, trg_vocab: ['a', 'aa', 'aaa'], num_heads: 2, num_layers: 1, batch_size: 1, dim_feedforward: 50, max_len_trg: 16, device: cuda, }\tForward Input Shape (dec_in): torch.Size([16, 1])\tExpected Output Shape: torch.Size([16, 1, 5])\tYour Output Shape: torch.Size([16, 1, 5])\n",
      "\tPASSED\t Init Input: {embedding_dim: 100, trg_vocab: ['a', 'aa', 'aaa'], num_heads: 2, num_layers: 1, batch_size: 2, dim_feedforward: 50, max_len_trg: 16, device: cuda, }\tForward Input Shape (dec_in): torch.Size([16, 2])\tExpected Output Shape: torch.Size([16, 2, 5])\tYour Output Shape: torch.Size([16, 2, 5])\n",
      "\tPASSED\t Init Input: {embedding_dim: 200, trg_vocab: ['a', 'aa', 'aaa'], num_heads: 2, num_layers: 1, batch_size: 4, dim_feedforward: 50, max_len_trg: 16, device: cuda, }\tForward Input Shape (dec_in): torch.Size([16, 4])\tExpected Output Shape: torch.Size([16, 4, 5])\tYour Output Shape: torch.Size([16, 4, 5])\n",
      "\tPASSED\t Init Input: {embedding_dim: 200, trg_vocab: ['a', 'aa', 'aaa'], num_heads: 2, num_layers: 1, batch_size: 1, dim_feedforward: 100, max_len_trg: 32, device: cuda, }\tForward Input Shape (dec_in): torch.Size([32, 1])\tExpected Output Shape: torch.Size([32, 1, 5])\tYour Output Shape: torch.Size([32, 1, 5])\n",
      "\tPASSED\t Init Input: {embedding_dim: 200, trg_vocab: ['a', 'aa', 'aaa'], num_heads: 2, num_layers: 1, batch_size: 2, dim_feedforward: 100, max_len_trg: 32, device: cuda, }\tForward Input Shape (dec_in): torch.Size([32, 2])\tExpected Output Shape: torch.Size([32, 2, 5])\tYour Output Shape: torch.Size([32, 2, 5])\n",
      "\tPASSED\t Init Input: {embedding_dim: 400, trg_vocab: ['a', 'aa', 'aaa'], num_heads: 2, num_layers: 1, batch_size: 4, dim_feedforward: 100, max_len_trg: 32, device: cuda, }\tForward Input Shape (dec_in): torch.Size([32, 4])\tExpected Output Shape: torch.Size([32, 4, 5])\tYour Output Shape: torch.Size([32, 4, 5])\n",
      "\tPASSED\t Init Input: {embedding_dim: 400, trg_vocab: ['a', 'aa', 'aaa'], num_heads: 2, num_layers: 1, batch_size: 1, dim_feedforward: 200, max_len_trg: 64, device: cuda, }\tForward Input Shape (dec_in): torch.Size([64, 1])\tExpected Output Shape: torch.Size([64, 1, 5])\tYour Output Shape: torch.Size([64, 1, 5])\n",
      "\tPASSED\t Init Input: {embedding_dim: 800, trg_vocab: ['a', 'aa', 'aaa'], num_heads: 2, num_layers: 1, batch_size: 2, dim_feedforward: 200, max_len_trg: 64, device: cuda, }\tForward Input Shape (dec_in): torch.Size([64, 2])\tExpected Output Shape: torch.Size([64, 2, 5])\tYour Output Shape: torch.Size([64, 2, 5])\n",
      "\tPASSED\t Init Input: {embedding_dim: 800, trg_vocab: ['a', 'aa', 'aaa'], num_heads: 2, num_layers: 1, batch_size: 4, dim_feedforward: 200, max_len_trg: 128, device: cuda, }\tForward Input Shape (dec_in): torch.Size([128, 4])\tExpected Output Shape: torch.Size([128, 4, 5])\tYour Output Shape: torch.Size([128, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Set random seed\n",
    "    torch.manual_seed(42)\n",
    "    # Create test inputs\n",
    "    hidden_units = [50, 100, 200]\n",
    "    embedding_dim = [8, 16]\n",
    "    num_heads = [1, 2]\n",
    "    dim_feedforward = [50, 100]\n",
    "    num_layers = [1, 2]\n",
    "    max_lens = 64\n",
    "    sanity_vocab = Vocab_Lang(vocab=[\"a\", \"aa\", \"aaa\"])\n",
    "    params = []\n",
    "    inputs = []\n",
    "    for ed in embedding_dim:\n",
    "        for df in dim_feedforward:\n",
    "            for nh in num_heads:\n",
    "                for nl in num_layers:\n",
    "                    inp = {}\n",
    "                    inp['trg_vocab'] = sanity_vocab\n",
    "                    inp['embedding_dim'] = ed\n",
    "                    inp['num_heads'] = nh\n",
    "                    inp['num_layers'] = nl\n",
    "                    inp['dim_feedforward'] = df\n",
    "                    inp['max_len_trg'] = max_lens\n",
    "                    inp['device'] = DEVICE\n",
    "                    inputs.append(inp)\n",
    "    # Test init\n",
    "    expected_outputs = [1567, 3049, 1567, 3049, 2417, 4749, 2417, 4749]\n",
    "    sanityCheckModel(inputs, TransformerDecoder, expected_outputs, \"init\")\n",
    "    print()\n",
    "\n",
    "    # Test forward\n",
    "    inputs = []\n",
    "    batch_sizes = [1, 2, 4]\n",
    "    num_heads = 2\n",
    "    num_layers = 1\n",
    "    embedding_dims = iter([100, 100, 200, 200, 200, 400, 400, 800, 800])\n",
    "    max_lens = iter([16, 16, 16, 32, 32, 32, 64, 64, 128])\n",
    "    expected_outputs = [torch.Size([16, 1, 5]),torch.Size([16, 2, 5]),torch.Size([16, 4, 5]),torch.Size([32, 1, 5]),torch.Size([32, 2, 5]),torch.Size([32, 4, 5]),torch.Size([64, 1, 5]),torch.Size([64, 2, 5]),torch.Size([128, 4, 5])]\n",
    "\n",
    "    for hu in hidden_units:\n",
    "        for b in batch_sizes:\n",
    "            inp = {}\n",
    "            edim = next(embedding_dims)\n",
    "            inp['embedding_dim'] = edim\n",
    "            inp['trg_vocab'] = sanity_vocab\n",
    "            inp['num_heads'] = num_heads\n",
    "            inp['num_layers'] = num_layers\n",
    "            inp[\"batch_size\"] = b\n",
    "            inp['dim_feedforward'] = hu\n",
    "            inp['max_len_trg'] = next(max_lens)\n",
    "            inp['device'] = DEVICE\n",
    "            inputs.append(inp)\n",
    "\n",
    "    sanityCheckTransformerDecoderModelForward(inputs, TransformerDecoder, expected_outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g3zXR9gYMgek"
   },
   "source": [
    "## Train Transformer Model\n",
    "\n",
    "Like the RNN, we train the encoder and decoder using cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dN6rnn4N3vcc"
   },
   "outputs": [],
   "source": [
    "def train_transformer_model(encoder, decoder, optimizer, device, n_epochs):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    for epoch in range(n_epochs):\n",
    "        start = time.time()\n",
    "        losses = []\n",
    "\n",
    "        for src, trg in tqdm(train_dataset):\n",
    "\n",
    "            src = src.to(device).transpose(0,1) # [max_src_length, batch_size]\n",
    "            trg = trg.to(device).transpose(0,1) # [max_trg_length, batch_size]\n",
    "\n",
    "            enc_out = encoder(src)\n",
    "            output = decoder(trg[:-1, :], enc_out)\n",
    "\n",
    "            output = output.reshape(-1, output.shape[2])\n",
    "            trg = trg[1:].reshape(-1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip to avoid exploding grading issues\n",
    "            torch.nn.utils.clip_grad_norm_(encoder.parameters(), max_norm=1)\n",
    "            torch.nn.utils.clip_grad_norm_(decoder.parameters(), max_norm=1)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "        mean_loss = sum(losses) / len(losses)\n",
    "        print('Epoch:{:2d}/{}\\t Loss:{:.4f} ({:.2f}s)'.format(epoch + 1, n_epochs, mean_loss, time.time() - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B15xUHct1fuw",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "323406bc-982d-401e-b499-ed31a27cb9bb"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Encoder and Decoder models initialized!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # HYPERPARAMETERS\n",
    "    LEARNING_RATE = 0.001\n",
    "    DIM_FEEDFORWARD=512\n",
    "    N_EPOCHS=10\n",
    "    N_HEADS=2\n",
    "    N_LAYERS=2\n",
    "    DROPOUT=0.1\n",
    "\n",
    "    transformer_encoder = TransformerEncoder(src_vocab, EMBEDDING_DIM, N_HEADS,\n",
    "                                 N_LAYERS,DIM_FEEDFORWARD,\n",
    "                                 max_length_src, DEVICE, DROPOUT).to(DEVICE)\n",
    "    transformer_decoder = TransformerDecoder(trg_vocab, EMBEDDING_DIM, N_HEADS,\n",
    "                              N_LAYERS,DIM_FEEDFORWARD,\n",
    "                              max_length_trg, DEVICE, DROPOUT).to(DEVICE)\n",
    "\n",
    "    transformer_model_params = list(transformer_encoder.parameters()) + list(transformer_decoder.parameters())\n",
    "    optimizer = torch.optim.Adam(transformer_model_params, lr=LEARNING_RATE)\n",
    "\n",
    "    print('Encoder and Decoder models initialized!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur0ucStn3vi5",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 510,
     "referenced_widgets": [
      "b24be8f6605a472b82727c7187276545",
      "a5cdcd2fe60c4e14a6a2aeb9c9b770a4",
      "59ca9b2c0bc24daea41000384bc0f0a1",
      "99c6a09b9cb64f46826a638502c5a843",
      "78ce102b4ec04b208d4fdd9b0925507b",
      "d1d38f1e7e674f14b050eb937fa34336",
      "417dd846901945d5b7912bb306b2cd67",
      "9d5abb96b5e34b329cc43b02f0ba5073",
      "cb679e50aef2493b89478ce8b7fd2c1a",
      "45de49460de343dab0dd9d7e854440f6",
      "120a427e432a43f3b376641191db018b",
      "375bbdbf987a467196cfa24b24ada57e",
      "192b2bac6d104287ac9d616858e5657f",
      "bc2ad7c1fd344546a4da53ffa58e6b49",
      "11a6474b72824e8dba2aa98312d0331b",
      "14a6f0ed7d5c4381b7cb3d5a28935ab3",
      "113299e4e41f40949aafc807f9e9d0ce",
      "96425fb6f31040cbb6c0e1648209c6c3",
      "6a609ddcd0354ac5858bfa73cd1d2c1f",
      "5fb2723cfa9245c3ba67458cd1d933f5",
      "23a08f4a38d24304bf373592783f4542",
      "b5ee098656324718a88a713c97216df6",
      "d5553ef9f9774169818684b6df4060da",
      "b8decf69ec924ef4937ea85df1a64c6f",
      "3a42a20e911940bd845b68a78e2bf612",
      "0b56b0a7a1304902be8da3d960bfea63",
      "0d49b422e7ac468383cf716bd08b8b74",
      "9a770152da284f26a2cbef0b06190f49",
      "026444c6838b45498308c0f3a7b62dd8",
      "da0b8aed88bc4fbab0544a18bb11af55",
      "f65ad3f5f200422ab60790e566f30ec8",
      "25229181fcf348beb16e95b1a93b19d8",
      "82ef5317ff1e4e69a0cc811926821e57",
      "a45b5d5d0afe4709b08ea1631d2e4cfd",
      "e3a8041b339e4c6aa98a3619688b978e",
      "22c8827f8b36442a9f28bc5d5f2a7118",
      "684c424ca72d45f1bfbaaec36c781c69",
      "94a3b158d6b64e209b346be109f6b5d0",
      "ca54ec865d8a42298f67fc0b211c9ecf",
      "e906e911e35d48eda5ffbf33ff54d08d",
      "6c3445843c12434cb0d3942561613ada",
      "fdbd2f6cb94e4d72a397cb3672c5f352",
      "f7477ecbc2944e8c9449d47451e7a6d7",
      "445506fed33f4e4e8bfe3865cb564e8c",
      "55b8a2a1c1664bb790695bf9b32a01a1",
      "21cadba2341c49dabac66573fafe6f7a",
      "c36f4d8c7e984525bf183c675ef034d1",
      "69378984c7094491aaa8b87982965e62",
      "69642a58326249e6a3ee0055e546a2e4",
      "fd159adb51cf45269a747e0d942f6104",
      "0015323c9a32468d8540ea7bdf8e5dea",
      "c3c2411812af421bb12fa7e4f17998ed",
      "53ace32a5ad54e5eb4a8e32a9a7aa1d7",
      "d2890c99db504e0f8feb726405385e6f",
      "ee836d19abb6438586bf9f76be507261",
      "6faf9d38032b4f6f97050af3c6d4cb75",
      "71ffda5c620e4f8197125ea1f8fceff8",
      "4b0c1058f98a4bd0966616f4f20dfc4a",
      "90685fb611a44979a2fe34ca27a04965",
      "197cf3f4ff754b10a87ffe88d673e53a",
      "e4762987391d4d88b0ff861f5b95be62",
      "c7edacbce4c64dcd84e4a7b35ff6fb5f",
      "9877f99bdb364228a67224fe83645e5d",
      "17c92791e2774830b437b126d92c094c",
      "5f00167215904984b78bd9d197bcbef6",
      "c8422618c72247b3a2b34a90e13e7694",
      "e741a3b420444e8d8b403948ef4ac4fe",
      "f3e26680d79f4277b2cef8d3b8217077",
      "75b625faf20842b5b75ae50ad5ee7e49",
      "2cbe8bce800645938c54b75e5f9aa887",
      "529784ae1e4a4f238945288ccae38237",
      "46c974b87e7c4793a2a9ccd100025584",
      "78bc0017621248b68f2c7547f50944ca",
      "9230ecf449a94eaf86b1205321ef6c97",
      "f69abef160764c28bb1013f02946fc21",
      "4dff1324d217449489b12728fee68071",
      "a442614367b24216a9e4df557e9c646e",
      "36e40cc5e7784d63863a94d390d74231",
      "58f3ba00b8ca4d63921cee3b51bd7727",
      "eff4e17690354101a81172d510f71bd7",
      "ea553d9668f747779fc502ab47030c3f",
      "ec11389017764a608589b31e15d471f7",
      "1dbfc8edefec4424b4e2623abf2b474f",
      "636b831acdcc44aeb403048c39877b60",
      "669f2126564c4543aaa9f8bbdd3f8480",
      "1c8a837c971b4c148a1c1845f2132882",
      "5c1e0d6a60564eb9a9485d4e972b042d",
      "98bcac66bee94736b1939b6538f49992",
      "6b531871e8544582bb57111d8c30f958",
      "13c5d88b69bc4b63a3a6659e6a4cf917",
      "10bda135922144a6a56d67e01bf3e2ed",
      "de1f2509e63c4f43882633342be174c3",
      "0fe94491428645bbb3e19a7f4b2717da",
      "fc05bd74c6c4405397261eb5b5e94ff6",
      "5fbf4fa104d34ba9b51e06013b6ab366",
      "f4500d2277bd439da559c87b0ecdbfce",
      "a85754508a5e49b9967e2ff4459284ee",
      "a5d39d3fcd58402f8473e68b363adfaa",
      "e6532d8e0f1143b782cfbfca890a371f",
      "5441fc6dad8e4b42a113adc2505e4ee7",
      "20ab2dd6f4f543c986744a04323da0bd",
      "e2c0b6070cb341888d5b35478994da2b",
      "0a4d9d408e2c4819a1843bd37b613827",
      "ce64de7f19a44c209cb142813d8cc2b7",
      "8f23313794164f7e962f0f84d432c479",
      "9f1b17e5f9434567beb0a4495f971f41",
      "7d14de7af47f43929a60c55230490f81",
      "f63c9a629f3644b6a32beb1dfdd4fc1d",
      "dd60db2ef7014f78add4fe1f26beea5f",
      "2b5b54987bbc4ac0912984aa97f7a90c"
     ]
    },
    "outputId": "70b00742-18db-4281-8d56-bebc99502d93"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/468 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b24be8f6605a472b82727c7187276545"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 1/10\t Loss:2.9319 (9.89s)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/468 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "375bbdbf987a467196cfa24b24ada57e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 2/10\t Loss:1.8498 (10.12s)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/468 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d5553ef9f9774169818684b6df4060da"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 3/10\t Loss:1.4014 (9.83s)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/468 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a45b5d5d0afe4709b08ea1631d2e4cfd"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 4/10\t Loss:1.1273 (9.72s)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/468 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "55b8a2a1c1664bb790695bf9b32a01a1"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 5/10\t Loss:0.9399 (10.41s)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/468 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6faf9d38032b4f6f97050af3c6d4cb75"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 6/10\t Loss:0.8165 (10.26s)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/468 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e741a3b420444e8d8b403948ef4ac4fe"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 7/10\t Loss:0.7292 (9.67s)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/468 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "36e40cc5e7784d63863a94d390d74231"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 8/10\t Loss:0.6674 (10.20s)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/468 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6b531871e8544582bb57111d8c30f958"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 9/10\t Loss:0.6175 (10.17s)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/468 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5441fc6dad8e4b42a113adc2505e4ee7"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch:10/10\t Loss:0.5740 (10.03s)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    train_transformer_model(transformer_encoder, transformer_decoder, optimizer, DEVICE, N_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LbwJp2AMM0bz"
   },
   "source": [
    "## Inference\n",
    "\n",
    "Now that we have trained the model, we can use it on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yv3OYDbaWfGd",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "6dfc0389-354b-4ea9-a3dd-c9f0ab125f00"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Source sentence:\t\t <start> \u00bf eso es un murcielago ? <end>\n",
      "Target sentence:\t\t <start> is that a bat ? <end>\n",
      "Sampled sentence (T=0.01):\t <start> is that a bat ? <end>\n",
      "Sampled sentence (T=1.0):\t <start> is that a bat ? <end>\n",
      "Sampled sentence (T=2.0):\t <start> is . that have a bat ? <end>\n",
      "----------------\n",
      "Source sentence:\t\t <start> quiero que se vaya tom . <end>\n",
      "Target sentence:\t\t <start> i want tom to leave . <end>\n",
      "Sampled sentence (T=0.01):\t <start> i want tom to go . <end>\n",
      "Sampled sentence (T=1.0):\t <start> i want you to go . <end>\n",
      "Sampled sentence (T=2.0):\t <start> i want wish you is sleepy . <end>\n",
      "----------------\n",
      "Source sentence:\t\t <start> sobre la mesa habia un gato . <end>\n",
      "Target sentence:\t\t <start> a cat was on the table . <end>\n",
      "Sampled sentence (T=0.01):\t <start> i was on the cat . <end>\n",
      "Sampled sentence (T=1.0):\t <start> i took my cat . <end>\n",
      "Sampled sentence (T=2.0):\t <start> tulips swimmer . <end>\n",
      "----------------\n",
      "Source sentence:\t\t <start> tom renuncio al plan . <end>\n",
      "Target sentence:\t\t <start> tom abandoned the plan . <end>\n",
      "Sampled sentence (T=0.01):\t <start> tom abandoned the plan . <end>\n",
      "Sampled sentence (T=1.0):\t <start> tom resigned the plan . <end>\n",
      "Sampled sentence (T=2.0):\t <start> tom abandoned his plan . <end>\n",
      "----------------\n",
      "Source sentence:\t\t <start> \u00bf todos lo quieren ? <end>\n",
      "Target sentence:\t\t <start> does everybody want it ? <end>\n",
      "Sampled sentence (T=0.01):\t <start> everyone likes it ? <end>\n",
      "Sampled sentence (T=1.0):\t <start> do you love him ? <end>\n",
      "Sampled sentence (T=2.0):\t <start> they want tom it ? <end>\n",
      "----------------\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    transformer_encoder.eval()\n",
    "    transformer_decoder.eval()\n",
    "    idxes = random.choices(range(len(test_dataset.dataset)), k=5)\n",
    "    src, trg =  train_dataset.dataset[idxes]\n",
    "    results = {}\n",
    "    for temp in [0.01, 1.0, 2.0]:\n",
    "        results[temp] = sample_model(transformer_encoder, transformer_decoder, src.transpose(0,1).to(DEVICE), trg.size(1), temp)\n",
    "    for i in range(len(src)):\n",
    "        print(\"Source sentence:\\t\\t\", ' '.join([x for x in [src_vocab.idx2word[j.item()] for j in src[i]] if x != '<pad>']))\n",
    "        print(\"Target sentence:\\t\\t\", ' '.join([x for x in [trg_vocab.idx2word[j.item()] for j in trg[i]] if x != '<pad>']))\n",
    "        for temp in results:\n",
    "            print(\"Sampled sentence (T=\"+str(temp)+\"):\\t\", ' '.join([x for x in [trg_vocab.idx2word[j] for j in results[temp][i]] if x != '<pad>']))\n",
    "        print(\"----------------\")"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "if __name__ == '__main__':\n",
    "    transformer_encoder.eval()\n",
    "    transformer_decoder.eval()\n",
    "    for h in range(5): # Do whole thing 5 times here, since doing batch size of 1 for beam search\n",
    "        idxes = random.choices(range(len(test_dataset.dataset)), k=1)\n",
    "        src, trg =  train_dataset.dataset[idxes]\n",
    "        beam_size = 5\n",
    "        alphas = [0.6, 100]\n",
    "        beam_result = [beam_decode_model(transformer_encoder, transformer_decoder, src.transpose(0,1).to(DEVICE), trg.size(1), beam_size,alpha=alpha) for alpha in alphas]\n",
    "        greedy_result = beam_decode_model(transformer_encoder, transformer_decoder, src.transpose(0,1).to(DEVICE), trg.size(1), 1,alpha=0)\n",
    "\n",
    "        print(\"Source sentence:\\t\\t\\t\\t\", ' '.join([x for x in [src_vocab.idx2word[j.item()] for j in src[0]] if x != '<pad>']))\n",
    "        print(\"Target sentence:\\t\\t\\t\\t\", ' '.join([x for x in [trg_vocab.idx2word[j.item()] for j in trg[0]] if x != '<pad>']))\n",
    "        print(\"Predicted sentence (greedy search):\\t\\t\", ' '.join([x for x in [trg_vocab.idx2word[j] for j in greedy_result] if x != '<pad>']))\n",
    "        for i in range(len(alphas)):\n",
    "            print(\"Predicted sentence (beam search, alpha=\"+str(alphas[i])+\"):\\t\", ' '.join([x for x in [trg_vocab.idx2word[j] for j in beam_result[i]] if x != '<pad>']))\n",
    "        print(\"----------------\")"
   ],
   "metadata": {
    "id": "sjIH9i8ycOSm",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "8f7c40c2-590c-4550-fd44-683e9888bb3a"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Source sentence:\t\t\t\t <start> el no ha estado aqui mucho tiempo . <end>\n",
      "Target sentence:\t\t\t\t <start> he hasn t been here long . <end>\n",
      "Predicted sentence (greedy search):\t\t <start> he hasn t been here long . <end>\n",
      "Predicted sentence (beam search, alpha=0.6):\t <start> he hasn t been here long . <end>\n",
      "Predicted sentence (beam search, alpha=100):\t <start> the weather hasn t been here long long . . <end>\n",
      "----------------\n",
      "Source sentence:\t\t\t\t <start> \u00bf lo puedo hacer enseguida ? <end>\n",
      "Target sentence:\t\t\t\t <start> may i do it right now ? <end>\n",
      "Predicted sentence (greedy search):\t\t <start> can i be done it ? <end>\n",
      "Predicted sentence (beam search, alpha=0.6):\t <start> can i be done it ? <end>\n",
      "Predicted sentence (beam search, alpha=100):\t <start> can i be done it now ? sorry for ? <end>\n",
      "----------------\n",
      "Source sentence:\t\t\t\t <start> hemos estado alli . <end>\n",
      "Target sentence:\t\t\t\t <start> we ve been there . <end>\n",
      "Predicted sentence (greedy search):\t\t <start> we ve been there . <end>\n",
      "Predicted sentence (beam search, alpha=0.6):\t <start> we ve been there . <end>\n",
      "Predicted sentence (beam search, alpha=100):\t <start> we ve been there before you are over there . <end>\n",
      "----------------\n",
      "Source sentence:\t\t\t\t <start> aprecio el consejo . <end>\n",
      "Target sentence:\t\t\t\t <start> i appreciate the advice . <end>\n",
      "Predicted sentence (greedy search):\t\t <start> i appreciate the advice . <end>\n",
      "Predicted sentence (beam search, alpha=0.6):\t <start> i appreciate the advice . <end>\n",
      "Predicted sentence (beam search, alpha=100):\t <start> i appreciate the advice . maybe it s second . <end>\n",
      "----------------\n",
      "Source sentence:\t\t\t\t <start> \u00bf quien se rie ahora ? <end>\n",
      "Target sentence:\t\t\t\t <start> who s laughing now ? <end>\n",
      "Predicted sentence (greedy search):\t\t <start> who are you laughing now ? <end>\n",
      "Predicted sentence (beam search, alpha=0.6):\t <start> who are you laughing now ? <end>\n",
      "Predicted sentence (beam search, alpha=100):\t <start> who are you laughing now ? maybe ? maybe ? <end>\n",
      "----------------\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W3KeBgqjoVP-"
   },
   "source": [
    "## Evaluate Transformer Model\n",
    "\n",
    "Now we can run the test set through the transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UaIW7SmMEBZ_",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "f490900c-fc90-4df1-fcd2-8b70b37f9ac1"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "BLEU 1-gram: 0.300730\n",
      "BLEU 2-gram: 0.084633\n",
      "BLEU 3-gram: 0.062233\n",
      "BLEU 4-gram: 0.059465\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    transformer_save_candidate, transformer_scores = evaluate_model(transformer_encoder, transformer_decoder, test_dataset, trg_tensor_val, DEVICE)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true,
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}