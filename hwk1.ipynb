{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AL8T2iZUN2Qj"
   },
   "source": [
    "# Tokenization & Language Models\n",
    "We will study **tokenization** and **language modeling**. In particular, WordPiece tokenizer and some n-gram language models on a corpus of Wikipedia articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6zXwUyLZogP_"
   },
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FCTcbDLsh-nS"
   },
   "source": [
    "# Part 1: WordPiece Tokenization [28 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UU08lg2JoX9A"
   },
   "source": [
    "## Download & preprocess the data\n",
    "\n",
    "We use a subset of the FineWeb BBC News dataset. You can see the Huggingface dataset card [here](https://huggingface.co/datasets/permutans/fineweb-bbc-news) (including a discussion of potential limitations or biases), and the corresponding research paper [here](https://arxiv.org/pdf/2406.17557)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LwpobI7Moeo-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "class NewsDataset():\n",
    "    \"\"\"\n",
    "    A class to manage and preprocess the News dataset for this homework.\n",
    "\n",
    "    Attributes:\n",
    "        train (list[str]): The training dataset as a list of sentences.\n",
    "        test (list[str]): The testing dataset as a list of sentences.\n",
    "    \"\"\"\n",
    "    def __init__(self, redownload_dataset=False):\n",
    "        \"\"\"\n",
    "        Initializes the NewsDataset object. If the dataset files do not exist\n",
    "        or redownload_dataset is set to True, it downloads the dataset.\n",
    "\n",
    "        Args:\n",
    "            redownload_dataset (bool): If True, redownloads the dataset.\n",
    "        \"\"\"\n",
    "        self.train, self.test = [], []\n",
    "        self.load_data(redownload_dataset)\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Returns a string representation of the dataset, including the number of\n",
    "        training and testing examples.\n",
    "\n",
    "        Returns:\n",
    "            str: Information about the dataset size\n",
    "        \"\"\"\n",
    "        repr = {}\n",
    "        repr[\"train\"] = len(self.train) if self.train else 0\n",
    "        repr[\"test\"] = len(self.test) if self.test else 0\n",
    "        return f\"{type(self).__name__}({repr})\"\n",
    "\n",
    "    @staticmethod\n",
    "    def isValid(s: str) -> bool:\n",
    "        \"\"\"\n",
    "        Checks whether a given string is valid for inclusion in the dataset.\n",
    "        A string is considered valid if it contains more than five words and\n",
    "        at least one alphabetic character.\n",
    "\n",
    "        Args:\n",
    "            s (str): The string to validate.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if the string is valid, False otherwise.\n",
    "        \"\"\"\n",
    "        return s and (len(s.strip().split()) > 5) and any(c.isalpha() for c in s)\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess(paragraph: str):\n",
    "        paragraph = paragraph.translate(str.maketrans({'?': '.', '!': '.'}))\n",
    "        sentences = paragraph.split('.')\n",
    "        sentences = [s.replace(\"\\\"\", \"\").replace(\"\\'\", \"\").strip().replace(\"\\n\", \" \") + '.' for s in sentences]\n",
    "        sentences = [s.lower() for s in sentences if '  ' not in s]\n",
    "        sentences = [s for s in sentences if not any(char.isdigit() for char in s)]\n",
    "        return sentences\n",
    "\n",
    "    def preprocessWP(self, data):\n",
    "        \"\"\"\n",
    "        Preprocesses a dataset by tokenizing them into\n",
    "        sentences and filtering out invalid ones.\n",
    "\n",
    "        Args:\n",
    "            data: An iterable containing the text of articles.\n",
    "\n",
    "        Returns:\n",
    "            list[str]: A list of valid sentences.\n",
    "        \"\"\"\n",
    "        dataset = []\n",
    "        for item in data:\n",
    "            sentences = self.preprocess(item[\"text\"])\n",
    "            sentences = [sent for sent in sentences if self.isValid(sent)]\n",
    "            dataset += sentences\n",
    "        return dataset\n",
    "\n",
    "    def downloadDatasetWP(self):\n",
    "        \"\"\"\n",
    "        Downloads and preprocesses the News dataset.\n",
    "\n",
    "        Returns:\n",
    "            list[str]: A list of preprocessed sentences from the dataset.\n",
    "        \"\"\"\n",
    "        ds = load_dataset(\"permutans/fineweb-bbc-news\", \"sample-10BT\")\n",
    "        ds = self.preprocessWP(ds[\"train\"])\n",
    "        return ds\n",
    "\n",
    "    def generateDatasetWP(self):\n",
    "        \"\"\"\n",
    "        Downloads, preprocesses, and splits the dataset into training and\n",
    "        testing sets.\n",
    "        The processed data is saved as parquet files.\n",
    "        \"\"\"\n",
    "        ds = self.downloadDatasetWP()\n",
    "        data = {\"text\": ds}\n",
    "        df = pd.DataFrame(data)\n",
    "        train = df.sample(frac=0.05,random_state=200)\n",
    "        test = train.sample(frac=0.1,random_state=200)\n",
    "        train = train.drop(test.index)\n",
    "        train.to_parquet('train.parquet', index=False)\n",
    "        test.to_parquet('test.parquet', index=False)\n",
    "\n",
    "    def load_data(self, redownload_dataset):\n",
    "        \"\"\"\n",
    "        Loads the dataset from parquet files. If the files do not exist or\n",
    "        redownload_dataset is set to True, it downloads the dataset.\n",
    "        \"\"\"\n",
    "        if (redownload_dataset or\n",
    "                not os.path.exists('train.parquet') or\n",
    "                not os.path.exists('test.parquet')):\n",
    "            self.generateDatasetWP()\n",
    "        self.train, self.test = (pd.read_parquet('train.parquet')[\"text\"].tolist(),\n",
    "                                 pd.read_parquet('test.parquet')[\"text\"].tolist())\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dataset = NewsDataset()\n",
    "\n",
    "    print('\\n', dataset, '\\n\\nExample sentences:')\n",
    "\n",
    "    for sentence in dataset.train[:10]:\n",
    "        print('\\t', sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NqinUgNann-8"
   },
   "source": [
    "## Tokenizer\n",
    "\n",
    "First, we will write a training (learning) algorithm, which takes in a corpus (list of sentences), and returns a token vocabulary. Then, we will write the tokenization algorithm. This takes in a sentence and returns its tokenization based on the vocabulary.\n",
    "\n",
    "Code for this section will all be in the `WordPieceTokenizer` class, where you will implement several functions.\n",
    "\n",
    "### Training algorithm\n",
    "\n",
    "WordPiece learns a token vocabulary in an iterative fashion. Starting with an initial token vocabulary (the set of characters in the training data), each step merges a pair of consecutive tokens, where the pair of tokens is selected according to a scoring function (described below). The process is continued until a desired `vocab_size` is reached.\n",
    "\n",
    "1. **`initialize(self, train_corpus)`: [6 points]** This returns the initial vocabulary, the initial tokenization of each word, the word frequncies, and a mapping from each token to the words containing that token. First, split each sentence in `train_corpus` on the space character. The initial vocabulary is the set of all characters that occur in any word; but characters occuring in the middle of a word should be prepended with the special sequence `##`. For example, if your corpus is the sentence `the old man the boat.`, your initial vocabulary would be:\n",
    "<div align=\"center\"><code>{'t', '##h', '##e', 'o', '##l', '##d', 'm', '##a', '##n', 'b', '##o', '##t', '##.'}</code></div>\n",
    "This function should also compute the initial tokenization of each word, storing them in a dictionary from word to tokenization. For example:\n",
    "<div align=\"center\"><code>{'the': ('t', '##h', '##e;), 'old': ('o', '##l', '##d'),                \n",
    "    'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'}\n",
    "</code></div>\n",
    "The function should also count the frequency of each word in the training corpus, and return a mapping from token to the words that use that token. For example:\n",
    "<div align=\"center\"><code>word_freqs = {'the': 100, 'old': 34, 'man': 76, 'boat.': 18}</code></div>\n",
    "    <div align=\"center\"><code>tokens2word = {'t': {'the'}, '##h': {'the'}, '##e': {'the'}, ...}</code></div>\n",
    "\n",
    "2. **`find_best_pair(self, word_tokenizations, word_freqs, vocab)`: [6 points]** This function computes a score for each *consecutive pair* $(t_1, t_2)$ of tokens, and returns the pair of tokens with highest score. The scoring function is $$\\frac{c(t_1, t_2) \\cdot |V|}{c(t_1)\\cdot c(t_2)}$$where $c(t_1, t_2)$ is the number of times $t_1$ and $t_2$ occur consecutively in the corpus, $c(t_i)$ is the nubmer of times token $t_i$ occurs, and $|V|$ is the size of the current token vocabulary. The function will return the pair with highest score (ties broken alphabetically).\n",
    "\n",
    "    The idea behind the scoring function is that a pair of tokens will have high score if they occur frequently together *and* each token does not occur frequently on its own. Factoring in the size of the vocabulary gives a (slight) preference to longer subword units as the vocabulary size grows.\n",
    "    \n",
    "\n",
    "3. **`merge_best_pair(self, best_pair, vocab, word_tokenizations, tokens2word)`: [6 points]** This function updates your current vocabulary (`vocab`), tokenizations (`word_tokenizations`), and token-to-word mapping (`tokens2word`) based on the pair of tokens to merge (`best_pair`). The `vocab` should simply be updated to include the new token, and any word in `word_tokenizations` that contains the consecutive pair of tokens should be updated to merge that pair. For example, suppose the pair of tokens to merge is `(##h, ##e)` and `word_tokenizations` is\n",
    "<div align=\"center\"><code>  {'the': ('t', '##h', '##e;), 'old': ('o', '##l', '##d'),                \n",
    "    'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'}</code></div>\n",
    "Then <code>word_tokenizations</code> should be updated to\n",
    "<div align=\"center\"><code>  {'the': ('t', '##he'), 'old': ('o', '##l', '##d'),                      \n",
    "    'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'}</code></div>\n",
    "Similarly, <code>tokens2word</code> should be updated to include the new token:\n",
    "<div align=\"center\"><code>tokens2word['##he'] = {'the'}</code></div>\n",
    "\n",
    "\n",
    "### Tokenization algorithm\n",
    "\n",
    "Finally, you will write the `tokenize` and `detokenize` functions, which use the token vocabulary you learned to tokenize and detokenize arbitrary sentences:\n",
    "\n",
    "1. **`tokenize(self, sentence)`: [5 points]** This function takes a sentence and returns its tokenization. First, split on the space character. For each word, find the longest substring starting at the beginning of the word that is in the token vocabulary, and choose that token. Then, iteratively repeat this procedure on the remainder of the word, until the entire word is tokenized. For example, consider the word `swiftly`:\n",
    "    * If `s` and `sw` are tokens but `swi` is not, then choose `sw`.\n",
    "    * Then, proceed with `##iftly`. If `##i`, `##if`, and `##ift` are tokens, but `##iftl` is not, choose `##ift`.\n",
    "    * Then, proceed with `##ly`. If both `##l` and `##ly` are tokens, choose `##ly`.\n",
    "\n",
    "  After tokenizing each word in this way, return a list containing the tokenization of each word separated by space tokens. For the sentence `he is swift.`, for example, you might return `['he', ' ', 'is', ' ', 'sw', '##ift', '##.']`\n",
    "\n",
    "  **Note:** If at any step during the tokenization of a word, you cannot find a matching token in your vocabulary, then the *entire* word should be tokenized as `<UNK>`. For example, if you are considering `##mat` but `##m`, `##ma`, and `##mat` are not in the vocabulary, the entire word (including the part of the word preceeding `##mat`) should be tokenized as `<UNK>`.\n",
    "\n",
    "\n",
    "2. **`detokenize(self, tokens)`: [5 points]** This takes a list of tokens and returns its corresponding sentence. Simply copy tokens into a string from left to right, being careful to remove `##` at the beginning of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RWRZljnLnMWu"
   },
   "outputs": [],
   "source": [
    "class WordPieceTokenizer(object):\n",
    "    def __init__(self, train_corpus, vocab_size, do_print=False, do_tqdm=True):\n",
    "        '''\n",
    "\n",
    "        Trains a WordPiece tokenizer.\n",
    "        Args:\n",
    "            train_corpus: List of strings (each string is a sentence)\n",
    "            vocab_size: Integer indicating how big the vocabulary should be\n",
    "            do_print: Whether or not to print merges\n",
    "        '''\n",
    "        self.train_corpus = train_corpus\n",
    "        self.vocab_size = vocab_size\n",
    "        self.do_print = do_print\n",
    "        self.do_tqdm = do_tqdm\n",
    "        for sent in train_corpus: assert '##' not in sent, sent\n",
    "        self.vocab = None\n",
    "\n",
    "    def initialize(self, train_corpus):\n",
    "        '''\n",
    "        Initialize token vocabulary, word frequencies, and token-to-word dictionaries.\n",
    "        Args:\n",
    "            train_corpus: List of strings, where each string is a sentence\n",
    "        Returns:\n",
    "            vocab: A set containing the initial token vocabulary (each token is a string)\n",
    "            word_tokenizations: A dictionary of word to its tokenization (a tuple of strings)\n",
    "            word_freqs: A dictionary that maps each word to its frequency in the training data\n",
    "            tokens2word: A dictionary that maps each token to the set of words with that token\n",
    "        '''\n",
    "\n",
    "        vocab = {'<UNK>', ' '} # Initialize vocab with special tokens (don't change this)\n",
    "        word_tokenizations, word_freqs, tokens2word = {}, {}, {}\n",
    "\n",
    "        ### TODO ###\n",
    "\n",
    "        return vocab, word_tokenizations, word_freqs, tokens2word\n",
    "\n",
    "    def find_best_pair(self, word_tokenizations, word_freqs, vocab):\n",
    "        '''\n",
    "        Score all pairs of consecutive tokens (bigrams) in train_corpus, and return the pair with\n",
    "          highest score. If there is a tie, choose the pair that is first alphabetically.\n",
    "        Args:\n",
    "            word_freqs: Dictionary of word to frqeuency\n",
    "            word_tokenizations: Dictionary of word to its tokenization (a tuple of strings)\n",
    "            vocab: A set of tokens (strings)\n",
    "        Returns:\n",
    "            best_pair: The pair (tuple) of tokens (t1, t2) that has highest score\n",
    "            scores: Dictionary of tokens (t1, t2) to its score\n",
    "        '''\n",
    "\n",
    "        best_pair, scores = None, {}\n",
    "\n",
    "        ### TODO ###\n",
    "\n",
    "        return best_pair, scores\n",
    "\n",
    "    def merge_best_pair(self, best_pair, vocab, word_tokenizations, tokens2word):\n",
    "        '''\n",
    "        Update vocab, word_tokenizations, and tokens2word based on pair to merge.\n",
    "        Args:\n",
    "            best_pair: The pair of tokens that had highest score; that is, the pair to be merged\n",
    "            vocab: The token vocabulary (set of strings)\n",
    "            word_tokenizations: Dictionary from word to tokenization\n",
    "            tokens2word: A dictionary from token to the set of words with that token\n",
    "        Returns: Nothing\n",
    "        Modifies:\n",
    "            vocab: The new token should be added to the vocab set.\n",
    "            word_tokenizations: Any word that contains the consecutive pair of tokens in best_pair\n",
    "              should be re-tokenized such that that pair of tokens has been merged.\n",
    "            tokens2word: The new token should be mapped to the set of words that use this token;\n",
    "              do NOT otherwise modify this dictionary.\n",
    "        Hint: When looking for words that contain the new token, look ONLY at words that use either of\n",
    "          the tokens in best_pair (by using tokens2word).\n",
    "        '''\n",
    "\n",
    "        ### TODO ###\n",
    "\n",
    "        pass\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        '''\n",
    "\n",
    "        Trains the WordPiece tokenization algorithm by calling iteratively merging tokens until\n",
    "          vocab_size is reached.\n",
    "        Args:\n",
    "            train_corpus: List of strings, where each string is a sentence\n",
    "            vocab_size: The desired vocabulary size\n",
    "        Returns:\n",
    "            vocab: The set of tokens in the vocabulary, returned as a sorted list\n",
    "        '''\n",
    "        vocab, word_tokenizations, word_freqs, tokens2word = self.initialize(self.train_corpus)\n",
    "        init_vocab_size = len(vocab)\n",
    "        if self.do_print: print(\"Initial vocab size:\", init_vocab_size)\n",
    "        assert self.vocab_size >= len(vocab), 'Cannot give a vocab size smaller than initial vocab size'\n",
    "\n",
    "        itr = tqdm(range(self.vocab_size - len(vocab))) if self.do_tqdm else range(self.vocab_size - len(vocab))\n",
    "        for i in itr:\n",
    "            best_pair, scores = self.find_best_pair(word_tokenizations, word_freqs, vocab)\n",
    "            self.merge_best_pair(best_pair, vocab, word_tokenizations, tokens2word)\n",
    "            outputs = (vocab, word_tokenizations, tokens2word)\n",
    "            # print(\"\\tMerging \", best_pair) # UNCOMMENT IF YOU WANT TO SEE THE MERGES\n",
    "            assert len(vocab) == init_vocab_size + i + 1, str(len(vocab)) + ' '+ str(init_vocab_size) + ' ' + str(i)\n",
    "            if all(len(word_tokenizations[x]) == 1 for x in word_tokenizations):\n",
    "                print(\"All words have been maximally merged at vocab_size=\" +str(len(vocab)) + \" – breaking.\")\n",
    "                break\n",
    "        if self.do_print: print(\"Done!\")\n",
    "        self.vocab = sorted(vocab)\n",
    "\n",
    "\n",
    "    def tokenize(self, sentence):\n",
    "        '''\n",
    "        Tokenizes a sentence.\n",
    "        Args:\n",
    "            sentence: A string representing a sentence\n",
    "        Returns:\n",
    "            tokens: A list containing the tokens of the sentence\n",
    "        '''\n",
    "        assert type(sentence) == str\n",
    "        tokens = []\n",
    "\n",
    "        ### TODO ###\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def detokenize(self, tokens):\n",
    "        '''\n",
    "        Detokenizes a sentence.\n",
    "        Args:\n",
    "            tokens: A list of tokens representing a sentence\n",
    "        Returns:\n",
    "            sentence: A string representing the sentence\n",
    "        '''\n",
    "        assert type(tokens) == list and len(tokens) > 0 and type(tokens[0]) == str\n",
    "        sentence = ''\n",
    "\n",
    "\n",
    "        return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nXCkjvBaLp_B"
   },
   "source": [
    "##Sanity Check: Tokenizer Class\n",
    "\n",
    "The code below runs a sanity check for `WordPieceTokenizer` class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "1h6jCy1WL9WG"
   },
   "outputs": [],
   "source": [
    "# @title Sanity Check Code\n",
    "\n",
    "def check_dictionary(test_dict, correct_dict):\n",
    "    if not isinstance(test_dict, dict) or not isinstance(correct_dict, dict):\n",
    "        return \"INCORRECT\", \"Is not a dictionary.\"\n",
    "    if set(test_dict.keys()) != set(correct_dict.keys()):\n",
    "        return \"INCORRECT\", f\"Key mismatch: Missing {set(correct_dict.keys()) - set(test_dict.keys())}, Extra {set(test_dict.keys()) - set(correct_dict.keys())}\"\n",
    "    for key in correct_dict:\n",
    "        if test_dict[key] != correct_dict[key]:\n",
    "            return \"INCORRECT\", f\"Value mismatch for key '{key}': Expected {correct_dict[key]}, Got {test_dict[key]}\"\n",
    "    return \"CORRECT\", ''\n",
    "\n",
    "def check_set(test_set, correct_set):\n",
    "    if not isinstance(test_set, set) or not isinstance(correct_set, set):\n",
    "        return \"INCORRECT\", \"Is not a set.\"\n",
    "    if test_set != correct_set:\n",
    "        return \"INCORRECT\", f\"Set mismatch: Missing {correct_set - test_set}, Extra {test_set - correct_set}\"\n",
    "    return \"CORRECT\", \"\"\n",
    "\n",
    "def sanityCheckInitialize(train):\n",
    "    tokenizer = WordPieceTokenizer(train, 45, do_print=False, do_tqdm=False)\n",
    "\n",
    "    print('\\n\\n--- TEST: initialize(self, train_corpus) ---')\n",
    "    res = tokenizer.initialize(train)\n",
    "    if len(res) != 4:\n",
    "        print('FAILED\\ninitialize(self, train_corpus) must return 4 items.')\n",
    "        return\n",
    "    vocab, word_tokenizations, word_freqs, tokens2word = res\n",
    "\n",
    "    correct_vocab = {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##i', '##m', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', 'b', '##e', '##w', '##l', ' ', '##;', '##.', 's', 'o', '##h', '##s', 'f'}\n",
    "    correct_word_tokenizations = {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##o', '##w', '##;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\n",
    "    correct_word_freqs = {'the': 2, 'old': 1, 'man': 1, 'boat.': 1, 'time': 1, 'flies': 2, 'like': 2, 'an': 1, 'arrow;': 1, 'fruit': 1, 'a': 1, 'banana.': 1, 'she': 2, 'told': 1, 'him': 1, 'that': 2, 'knew': 1, 'he': 1, 'lied.': 1}\n",
    "    correct_tokens2word = {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'that', 'banana.', 'boat.', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'that', 'boat.', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'lied.', 'like'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}}\n",
    "\n",
    "    pass1, msg1 = check_set(vocab, correct_vocab)\n",
    "    pass2, msg2 = check_dictionary(word_tokenizations, correct_word_tokenizations)\n",
    "    pass3, msg3 = check_dictionary(word_freqs, correct_word_freqs)\n",
    "    pass4, msg4 = check_dictionary(tokens2word, correct_tokens2word)\n",
    "\n",
    "    print('\\tvocab:\\t\\t\\t'+pass1+'\\t'+msg1 + '\\tYour vocab: ' +str(vocab) + '\\tCorrect vocab:' + str(correct_vocab))\n",
    "    print('\\tword_tokenizations:\\t'+pass2+'\\t'+msg2 + '\\tYour word_tokenizations: ' +str(word_tokenizations) + '\\tCorrect word_tokenizations: ' + str(correct_word_tokenizations))\n",
    "    print('\\tword_freqs:\\t\\t'+pass3+'\\t'+msg3 + '\\tYour word_freqs: ' +str(word_freqs) + '\\tCorrect word_freqs: ' + str(correct_word_freqs))\n",
    "    print('\\ttokens2word:\\t\\t'+pass4+'\\t'+msg4 + '\\tYour tokens2word: ' +str(tokens2word) + '\\tCorrect tokens2word: ' + str(correct_tokens2word))\n",
    "\n",
    "    if len({pass1, pass2, pass3, pass4}) == 1 and pass1 == 'CORRECT':\n",
    "        print('\\n  Passed!')\n",
    "    else: print('  Failed.')\n",
    "\n",
    "def sanityCheckFindBestPair(train):\n",
    "    print('\\n\\n--- TEST: find_best_pair(self, word_tokenizations, word_freqs, vocab) ---') # max_len does not matter for this test\n",
    "    test_cases = ({'the': 2, 'old': 1, 'man': 1, 'boat.': 1, 'time': 1, 'flies': 2, 'like': 2, 'an': 1, 'arrow;': 1, 'fruit': 1, 'a': 1, 'banana.': 1, 'she': 2, 'told': 1, 'him': 1, 'that': 2, 'knew': 1, 'he': 1, 'lied.': 1}, [(({'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##o', '##w', '##;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##i', '##m', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', 'b', '##e', '##w', '##l', ' ', '##;', '##.', 's', 'o', '##h', '##s', 'f'}), (('##w', '##;'), {('t', '##h'): 3.2222222222222223, ('##h', '##e'): 1.6111111111111112, ('o', '##l'): 7.25, ('##l', '##d'): 4.833333333333333, ('m', '##a'): 4.142857142857143, ('##a', '##n'): 2.4857142857142858, ('b', '##o'): 4.833333333333333, ('##o', '##a'): 1.380952380952381, ('##a', '##t'): 3.107142857142857, ('##t', '##.'): 2.4166666666666665, ('t', '##i'): 0.6041666666666666, ('##i', '##m'): 3.625, ('##m', '##e'): 1.2083333333333333, ('f', '##l'): 4.833333333333333, ('##l', '##i'): 1.8125, ('##i', '##e'): 0.90625, ('##e', '##s'): 2.4166666666666665, ('l', '##i'): 3.625, ('##i', '##k'): 3.625, ('##k', '##e'): 2.4166666666666665, ('a', '##n'): 1.9333333333333333, ('a', '##r'): 3.2222222222222223, ('##r', '##r'): 3.2222222222222223, ('##r', '##o'): 3.2222222222222223, ('##o', '##w'): 4.833333333333333, ('##w', '##;'): 14.5, ('f', '##r'): 3.2222222222222223, ('##r', '##u'): 9.666666666666666, ('##u', '##i'): 3.625, ('##i', '##t'): 0.90625, ('b', '##a'): 2.0714285714285716, ('##n', '##a'): 1.6571428571428573, ('##a', '##.'): 1.380952380952381, ('s', '##h'): 4.833333333333333, ('t', '##o'): 1.6111111111111112, ('##o', '##l'): 2.4166666666666665, ('h', '##i'): 1.8125, ('##h', '##a'): 1.380952380952381, ('k', '##n'): 5.8, ('##n', '##e'): 0.48333333333333334, ('##e', '##w'): 1.2083333333333333, ('h', '##e'): 1.2083333333333333, ('##e', '##d'): 0.8055555555555556, ('##d', '##.'): 3.2222222222222223})), (({'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##o', '##w;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##i', '##m', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', '##w;', 'b', '##e', '##w', '##l', ' ', '##;', '##.', 's', 'o', '##h', '##s', 'f'}), (('##o', '##w;'), {('t', '##h'): 3.3333333333333335, ('##h', '##e'): 1.6666666666666667, ('o', '##l'): 7.5, ('##l', '##d'): 5.0, ('m', '##a'): 4.285714285714286, ('##a', '##n'): 2.5714285714285716, ('b', '##o'): 5.0, ('##o', '##a'): 1.4285714285714286, ('##a', '##t'): 3.2142857142857144, ('##t', '##.'): 2.5, ('t', '##i'): 0.625, ('##i', '##m'): 3.75, ('##m', '##e'): 1.25, ('f', '##l'): 5.0, ('##l', '##i'): 1.875, ('##i', '##e'): 0.9375, ('##e', '##s'): 2.5, ('l', '##i'): 3.75, ('##i', '##k'): 3.75, ('##k', '##e'): 2.5, ('a', '##n'): 2.0, ('a', '##r'): 3.3333333333333335, ('##r', '##r'): 3.3333333333333335, ('##r', '##o'): 3.3333333333333335, ('##o', '##w;'): 10.0, ('f', '##r'): 3.3333333333333335, ('##r', '##u'): 10.0, ('##u', '##i'): 3.75, ('##i', '##t'): 0.9375, ('b', '##a'): 2.142857142857143, ('##n', '##a'): 1.7142857142857142, ('##a', '##.'): 1.4285714285714286, ('s', '##h'): 5.0, ('t', '##o'): 1.6666666666666667, ('##o', '##l'): 2.5, ('h', '##i'): 1.875, ('##h', '##a'): 1.4285714285714286, ('k', '##n'): 6.0, ('##n', '##e'): 0.5, ('##e', '##w'): 2.5, ('h', '##e'): 1.25, ('##e', '##d'): 0.8333333333333334, ('##d', '##.'): 3.3333333333333335})), (({'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##ow;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##i', '##m', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', '##w;', 'b', '##e', '##w', '##l', ' ', '##;', '##.', 's', 'o', '##h', '##s', 'f'}), (('##r', '##ow;'), {('t', '##h'): 3.4444444444444446, ('##h', '##e'): 1.7222222222222223, ('o', '##l'): 7.75, ('##l', '##d'): 5.166666666666667, ('m', '##a'): 4.428571428571429, ('##a', '##n'): 2.657142857142857, ('b', '##o'): 7.75, ('##o', '##a'): 2.2142857142857144, ('##a', '##t'): 3.3214285714285716, ('##t', '##.'): 2.5833333333333335, ('t', '##i'): 0.6458333333333334, ('##i', '##m'): 3.875, ('##m', '##e'): 1.2916666666666667, ('f', '##l'): 5.166666666666667, ('##l', '##i'): 1.9375, ('##i', '##e'): 0.96875, ('##e', '##s'): 2.5833333333333335, ('l', '##i'): 3.875, ('##i', '##k'): 3.875, ('##k', '##e'): 2.5833333333333335, ('a', '##n'): 2.066666666666667, ('a', '##r'): 3.4444444444444446, ('##r', '##r'): 3.4444444444444446, ('##r', '##ow;'): 10.333333333333334, ('f', '##r'): 3.4444444444444446, ('##r', '##u'): 10.333333333333334, ('##u', '##i'): 3.875, ('##i', '##t'): 0.96875, ('b', '##a'): 2.2142857142857144, ('##n', '##a'): 1.7714285714285714, ('##a', '##.'): 1.4761904761904763, ('s', '##h'): 5.166666666666667, ('t', '##o'): 2.5833333333333335, ('##o', '##l'): 3.875, ('h', '##i'): 1.9375, ('##h', '##a'): 1.4761904761904763, ('k', '##n'): 6.2, ('##n', '##e'): 0.5166666666666667, ('##e', '##w'): 2.5833333333333335, ('h', '##e'): 1.2916666666666667, ('##e', '##d'): 0.8611111111111112, ('##d', '##.'): 3.4444444444444446})), (({'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##row;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', '##w;', 'b', '##e', '##w', '##l', ' ', '##;', '##.', 's', 'o', '##h', '##s', 'f'}), (('##r', '##row;'), {('t', '##h'): 3.5555555555555554, ('##h', '##e'): 1.7777777777777777, ('o', '##l'): 8.0, ('##l', '##d'): 5.333333333333333, ('m', '##a'): 4.571428571428571, ('##a', '##n'): 2.742857142857143, ('b', '##o'): 8.0, ('##o', '##a'): 2.2857142857142856, ('##a', '##t'): 3.4285714285714284, ('##t', '##.'): 2.6666666666666665, ('t', '##i'): 0.6666666666666666, ('##i', '##m'): 4.0, ('##m', '##e'): 1.3333333333333333, ('f', '##l'): 5.333333333333333, ('##l', '##i'): 2.0, ('##i', '##e'): 1.0, ('##e', '##s'): 2.6666666666666665, ('l', '##i'): 4.0, ('##i', '##k'): 4.0, ('##k', '##e'): 2.6666666666666665, ('a', '##n'): 2.1333333333333333, ('a', '##r'): 5.333333333333333, ('##r', '##row;'): 16.0, ('f', '##r'): 5.333333333333333, ('##r', '##u'): 16.0, ('##u', '##i'): 4.0, ('##i', '##t'): 1.0, ('b', '##a'): 2.2857142857142856, ('##n', '##a'): 1.8285714285714285, ('##a', '##.'): 1.5238095238095237, ('s', '##h'): 5.333333333333333, ('t', '##o'): 2.6666666666666665, ('##o', '##l'): 4.0, ('h', '##i'): 2.0, ('##h', '##a'): 1.5238095238095237, ('k', '##n'): 6.4, ('##n', '##e'): 0.5333333333333333, ('##e', '##w'): 2.6666666666666665, ('h', '##e'): 1.3333333333333333, ('##e', '##d'): 0.8888888888888888, ('##d', '##.'): 3.5555555555555554})), (({'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##rrow;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', '##w;', 'b', '##e', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}), (('##r', '##u'), {('t', '##h'): 3.6666666666666665, ('##h', '##e'): 1.8333333333333333, ('o', '##l'): 8.25, ('##l', '##d'): 5.5, ('m', '##a'): 4.714285714285714, ('##a', '##n'): 2.8285714285714287, ('b', '##o'): 8.25, ('##o', '##a'): 2.357142857142857, ('##a', '##t'): 3.5357142857142856, ('##t', '##.'): 2.75, ('t', '##i'): 0.6875, ('##i', '##m'): 4.125, ('##m', '##e'): 1.375, ('f', '##l'): 5.5, ('##l', '##i'): 2.0625, ('##i', '##e'): 1.03125, ('##e', '##s'): 2.75, ('l', '##i'): 4.125, ('##i', '##k'): 4.125, ('##k', '##e'): 2.75, ('a', '##n'): 2.2, ('a', '##rrow;'): 11.0, ('f', '##r'): 11.0, ('##r', '##u'): 33.0, ('##u', '##i'): 4.125, ('##i', '##t'): 1.03125, ('b', '##a'): 2.357142857142857, ('##n', '##a'): 1.8857142857142857, ('##a', '##.'): 1.5714285714285714, ('s', '##h'): 5.5, ('t', '##o'): 2.75, ('##o', '##l'): 4.125, ('h', '##i'): 2.0625, ('##h', '##a'): 1.5714285714285714, ('k', '##n'): 6.6, ('##n', '##e'): 0.55, ('##e', '##w'): 2.75, ('h', '##e'): 1.375, ('##e', '##d'): 0.9166666666666666, ('##d', '##.'): 3.6666666666666665})), (({'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##rrow;'), 'fruit': ('f', '##ru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}), (('a', '##rrow;'), {('t', '##h'): 3.7777777777777777, ('##h', '##e'): 1.8888888888888888, ('o', '##l'): 8.5, ('##l', '##d'): 5.666666666666667, ('m', '##a'): 4.857142857142857, ('##a', '##n'): 2.914285714285714, ('b', '##o'): 8.5, ('##o', '##a'): 2.4285714285714284, ('##a', '##t'): 3.642857142857143, ('##t', '##.'): 2.8333333333333335, ('t', '##i'): 0.7083333333333334, ('##i', '##m'): 4.25, ('##m', '##e'): 1.4166666666666667, ('f', '##l'): 5.666666666666667, ('##l', '##i'): 2.125, ('##i', '##e'): 1.0625, ('##e', '##s'): 2.8333333333333335, ('l', '##i'): 4.25, ('##i', '##k'): 4.25, ('##k', '##e'): 2.8333333333333335, ('a', '##n'): 2.2666666666666666, ('a', '##rrow;'): 11.333333333333334, ('f', '##ru'): 11.333333333333334, ('##ru', '##i'): 4.25, ('##i', '##t'): 1.0625, ('b', '##a'): 2.4285714285714284, ('##n', '##a'): 1.9428571428571428, ('##a', '##.'): 1.619047619047619, ('s', '##h'): 5.666666666666667, ('t', '##o'): 2.8333333333333335, ('##o', '##l'): 4.25, ('h', '##i'): 2.125, ('##h', '##a'): 1.619047619047619, ('k', '##n'): 6.8, ('##n', '##e'): 0.5666666666666667, ('##e', '##w'): 2.8333333333333335, ('h', '##e'): 1.4166666666666667, ('##e', '##d'): 0.9444444444444444, ('##d', '##.'): 3.7777777777777777})), (({'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('f', '##ru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}), (('f', '##ru'), {('t', '##h'): 3.888888888888889, ('##h', '##e'): 1.9444444444444444, ('o', '##l'): 8.75, ('##l', '##d'): 5.833333333333333, ('m', '##a'): 5.0, ('##a', '##n'): 3.0, ('b', '##o'): 8.75, ('##o', '##a'): 2.5, ('##a', '##t'): 3.75, ('##t', '##.'): 2.9166666666666665, ('t', '##i'): 0.7291666666666666, ('##i', '##m'): 4.375, ('##m', '##e'): 1.4583333333333333, ('f', '##l'): 5.833333333333333, ('##l', '##i'): 2.1875, ('##i', '##e'): 1.09375, ('##e', '##s'): 2.9166666666666665, ('l', '##i'): 4.375, ('##i', '##k'): 4.375, ('##k', '##e'): 2.9166666666666665, ('a', '##n'): 3.5, ('f', '##ru'): 11.666666666666666, ('##ru', '##i'): 4.375, ('##i', '##t'): 1.09375, ('b', '##a'): 2.5, ('##n', '##a'): 2.0, ('##a', '##.'): 1.6666666666666667, ('s', '##h'): 5.833333333333333, ('t', '##o'): 2.9166666666666665, ('##o', '##l'): 4.375, ('h', '##i'): 2.1875, ('##h', '##a'): 1.6666666666666667, ('k', '##n'): 7.0, ('##n', '##e'): 0.5833333333333334, ('##e', '##w'): 2.9166666666666665, ('h', '##e'): 1.4583333333333333, ('##e', '##d'): 0.9722222222222222, ('##d', '##.'): 3.888888888888889})), (({'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', 'fru', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}), (('b', '##o'), {('t', '##h'): 4.0, ('##h', '##e'): 2.0, ('o', '##l'): 9.0, ('##l', '##d'): 6.0, ('m', '##a'): 5.142857142857143, ('##a', '##n'): 3.085714285714286, ('b', '##o'): 9.0, ('##o', '##a'): 2.5714285714285716, ('##a', '##t'): 3.857142857142857, ('##t', '##.'): 3.0, ('t', '##i'): 0.75, ('##i', '##m'): 4.5, ('##m', '##e'): 1.5, ('f', '##l'): 9.0, ('##l', '##i'): 2.25, ('##i', '##e'): 1.125, ('##e', '##s'): 3.0, ('l', '##i'): 4.5, ('##i', '##k'): 4.5, ('##k', '##e'): 3.0, ('a', '##n'): 3.6, ('fru', '##i'): 4.5, ('##i', '##t'): 1.125, ('b', '##a'): 2.5714285714285716, ('##n', '##a'): 2.057142857142857, ('##a', '##.'): 1.7142857142857142, ('s', '##h'): 6.0, ('t', '##o'): 3.0, ('##o', '##l'): 4.5, ('h', '##i'): 2.25, ('##h', '##a'): 1.7142857142857142, ('k', '##n'): 7.2, ('##n', '##e'): 0.6, ('##e', '##w'): 3.0, ('h', '##e'): 1.5, ('##e', '##d'): 1.0, ('##d', '##.'): 4.0})), (({'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', 'fru', 'bo', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}), (('##o', '##l'), {('t', '##h'): 4.111111111111111, ('##h', '##e'): 2.0555555555555554, ('o', '##l'): 9.25, ('##l', '##d'): 6.166666666666667, ('m', '##a'): 5.285714285714286, ('##a', '##n'): 3.1714285714285713, ('bo', '##a'): 5.285714285714286, ('##a', '##t'): 3.9642857142857144, ('##t', '##.'): 3.0833333333333335, ('t', '##i'): 0.7708333333333334, ('##i', '##m'): 4.625, ('##m', '##e'): 1.5416666666666667, ('f', '##l'): 9.25, ('##l', '##i'): 2.3125, ('##i', '##e'): 1.15625, ('##e', '##s'): 3.0833333333333335, ('l', '##i'): 4.625, ('##i', '##k'): 4.625, ('##k', '##e'): 3.0833333333333335, ('a', '##n'): 3.7, ('fru', '##i'): 4.625, ('##i', '##t'): 1.15625, ('b', '##a'): 5.285714285714286, ('##n', '##a'): 2.1142857142857143, ('##a', '##.'): 1.7619047619047619, ('s', '##h'): 6.166666666666667, ('t', '##o'): 6.166666666666667, ('##o', '##l'): 9.25, ('h', '##i'): 2.3125, ('##h', '##a'): 1.7619047619047619, ('k', '##n'): 7.4, ('##n', '##e'): 0.6166666666666667, ('##e', '##w'): 3.0833333333333335, ('h', '##e'): 1.5416666666666667, ('##e', '##d'): 1.0277777777777777, ('##d', '##.'): 4.111111111111111})), (({'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##ol', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}), (('##ol', '##d'), {('t', '##h'): 4.222222222222222, ('##h', '##e'): 2.111111111111111, ('o', '##l'): 12.666666666666666, ('##l', '##d'): 4.222222222222222, ('m', '##a'): 5.428571428571429, ('##a', '##n'): 3.257142857142857, ('bo', '##a'): 5.428571428571429, ('##a', '##t'): 4.071428571428571, ('##t', '##.'): 3.1666666666666665, ('t', '##i'): 0.7916666666666666, ('##i', '##m'): 4.75, ('##m', '##e'): 1.5833333333333333, ('f', '##l'): 12.666666666666666, ('##l', '##i'): 3.1666666666666665, ('##i', '##e'): 1.1875, ('##e', '##s'): 3.1666666666666665, ('l', '##i'): 4.75, ('##i', '##k'): 4.75, ('##k', '##e'): 3.1666666666666665, ('a', '##n'): 3.8, ('fru', '##i'): 4.75, ('##i', '##t'): 1.1875, ('b', '##a'): 5.428571428571429, ('##n', '##a'): 2.1714285714285713, ('##a', '##.'): 1.8095238095238095, ('s', '##h'): 6.333333333333333, ('t', '##ol'): 6.333333333333333, ('##ol', '##d'): 12.666666666666666, ('h', '##i'): 2.375, ('##h', '##a'): 1.8095238095238095, ('k', '##n'): 7.6, ('##n', '##e'): 0.6333333333333333, ('##e', '##w'): 3.1666666666666665, ('h', '##e'): 1.5833333333333333, ('##e', '##d'): 1.0555555555555556, ('##d', '##.'): 4.222222222222222})), (({'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}), (('f', '##l'), {('t', '##h'): 4.333333333333333, ('##h', '##e'): 2.1666666666666665, ('o', '##l'): 13.0, ('##l', '##d'): 6.5, ('m', '##a'): 5.571428571428571, ('##a', '##n'): 3.342857142857143, ('bo', '##a'): 5.571428571428571, ('##a', '##t'): 4.178571428571429, ('##t', '##.'): 3.25, ('t', '##i'): 0.8125, ('##i', '##m'): 4.875, ('##m', '##e'): 1.625, ('f', '##l'): 13.0, ('##l', '##i'): 3.25, ('##i', '##e'): 1.21875, ('##e', '##s'): 3.25, ('l', '##i'): 4.875, ('##i', '##k'): 4.875, ('##k', '##e'): 3.25, ('a', '##n'): 3.9, ('fru', '##i'): 4.875, ('##i', '##t'): 1.21875, ('b', '##a'): 5.571428571428571, ('##n', '##a'): 2.2285714285714286, ('##a', '##.'): 1.8571428571428572, ('s', '##h'): 6.5, ('t', '##old'): 6.5, ('h', '##i'): 2.4375, ('##h', '##a'): 1.8571428571428572, ('k', '##n'): 7.8, ('##n', '##e'): 0.65, ('##e', '##w'): 3.25, ('h', '##e'): 1.625, ('##e', '##d'): 1.625, ('##d', '##.'): 6.5})), (({'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'fl', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}), (('o', '##l'), {('t', '##h'): 4.444444444444445, ('##h', '##e'): 2.2222222222222223, ('o', '##l'): 40.0, ('##l', '##d'): 20.0, ('m', '##a'): 5.714285714285714, ('##a', '##n'): 3.4285714285714284, ('bo', '##a'): 5.714285714285714, ('##a', '##t'): 4.285714285714286, ('##t', '##.'): 3.3333333333333335, ('t', '##i'): 0.8333333333333334, ('##i', '##m'): 5.0, ('##m', '##e'): 1.6666666666666667, ('fl', '##i'): 5.0, ('##i', '##e'): 1.25, ('##e', '##s'): 3.3333333333333335, ('l', '##i'): 5.0, ('##i', '##k'): 5.0, ('##k', '##e'): 3.3333333333333335, ('a', '##n'): 4.0, ('fru', '##i'): 5.0, ('##i', '##t'): 1.25, ('b', '##a'): 5.714285714285714, ('##n', '##a'): 2.2857142857142856, ('##a', '##.'): 1.9047619047619047, ('s', '##h'): 6.666666666666667, ('t', '##old'): 6.666666666666667, ('h', '##i'): 2.5, ('##h', '##a'): 1.9047619047619047, ('k', '##n'): 8.0, ('##n', '##e'): 0.6666666666666666, ('##e', '##w'): 3.3333333333333335, ('h', '##e'): 1.6666666666666667, ('##e', '##d'): 1.6666666666666667, ('##d', '##.'): 6.666666666666667})), (({'the': ('t', '##h', '##e'), 'old': ('ol', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'fl', 'ol', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}), (('ol', '##d'), {('t', '##h'): 4.555555555555555, ('##h', '##e'): 2.2777777777777777, ('ol', '##d'): 20.5, ('m', '##a'): 5.857142857142857, ('##a', '##n'): 3.5142857142857142, ('bo', '##a'): 5.857142857142857, ('##a', '##t'): 4.392857142857143, ('##t', '##.'): 3.4166666666666665, ('t', '##i'): 0.8541666666666666, ('##i', '##m'): 5.125, ('##m', '##e'): 1.7083333333333333, ('fl', '##i'): 5.125, ('##i', '##e'): 1.28125, ('##e', '##s'): 3.4166666666666665, ('l', '##i'): 5.125, ('##i', '##k'): 5.125, ('##k', '##e'): 3.4166666666666665, ('a', '##n'): 4.1, ('fru', '##i'): 5.125, ('##i', '##t'): 1.28125, ('b', '##a'): 5.857142857142857, ('##n', '##a'): 2.342857142857143, ('##a', '##.'): 1.9523809523809523, ('s', '##h'): 6.833333333333333, ('t', '##old'): 6.833333333333333, ('h', '##i'): 2.5625, ('##h', '##a'): 1.9523809523809523, ('k', '##n'): 8.2, ('##n', '##e'): 0.6833333333333333, ('##e', '##w'): 3.4166666666666665, ('h', '##e'): 1.7083333333333333, ('##e', '##d'): 1.7083333333333333, ('##d', '##.'): 6.833333333333333})), (({'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'old', '<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'fl', 'ol', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}), (('##d', '##.'), {('t', '##h'): 4.666666666666667, ('##h', '##e'): 2.3333333333333335, ('m', '##a'): 6.0, ('##a', '##n'): 3.6, ('bo', '##a'): 6.0, ('##a', '##t'): 4.5, ('##t', '##.'): 3.5, ('t', '##i'): 0.875, ('##i', '##m'): 5.25, ('##m', '##e'): 1.75, ('fl', '##i'): 5.25, ('##i', '##e'): 1.3125, ('##e', '##s'): 3.5, ('l', '##i'): 5.25, ('##i', '##k'): 5.25, ('##k', '##e'): 3.5, ('a', '##n'): 4.2, ('fru', '##i'): 5.25, ('##i', '##t'): 1.3125, ('b', '##a'): 6.0, ('##n', '##a'): 2.4, ('##a', '##.'): 2.0, ('s', '##h'): 7.0, ('t', '##old'): 7.0, ('h', '##i'): 2.625, ('##h', '##a'): 2.0, ('k', '##n'): 8.4, ('##n', '##e'): 0.7, ('##e', '##w'): 3.5, ('h', '##e'): 1.75, ('##e', '##d'): 3.5, ('##d', '##.'): 14.0})), (({'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d.')}, {'old', '<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'fl', 'ol', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##d.', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}), (('k', '##n'), {('t', '##h'): 4.777777777777778, ('##h', '##e'): 2.388888888888889, ('m', '##a'): 6.142857142857143, ('##a', '##n'): 3.6857142857142855, ('bo', '##a'): 6.142857142857143, ('##a', '##t'): 4.607142857142857, ('##t', '##.'): 5.375, ('t', '##i'): 0.8958333333333334, ('##i', '##m'): 5.375, ('##m', '##e'): 1.7916666666666667, ('fl', '##i'): 5.375, ('##i', '##e'): 1.34375, ('##e', '##s'): 3.5833333333333335, ('l', '##i'): 5.375, ('##i', '##k'): 5.375, ('##k', '##e'): 3.5833333333333335, ('a', '##n'): 4.3, ('fru', '##i'): 5.375, ('##i', '##t'): 1.34375, ('b', '##a'): 6.142857142857143, ('##n', '##a'): 2.4571428571428573, ('##a', '##.'): 3.0714285714285716, ('s', '##h'): 7.166666666666667, ('t', '##old'): 7.166666666666667, ('h', '##i'): 2.6875, ('##h', '##a'): 2.0476190476190474, ('k', '##n'): 8.6, ('##n', '##e'): 0.7166666666666667, ('##e', '##w'): 3.5833333333333335, ('h', '##e'): 1.7916666666666667, ('##e', '##d.'): 3.5833333333333335})), (({'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('kn', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d.')}, {'old', '<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'fl', 'ol', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##d.', '##.', '##rrow;', 's', 'kn', 'o', '##h', '##s', 'f'}), (('s', '##h'), {('t', '##h'): 4.888888888888889, ('##h', '##e'): 2.4444444444444446, ('m', '##a'): 6.285714285714286, ('##a', '##n'): 4.714285714285714, ('bo', '##a'): 6.285714285714286, ('##a', '##t'): 4.714285714285714, ('##t', '##.'): 5.5, ('t', '##i'): 0.9166666666666666, ('##i', '##m'): 5.5, ('##m', '##e'): 1.8333333333333333, ('fl', '##i'): 5.5, ('##i', '##e'): 1.375, ('##e', '##s'): 3.6666666666666665, ('l', '##i'): 5.5, ('##i', '##k'): 5.5, ('##k', '##e'): 3.6666666666666665, ('a', '##n'): 5.5, ('fru', '##i'): 5.5, ('##i', '##t'): 1.375, ('b', '##a'): 6.285714285714286, ('##n', '##a'): 3.142857142857143, ('##a', '##.'): 3.142857142857143, ('s', '##h'): 7.333333333333333, ('t', '##old'): 7.333333333333333, ('h', '##i'): 2.75, ('##h', '##a'): 2.0952380952380953, ('kn', '##e'): 3.6666666666666665, ('##e', '##w'): 3.6666666666666665, ('h', '##e'): 1.8333333333333333, ('##e', '##d.'): 3.6666666666666665}))])\n",
    "\n",
    "    word_freqs, test_cases = test_cases[0], test_cases[1]\n",
    "    tokenizer = WordPieceTokenizer(train, 45, do_print=False, do_tqdm=False)\n",
    "\n",
    "    overall_pass = True\n",
    "    for i in range(len(test_cases)):\n",
    "        res = tokenizer.find_best_pair(test_cases[i][0][0], word_freqs, test_cases[i][0][1])\n",
    "        if len(res) != 2:\n",
    "            pass1, msg1 = 'INCORRECT', 'Did not return 2 items.'\n",
    "        else:\n",
    "            best_pair, scores = res\n",
    "            bp = best_pair == test_cases[i][1][0]\n",
    "            sc, scc = check_dictionary(scores, test_cases[i][1][1])\n",
    "            if bp and sc == 'CORRECT':\n",
    "                pass1, msg1 = 'CORRECT', ''\n",
    "            else:\n",
    "                overall_pass = False\n",
    "                pass1 = 'INCORRECT'\n",
    "                msg1 = ''\n",
    "                if not bp: msg1 += 'best_pair is incorrect. '\n",
    "                if sc == 'INCORRECT': msg1 += 'scores is incorrect (' + scc + ')'\n",
    "        print('\\tCase ' + (' ' if i < 10 else '') + str(i) + ':\\t' + pass1 + '\\t' + msg1 + '\\tword_tokenizations: ' + str(test_cases[i][0][0]) + '\\tword_freqs: ' + str(word_freqs) + '\\tvocab: ' + str(test_cases[i][0][1]) +\n",
    "              '\\tYour best_pair: ' + str(best_pair) + '\\tCorrect best_pair: ' + str(test_cases[i][1][0]) + '\\tYour scores: ' + str(scores) + '\\tCorrect scores: ' + str(test_cases[i][1][1]))\n",
    "    if overall_pass: print('\\n  Passed!')\n",
    "    else: print('  Failed.')\n",
    "\n",
    "def sanityCheckMergeBestPair(train):\n",
    "    print('\\n\\n--- TEST: merge_best_pair(self, best_pair, vocab, word_tokenizations, tokens2word) ---') # max_len does not matter for this test\n",
    "    test_cases = [((('##w', '##;'), {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##i', '##m', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', 'b', '##e', '##w', '##l', ' ', '##;', '##.', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##o', '##w', '##;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}}), ({'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##i', '##m', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', '##w;', 'b', '##e', '##w', '##l', ' ', '##;', '##.', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##o', '##w;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}})), ((('##o', '##w;'), {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##i', '##m', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', '##w;', 'b', '##e', '##w', '##l', ' ', '##;', '##.', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##o', '##w;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}}), ({'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##i', '##m', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', '##w;', 'b', '##e', '##w', '##l', ' ', '##;', '##.', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##ow;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}})), ((('##r', '##ow;'), {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##i', '##m', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', '##w;', 'b', '##e', '##w', '##l', ' ', '##;', '##.', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##ow;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}}), ({'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', '##w;', 'b', '##e', '##w', '##l', ' ', '##;', '##.', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##row;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}})), ((('##r', '##row;'), {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', '##w;', 'b', '##e', '##w', '##l', ' ', '##;', '##.', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##row;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}}), ({'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', '##w;', 'b', '##e', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##rrow;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}})), ((('##r', '##u'), {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', '##w;', 'b', '##e', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##rrow;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}}), ({'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##rrow;'), 'fruit': ('f', '##ru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}})), ((('a', '##rrow;'), {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##rrow;'), 'fruit': ('f', '##ru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}}), ({'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('f', '##ru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}})), ((('f', '##ru'), {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('f', '##ru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}}), ({'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', 'fru', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}})), ((('b', '##o'), {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', 'fru', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}}), ({'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', 'fru', 'bo', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}})), ((('##o', '##l'), {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', 'fru', 'bo', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}}), ({'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##ol', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}})), ((('##ol', '##d'), {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##ol', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}}), ({'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}})), ((('f', '##l'), {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}}), ({'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'fl', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}})), ((('o', '##l'), {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'fl', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}}), ({'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'fl', 'ol', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('ol', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}})), ((('ol', '##d'), {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'fl', 'ol', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('ol', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}}), ({'old', '<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'fl', 'ol', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}, 'old': {'old'}})), ((('##d', '##.'), {'old', '<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'fl', 'ol', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}, 'old': {'old'}}), ({'old', '<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'fl', 'ol', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##d.', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}, 'old': {'old'}, '##d.': {'lied.'}})), ((('k', '##n'), {'old', '<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'fl', 'ol', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##d.', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}, 'old': {'old'}, '##d.': {'lied.'}}), ({'old', '<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'fl', 'ol', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##d.', '##.', '##rrow;', 's', 'kn', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('kn', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}, 'old': {'old'}, '##d.': {'lied.'}, 'kn': {'knew'}})), ((('s', '##h'), {'old', '<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'fl', 'ol', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##d.', '##.', '##rrow;', 's', 'kn', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('kn', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}, 'old': {'old'}, '##d.': {'lied.'}, 'kn': {'knew'}}), ({'old', '<UNK>', 'sh', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'fl', 'ol', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##d.', '##.', '##rrow;', 's', 'kn', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('sh', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('kn', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}, 'old': {'old'}, '##d.': {'lied.'}, 'kn': {'knew'}, 'sh': {'she'}}))]\n",
    "\n",
    "    tokenizer = WordPieceTokenizer(train, 45, do_print=False, do_tqdm=False)\n",
    "\n",
    "    overall_pass = True\n",
    "    for i in range(len(test_cases)):\n",
    "        from copy import deepcopy\n",
    "        vocab, word_tokenizations, tokens2word = deepcopy(test_cases[i][0][1]), deepcopy(test_cases[i][0][2]), deepcopy(test_cases[i][0][3])\n",
    "        vocabo, word_tokenizationso, tokens2wordo = deepcopy(vocab), deepcopy(word_tokenizations), deepcopy(tokens2word)\n",
    "        tokenizer.merge_best_pair(test_cases[i][0][0], vocab, word_tokenizations, tokens2word)\n",
    "\n",
    "        pass1, msg0 = check_set(vocab, test_cases[i][1][0])\n",
    "        pass2, msg2 = check_dictionary(word_tokenizations, test_cases[i][1][1])\n",
    "        pass3, msg3 = check_dictionary(tokens2word, test_cases[i][1][2])\n",
    "        if pass1 == 'CORRECT' and pass2 == 'CORRECT' and pass3 == 'CORRECT':\n",
    "            pass1, msg1 = 'CORRECT', ''\n",
    "        else:\n",
    "            overall_pass = False\n",
    "            pass1 = 'INCORRECT'\n",
    "            msg1 = ''\n",
    "            if pass1 != 'CORRECT': msg1 += 'vocab is incorrect. (' + msg0 + ') '\n",
    "            if pass2 != 'CORRECT': msg1 += 'word_tokenizations is incorrect. (' + msg2 + ') '\n",
    "            if pass3 != 'CORRECT': msg1 += 'tokens2word is incorrect. (' + msg3 + ') '\n",
    "        print('\\tCase ' + (' ' if i < 10 else '') + str(i) + ':\\t' + pass1 + '\\t' + msg1 + '\\tbest_pair: ' + str(test_cases[i][0][0]) + '\\tvocab: ' + str(vocabo) + '\\tword_tokenizations: ' + str(word_tokenizationso) + '\\ttokens2word: ' + str(tokens2wordo) +\n",
    "              '\\tYour vocab: ' + str(vocab) + '\\tCorrect vocab: ' + str(test_cases[i][1][0]) + '\\tYour word_tokenizations: ' + str(word_tokenizations) + '\\tCorrect word_tokenizations: ' + str(test_cases[i][1][1])+\n",
    "              '\\tYour tokens2word: ' + str(tokens2word) + '\\tCorrect tokens2word: ' + str(test_cases[i][1][2]))\n",
    "    if overall_pass: print('\\n  Passed!')\n",
    "    else: print('  Failed.')\n",
    "\n",
    "\n",
    "def sanityCheckTokenize(train, test):\n",
    "\n",
    "    print('\\n\\n--- TEST: tokenize(self, sentence) ---')\n",
    "\n",
    "    tokenizer = WordPieceTokenizer(train, 45, do_print=False, do_tqdm=False)\n",
    "    tokenizer.vocab = [' ', '##.', '##;', '##a', '##d', '##d.', '##e', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##ol', '##old', '##ow;', '##r', '##row;', '##rrow;', '##ru', '##s', '##t', '##u', '##w', '##w;', '<UNK>', 'a', 'arrow;', 'b', 'bo', 'f', 'fl', 'fru', 'h', 'k', 'kn', 'l', 'm', 'o', 'ol', 'old', 's', 'sh', 't']\n",
    "    correct_ans = [['sh', '##e', ' ', '<UNK>', ' ', 't', '##h', '##e', ' ', 'bo', '##o', '##k', ' ', '<UNK>'], ['t', '##i', '##m', '##e', ' ', '<UNK>', ' ', 'f', '##o', '##r', ' ', '<UNK>', ' ', 'o', '##n', '##e', '##.']]\n",
    "\n",
    "    overall_pass = True\n",
    "    for i in range(len(test)):\n",
    "        toks = tokenizer.tokenize(test[i])\n",
    "        if toks == correct_ans[i]: pass1, msg1 = 'CORRECT', ''\n",
    "        else:\n",
    "            pass1, msg1 = 'INCORRECT', 'Tokenization is incorrect.'\n",
    "            overall_pass = False\n",
    "        print('\\tCase ' + (' ' if i < 10 else '') + str(i) + ':\\t' + pass1 + '\\t' + msg1 + '\\tsentence: ' + str(test[i]) +\n",
    "              '\\tYour tokenization: ' + str(toks) + '\\tCorrect tokenization: ' + str(correct_ans[i]))\n",
    "\n",
    "    if overall_pass: print('\\n  Passed!')\n",
    "    else: print('  Failed.')\n",
    "\n",
    "def sanityCheckDetokenize(train):\n",
    "\n",
    "    print('\\n\\n--- TEST: detokenize(self, tokens) ---')\n",
    "\n",
    "    tokenizer = WordPieceTokenizer(train, 45, do_print=False, do_tqdm=False)\n",
    "    tokenizer.vocab = [' ', '##.', '##;', '##a', '##d', '##d.', '##e', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##ol', '##old', '##ow;', '##r', '##row;', '##rrow;', '##ru', '##s', '##t', '##u', '##w', '##w;', '<UNK>', 'a', 'arrow;', 'b', 'bo', 'f', 'fl', 'fru', 'h', 'k', 'kn', 'l', 'm', 'o', 'ol', 'old', 's', 'sh', 't']\n",
    "    inputs = [['sh', '##e', ' ', '<UNK>', ' ', 't', '##h', '##e', ' ', 'bo', '##o', '##k', ' ', '<UNK>'], ['t', '##i', '##m', '##e', ' ', '<UNK>', ' ', 'f', '##o', '##r', ' ', '<UNK>', ' ', 'o', '##n', '##e', '##.']]\n",
    "    correct_ans = ['she <UNK> the book <UNK>', 'time <UNK> for <UNK> one.']\n",
    "\n",
    "    overall_pass = True\n",
    "    for i in range(len(inputs)):\n",
    "        sent = tokenizer.detokenize(inputs[i])\n",
    "        if sent == correct_ans[i]: pass1, msg1 = 'CORRECT', ''\n",
    "        else:\n",
    "            pass1, msg1 = 'INCORRECT', 'Sentence is incorrect.'\n",
    "            overall_pass = False\n",
    "        print('\\tCase ' + (' ' if i < 10 else '') + str(i) + ':\\t' + pass1 + '\\t' + msg1 + '\\ttokens: ' + str(inputs[i]) +\n",
    "              '\\tYour sentence: ' + str(sent) + '\\tCorrect sentence: ' + str(correct_ans[i]))\n",
    "\n",
    "    if overall_pass: print('\\n  Passed!')\n",
    "    else: print('  Failed.')\n",
    "\n",
    "\n",
    "def sanityCheckTokenizer():\n",
    "    sample_train_corpus = [\"the old man the boat.\",\n",
    "                   \"time flies like an arrow; fruit flies like a banana.\",\n",
    "                   \"she told him that she knew that he lied.\"]\n",
    "    sample_test_corpus = [\"she read the book 1984.\",\n",
    "                          \"time waits for no one.\"]\n",
    "\n",
    "    print(\"Sample train corpus:\", sample_train_corpus)\n",
    "    print(\"Sample test corpus:\", sample_test_corpus)\n",
    "    sanityCheckInitialize(sample_train_corpus)\n",
    "    sanityCheckFindBestPair(sample_train_corpus)\n",
    "    sanityCheckMergeBestPair(sample_train_corpus)\n",
    "    sanityCheckTokenize(sample_train_corpus, sample_test_corpus)\n",
    "    sanityCheckDetokenize(sample_train_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cQvj7i7S9_Ru"
   },
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    sanityCheckTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bCbUEgXfLz81"
   },
   "source": [
    "## Run tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lPdN2aTVvTu9"
   },
   "outputs": [],
   "source": [
    "\n",
    "if __name__=='__main__':\n",
    "    tokenizer = WordPieceTokenizer(dataset.train, 700, do_tqdm = True, do_print=False)\n",
    "    tokenizer.train()\n",
    "    print(\"Vocab:\", tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sCR5Iq6oo6wd"
   },
   "source": [
    "Now we can look at some tokenizations on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "csdUptctgyVu"
   },
   "outputs": [],
   "source": [
    "\n",
    "if __name__=='__main__':\n",
    "    for sent in random.sample(dataset.test, 10):\n",
    "        print('Sentence:', sent)\n",
    "        toks = tokenizer.tokenize(sent)\n",
    "        print('Tokenization:', toks)\n",
    "        print('Detokenization:', tokenizer.detokenize(toks))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZKt-WVr6OtP4"
   },
   "source": [
    "# Part 2: Language Models\n",
    "\n",
    "Here, we will train some <b>n-gram language models</b> on WikiText-2, a corpus of high-quality Wikipedia articles. The dataset was originally introduced in the following paper: https://arxiv.org/pdf/1609.07843v1.pdf. A raw version of the data can easily be viewed here: https://github.com/pytorch/examples/tree/master/word_language_model/data/wikitext-2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cN2Ja8MNP4qS"
   },
   "source": [
    "## Download & preprocess the data\n",
    "\n",
    "To make your models more robust, it is necessary to perform some basic preprocessing on the corpora. \n",
    "\n",
    "* <b>Sentence splitting:</b>&nbsp;&nbsp;&nbsp;&nbsp;We are interested in modeling individual sentences, rather than longer chunks of text such as paragraphs or documents. The WikiTest dataset provides paragraphs; thus, we provide a simple method to identify individual sentences by splitting paragraphs at punctuation tokens (\".\",  \"!\",  \"?\").\n",
    "\n",
    "* <b>Sentence markers:</b>&nbsp;&nbsp;&nbsp;&nbsp;For both training and testing corpora, each sentence must be surrounded by a start-of-sentence (`<s>`) and end-of-sentence marker (`/s`). These markers will allow your models to generate sentences that have realistic beginnings and endings.\n",
    "\n",
    "* <b>Unknown words:</b>&nbsp;&nbsp;&nbsp;&nbsp;In order to deal with unknown words in the test corpora, all words that do not appear in the vocabulary must be replaced with a special token for unknown words (`<UNK>`) before estimating your models. The WikiText dataset has already done this, and you can read about the method in the paper above. When unknown words are encountered in the test corpus, they should be treated as that special token instead.\n",
    "\n",
    "After the preprocessing, it is assumed that all words in the test set appear in the training set, as this code has already replaced the unseen tokens with `<UNK>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FCkrUjKEBrNp"
   },
   "outputs": [],
   "source": [
    "\n",
    "START = \"<s>\"   # Start-of-sentence token\n",
    "END = \"</s>\"    # End-of-sentence-token\n",
    "UNK = \"<UNK>\"   # Unknown word token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vUdZstjH30DL"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import sys\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "def preprocess(data, vocab=None):\n",
    "    final_data = []\n",
    "    lowercase = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "    for paragraph in data:\n",
    "        paragraph = [x if x != '<unk>' else UNK for x in paragraph.split()]\n",
    "        if vocab is not None:\n",
    "            paragraph = [x if x in vocab else UNK for x in paragraph]\n",
    "        if paragraph == [] or paragraph.count('=') >= 2: continue\n",
    "        sen = []\n",
    "        prev_punct, prev_quot = False, False\n",
    "        for word in paragraph:\n",
    "            if prev_quot:\n",
    "                if word[0] not in lowercase:\n",
    "                    final_data.append(sen)\n",
    "                    sen = []\n",
    "                    prev_punct, prev_quot = False, False\n",
    "            if prev_punct:\n",
    "                if word == '\"':\n",
    "                    prev_punct, prev_quot = False, True\n",
    "                else:\n",
    "                    if word[0] not in lowercase:\n",
    "                        final_data.append(sen)\n",
    "                        sen = []\n",
    "                        prev_punct, prev_quot = False, False\n",
    "            if word in {'.', '?', '!'}: prev_punct = True\n",
    "            sen += [word]\n",
    "        if sen[-1] not in {'.', '?', '!', '\"'}: continue # Prevent a lot of short sentences\n",
    "        final_data.append(sen)\n",
    "    vocab_was_none = vocab is None\n",
    "    if vocab is None:\n",
    "        vocab = set()\n",
    "    for i in range(len(final_data)):\n",
    "        final_data[i] = [START] + final_data[i] + [END]\n",
    "        if vocab_was_none:\n",
    "            for word in final_data[i]:\n",
    "                vocab.add(word)\n",
    "    return final_data, vocab\n",
    "\n",
    "def getDataset():\n",
    "    splits = ['train', 'valid']\n",
    "    datasets = []\n",
    "    path = './{}.txt'\n",
    "    url = 'https://raw.githubusercontent.com/pytorch/examples/main/word_language_model/data/wikitext-2/{}.txt'\n",
    "    for split in splits:\n",
    "        if os.path.exists(path.format(split)):\n",
    "            print(f\"{split} dataset already downloaded\")\n",
    "        else:\n",
    "            filename = f'{split}.txt'\n",
    "            urlretrieve(url.format(split), filename)\n",
    "        datasets.append(open(f'{split}.txt').read().split('\\n'))\n",
    "    train_dataset, vocab = preprocess(datasets[0])\n",
    "    test_dataset, _ = preprocess(datasets[1], vocab)\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_dataset, test_dataset = getDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mSFJ07ELGUMh"
   },
   "source": [
    "Run the next cell to see 10 random sentences of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "swPwiHBHDDkT"
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    for x in random.sample(train_dataset, 10):\n",
    "        print (x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YM6hNHMqTMt2"
   },
   "source": [
    "## The LanguageModel Class\n",
    "\n",
    "We will implement 4 types of language models: a <b>unigram</b> model, a <b>smoothed unigram</b> model, a <b>bigram</b> model, and a <b>smoothed bigram</b> model. Each of the models is worth 25 points and extends the following base class. <b>You do not need to implement anything in this class</b>; you will instead implement each of the following methods in the relevant subclass:\n",
    "\n",
    "* <b>`__init__(self, trainCorpus)`</b>: Train the language model on `trainCorpus`. This will involve calculating relative frequency estimates according to the type of model you're implementing.\n",
    "\n",
    "* <b>`generateSentence(self)`</b>:  Return a sentence that is generated by the language model. It should be a list of the form <TT>[&lt;s&gt;, w<sup>(1)</sup>, ..., w<sup>(n)</sup>, &lt;&sol;s&gt;]</TT>, where each <TT>w<sup>(i)</sup></TT> is a word in your vocabulary (including <TT>&lt;UNK&gt;</TT> but exlcuding <TT>&lt;s&gt;</TT> and <TT>&lt;&sol;s&gt;</TT>). You may assume that <TT>&lt;s&gt;</TT> starts each sentence (with probability $1$). The following words <TT>w<sup>(1)</sup></TT>, ... , <TT>w<sup>(n)</sup></TT>, <TT>&lt;&sol;s&gt;</TT> are generated according to your language model's distribution. Note that the number of words <TT>n</TT> is not fixed; instead, you should stop the sentence as soon as you generate the stop token <TT>&lt;&sol;s&gt;</TT>.\n",
    "\n",
    "* <b>`getSentenceLogProbability(self, sentence)`</b>: Return the <em> logarithm of the probability</em> of <TT>sentence</TT>, which is again a list of the form <TT>[&lt;s&gt;, w<sup>(1)</sup>, ..., w<sup>(n)</sup>, &lt;&sol;s&gt;]</TT>. You should use the natural logarithm $-$ that is, the base-<em>e</em> logarithm. See the note below about performing your calculations in log space.\n",
    "\n",
    "* <b>`getCorpusPerplexity(self, testCorpus)`</b>:  We need to compute the perplexity (normalized inverse log probability) of `testCorpus` according to your model. For a corpus $W$ with $N$ words and a bigram model, Jurafsky and Martin tells you to compute perplexity as follows:\n",
    "\n",
    "$$Perplexity(W) = \\Big [ \\prod_{i=1}^N \\frac{1}{P(w^{(i)}|w^{(i-1)})} \\Big ]^{1/N}$$\n",
    "\n",
    "In order to avoid underflow, all calculations are done in log-space. That is, instead of multiplying probabilities, we add the logarithms of the probabilities and exponentiate the result:\n",
    "\n",
    "$$\\prod_{i=1}^N P(w^{(i)}|w^{(i-1)}) = \\exp\\Big (\\sum_{i=1}^N \\log P(w^{(i)}|w^{(i-1)}) \\Big ) $$\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
